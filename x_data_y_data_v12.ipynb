{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jalalij/LIS-DeepLearning/blob/master/x_data_y_data_v12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ah-RDKsMS-WZ"
      },
      "source": [
        "# **Beam Managment as a Regression Problem**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdGzed0pT3rK"
      },
      "source": [
        "# Mounting Google Drive in Google Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkP32jo2_Svr",
        "outputId": "c17e74bd-fd78-463f-eef9-3b662871b901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idXGFlUfThGc"
      },
      "source": [
        "# Code Explanation: `prepData` Class\n",
        "\n",
        "The `prepData` class in the code contains methods for generating specific patterns of indices based on given parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6xtQwCqO_efc"
      },
      "outputs": [],
      "source": [
        "class prepData:\n",
        "\n",
        "    PRESET_K_FOR_FIXED_BPATTERN = 5\n",
        "    @staticmethod\n",
        "    def GetBPatternIdxsNonIncremental(N, blen, presetk):\n",
        "        idxs = np.linspace(0, N-1, blen).astype(int) + presetk\n",
        "        return np.array(sorted(list(map(lambda x: x % N, idxs))))\n",
        "\n",
        "    @staticmethod\n",
        "    def GetBPatternIdxs(N, blen, presetk):  # incremental version\n",
        "        if blen == 64 and presetk == prepData.PRESET_K_FOR_FIXED_BPATTERN:\n",
        "            idxs = [ 0, 7, 8, 15, 17, 23, 24, 30, 34, 38, 41, 45, 51, 54, 57, 60, 65, 68, 75, 78,\n",
        "                         82, 85, 90, 93, 99, 102, 105, 112, 119, 120, 123, 127, 128, 131, 135, 136, 138,\n",
        "                         142, 150, 153, 162, 165, 170, 173, 177, 180, 183, 187, 195, 198, 201, 204,\n",
        "                         210, 213, 218, 221, 225, 231, 232, 238, 240, 247, 247, 255]\n",
        "            return idxs\n",
        "        # print(' -> computing linear BPattern: N=%d, blen=%d, presetk=%d' % (N, blen, presetk))\n",
        "        wrapidx = lambda x: x if x >= 0 else x + N\n",
        "        S = set([wrapidx(N//4 - presetk), wrapidx(3*N//4 - presetk)])\n",
        "        for f in (16, 8, 4, 2): #lk in range(2, lkmax+1): # pow(2,1)=2 to pow(2,6)=64\n",
        "            if len(S) == blen:\n",
        "                break\n",
        "            cnt = N//f # min(N//f, blen-len(S))\n",
        "            adds = [f*k for k in range(1,cnt)]\n",
        "            for i,x in enumerate(adds): # [:(blen-len(S))]:\n",
        "                xx = wrapidx(x - presetk - i)\n",
        "                S2 = S | set([xx])\n",
        "                if len(S2) > len(S) and len(S2) <= blen:\n",
        "                    S = S2\n",
        "                    # print(' -> len(S)=%d, f=%d, cnt=%d, after adding x=%d, S=%s, adds=%s' % (len(S), f, cnt, x, sorted(list(S)), adds))\n",
        "                    # print(' -> after adding x=%d, d=%d, f=%d, cnt=%d, adds=%s, S=%s, len(S)=%d' %\n",
        "                    #   (lk, d, f, cnt-1, adds, S, len(S)))\n",
        "        return sorted(list(S))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpBv-sBcUO2e"
      },
      "source": [
        "# easyDataloader Function\n",
        "\n",
        "The `easyDataloader` function is a Python utility designed for loading data from files, specifically supporting both '.npy' (NumPy array files) and '.pkl' (pickle files)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8igTH7W0ym6t"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Created on Tue Dec 4 10:04:35 2023\n",
        "\n",
        "@author: jalal\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def easyDataloader(fpath):\n",
        "    \"\"\"\n",
        "    easyDataloader Function\n",
        "    The function supports loading from '.npy' and pickle ('.pkl') files.\n",
        "\n",
        "    Parameters:\n",
        "    fpath (str): The file path of the data file to be loaded.\n",
        "\n",
        "    Returns:\n",
        "    Return the loaded data\n",
        "    \"\"\"\n",
        "    # Extract the file extension from the file path to determine the file type\n",
        "    ext = fpath.split('.')[-1].lower()\n",
        "\n",
        "    # Define an internal function for loading pickle files\n",
        "    def _pklLoader(fpath):\n",
        "        # Informing the user that the pickle loading process has started\n",
        "        print(' -> pkloader, loading from: %s, ext=%s... ' % (fpath, ext), end='')\n",
        "\n",
        "        # Open and load the pickle file\n",
        "        with open(fpath, 'rb') as file:\n",
        "            data = pkl.load(file)\n",
        "\n",
        "        # Check if the loaded data is a dictionary\n",
        "        dataisDD = type(data) is dict\n",
        "\n",
        "        # If data is a dictionary, try to retrieve data associated with the 'rsrp' key\n",
        "        if dataisDD:\n",
        "            if 'rsrp' in data:\n",
        "                data = data['rsrp']\n",
        "            else:\n",
        "                # If 'rsrp' key is not found, output an error and exit the program\n",
        "                print('\\n ==> Error _pklLoader got dict but no rsrp key\\n   -> fpath=%s\\n    -> dd.keys: %s\\n\\n' % (fpath, data.keys()))\n",
        "                sys.exit(-1)\n",
        "        return data\n",
        "\n",
        "    # Determine the appropriate loader function based on the file extension\n",
        "    loader = np.load if ext == 'npy' else _pklLoader\n",
        "\n",
        "    # Load the data using the selected loader function\n",
        "    data = loader(fpath)\n",
        "\n",
        "    # Informing the user of the successful data load\n",
        "    print(' -> loading from: %s, ext=%s... got: \\n  -> %s' % (fpath, ext, data))\n",
        "\n",
        "    # Return the loaded data\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrIm12EKUo50"
      },
      "source": [
        "# Uploading Data in a Specified Directory\n",
        "\n",
        "This code snippet demonstrates the process of loading a data file from a specified directory, particularly from a Google Drive directory when working in a Google Colab environment. It utilizes the previously discussed `easyDataloader` function for data loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj41tKovf0Vf",
        "outputId": "e602f94f-f925-4dc2-a232-0ed0382cf7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -> loading from: /content/drive/MyDrive/regression_problem/data_npy/processed_rsrp_bst_rx_32x1_524288_1024UE_tx_32_indoor20_wo_SC_default_cluster_C_20230804_123305.npy, ext=npy... got: \n",
            "  -> [[ -88.75314499  -77.14872481  -79.55661402 ...  -69.23797925\n",
            "   -72.40235797  -54.39824452]\n",
            " [ -71.37112322  -87.47744453  -59.86277527 ...  -90.80840704\n",
            "   -72.89914256  -73.14614798]\n",
            " [-131.63571314 -121.71979339 -114.50317071 ... -126.08874352\n",
            "  -124.22857684 -108.31827401]\n",
            " ...\n",
            " [ -76.48051861  -84.81008757  -62.13328083 ...  -79.01981358\n",
            "   -56.3397604   -80.86505277]\n",
            " [ -73.66799437  -66.85643321  -54.58370915 ...  -67.18717965\n",
            "   -57.30622408  -54.50018522]\n",
            " [-144.19761432 -140.47091746 -122.89530827 ... -135.2553118\n",
            "  -117.0663885  -132.31134104]]\n"
          ]
        }
      ],
      "source": [
        "# upload the data in a directory of your choice\n",
        "dataDir = '/content/drive/MyDrive/regression_problem/data_npy'\n",
        "data_name = '/processed_rsrp_bst_rx_umi_uma.npy'\n",
        "data_name = '/processed_rsrp_bst_rx_32x1_524288_1024UE_tx_32_indoor20_wo_SC_default_cluster_C_20230804_123305.npy'\n",
        "fname = dataDir + data_name\n",
        "_rawData = easyDataloader(fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtHzkDmnWK40"
      },
      "source": [
        "# Data Processing and Visualization Code Explanation\n",
        "\n",
        "The provided Python code consists of functions for processing and visualizing data, particularly in the context of beam pattern analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OOE_aog9_piW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Created on Tue Nov 30 10:04:35 2023\n",
        "\n",
        "@author: jalal\n",
        "\"\"\"\n",
        "\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Constants\n",
        "SET_B_LEN = 8\n",
        "# RAND_DRAW_PER_SAMPLE = 1\n",
        "\n",
        "def get_indices(pattern, total_beams, set_b_len, sample=None, total_preset=None):\n",
        "    \"\"\"\n",
        "    Generate indices based on the specified pattern.\n",
        "\n",
        "    Parameters:\n",
        "    pattern (str): Type of pattern ('fixed', 'random', or 'preset').\n",
        "    total_beams (int): Total number of beams.\n",
        "    set_b_len (int): Length of the set B.\n",
        "    sample (int, optional): Sample number, used for 'preset' pattern.\n",
        "    total_preset (int, optional): Total number of presets, used for 'preset' pattern.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Array of indices.\n",
        "    \"\"\"\n",
        "    if pattern == 'fixed':\n",
        "        return prepData.GetBPatternIdxs(total_beams, set_b_len, 5)\n",
        "        # return prepData.GetBPatternIdxsNonIncremental(total_beams, set_b_len, 5)\n",
        "    elif pattern == 'random':\n",
        "        return np.random.choice(total_beams, set_b_len, replace=False).astype(int)\n",
        "    elif pattern == 'preset':\n",
        "        if sample is None or total_preset is None:\n",
        "            raise ValueError(\"Sample and total_preset must be provided for 'preset' pattern\")\n",
        "        return prepData.GetBPatternIdxs(total_beams, set_b_len, sample % total_preset)\n",
        "        # return prepData.GetBPatternIdxsNonIncremental(total_beams, set_b_len, sample % total_preset)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid pattern type\")\n",
        "\n",
        "def get_x_data_y_data(raw_data, set_b_pattern_string, miss_val_rep):\n",
        "    \"\"\"\n",
        "    Process raw data to generate xData and yData based on the specified beam pattern.\n",
        "\n",
        "    Parameters:\n",
        "    raw_data (numpy.ndarray): The input raw data.\n",
        "    set_b_pattern_string (str): The pattern string indicating the type of beam pattern.\n",
        "    miss_val_rep: Representation for missing values.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing processed xData, yData, and repData.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialization\n",
        "    nsamples, total_beams = raw_data.shape\n",
        "    lowest_possible_rsrp = np.min(raw_data)\n",
        "    x_data = np.ones(raw_data.shape) * lowest_possible_rsrp\n",
        "    y_data = raw_data\n",
        "    # rep_data = None\n",
        "    idxs_text = ''\n",
        "\n",
        "    if set_b_pattern_string == 'fixed':\n",
        "        idxs = get_indices('fixed', total_beams, SET_B_LEN)\n",
        "        print(print(\"Indices: %s\" % idxs))\n",
        "        x_data[:, idxs] = raw_data[:, idxs]\n",
        "        idxs_text = ';'.join(map(str, idxs))\n",
        "        print('idxs_text =%s' % idxs_text)\n",
        "\n",
        "    elif set_b_pattern_string == 'random':\n",
        "        x_data = np.ones((nsamples, total_beams)) * lowest_possible_rsrp\n",
        "        for sample in range(nsamples):\n",
        "            idxs = get_indices('random', total_beams, SET_B_LEN)\n",
        "            x_data[sample, idxs] = raw_data[sample, idxs]\n",
        "        idxs_text = 'random'\n",
        "        # rep_data = np.repeat(raw_data, RAND_DRAW_PER_SAMPLE, axis=0)\n",
        "\n",
        "    elif set_b_pattern_string == 'preset':\n",
        "        total_preset = 5\n",
        "        for sample in range(nsamples):\n",
        "            idxs = get_indices('preset', total_beams, SET_B_LEN, sample, total_preset)\n",
        "            x_data[sample, idxs] = raw_data[sample, idxs]\n",
        "        idxs_text = '|'.join([';'.join(map(str, get_indices('preset', total_beams, SET_B_LEN, k, total_preset))) for k in range(total_preset)])\n",
        "        print('idxs_text =%s' % idxs_text)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid setBpatternString value\")\n",
        "\n",
        "\n",
        "    # scale the x_data, y_data\n",
        "    # x_data = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\n",
        "    # y_data = (y_data-np.min(y_data))/(np.max(y_data)-np.min(y_data))\n",
        "\n",
        "    plot_data_statistics(raw_data, 'raw_data')  # Replace 'example_filename' with actual file name\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "def plot_data_statistics(data, filename):\n",
        "    \"\"\"\n",
        "    Plot statistics of the data.\n",
        "\n",
        "    Parameters:\n",
        "    data (numpy.ndarray): Data to be plotted.\n",
        "    filename (str): The name of the file for title.\n",
        "    \"\"\"\n",
        "    plt.close('all')\n",
        "    plt.suptitle(filename.replace('.txt', ''))\n",
        "    x = np.random.choice(data.flatten(), size=int(0.1 * len(data.flatten())))\n",
        "    plt.hist(x, bins=137)\n",
        "    plt.axvline(np.mean(data), color='k', linestyle='dashed', linewidth=1, label='Mean')\n",
        "    plt.axvline(np.median(data), color='blue', linestyle='dashed', linewidth=1, label='Median')\n",
        "    plt.legend(['Mean', 'Median'])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o7g4Doqi_tb3",
        "outputId": "9d9b8750-88f8-4380-b856-7998313d283d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indices: [2, 3, 5, 10, 11, 17, 19, 31]\n",
            "None\n",
            "idxs_text =2;3;5;10;11;17;19;31\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAHNCAYAAAD8AGr/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSUElEQVR4nO3deVxU9foH8M/sDMsM+6aouKRiJomK2GIWiUWLN0stS3BNQ0vJUu8ttbw3zDK1XMhbijfzutyb/koS4+JSVzEL5eaSpoXiNoAKDOus5/eHcmJEDRRm5PB5v17zcs45zznnOXOEefh+v+ccmSAIAoiIiIgkRu7qBIiIiIiaAoscIiIikiQWOURERCRJLHKIiIhIkljkEBERkSSxyCEiIiJJYpFDREREksQih4iIiCSJRQ4RERFJEoscImqwnTt3QiaTYefOna5O5bYwZ84cyGSyRt3mAw88gAceeKBRt0nU0rDIISKn+vrrrzFnzhxXp0FELQCLHCJyqq+//hpvvfWWq9MgohaARQ4RERFJEoscIgnasWMHZDIZNm3aVGfZ2rVrIZPJkJ2dXa9tnTlzBoMHD4aHhwcCAwMxdepUmEymOnHfffcdnnnmGbRp0wYajQZhYWGYOnUqqqqqxJjExEQsXboUACCTycRXjffffx/9+vWDn58ftFotoqKi8K9//auhhw+DwYBRo0ahdevW0Gg0CAkJwZNPPomTJ086xG3duhX9+/eHl5cXdDodevfujbVr1zbomG5kzZo1iIqKglarha+vL4YPH47Tp0/XiVuxYgU6dOgArVaLPn364LvvvmvwMRNRXUpXJ0BEje+BBx5AWFgYPv/8c/zpT39yWPb555+jQ4cOiImJ+cPtVFVV4aGHHkJ+fj5efvllhIaG4rPPPsP27dvrxG7cuBGVlZWYOHEi/Pz8sG/fPnz00Uc4c+YMNm7cCAB48cUXce7cOWRmZuKzzz6rs43FixfjiSeewIgRI2A2m7Fu3To888wz2LJlC+Lj4+t9/EOGDMHhw4cxefJktGvXDoWFhcjMzER+fj7atWsHAEhLS8Po0aPRrVs3zJw5E97e3jhw4AAyMjLw3HPP1fuYrudvf/sb3nzzTQwdOhRjx45FUVERPvroI9x///04cOAAvL29AQCffvopXnzxRfTr1w9TpkzBb7/9hieeeAK+vr4ICwur9zET0TUIRCRJM2fOFDQajVBSUiLOKywsFJRKpTB79ux6bWPRokUCAGHDhg3ivIqKCqFjx44CAGHHjh3i/MrKyjrrp6SkCDKZTDh16pQ4LykpSbjer56rt2E2m4U777xTePDBB+uVryAIQnFxsQBAeO+9964bU1JSInh5eQnR0dFCVVWVwzK73X7dfATh2sc0e/Zsh2M6efKkoFAohL/97W8O6x48eFBQKpXifLPZLAQGBgqRkZGCyWQS41asWCEAEPr371+/gyaia2J3FZFEjRw5EiaTyaG7Z/369bBarXj++efrtY2vv/4aISEhePrpp8V57u7uGD9+fJ1YrVYrvq+oqMCFCxfQr18/CIKAAwcO1Gt/tbdRXFyM0tJS3Hfffdi/f3+91q/Zhlqtxs6dO1FcXHzNmMzMTJSVlWHGjBlwc3NzWFa7++xmj+mLL76A3W7H0KFDceHCBfEVHByMTp06YceOHQCAH3/8EYWFhZgwYQLUarW4fmJiIvR6fb2PmYiujUUOkUR16dIFvXv3xueffy7O+/zzz9G3b1907NixXts4deoUOnbsWOceMJ07d64Tm5+fj8TERPj6+sLT0xMBAQHo378/AKC0tLRe+9uyZQv69u0LNzc3+Pr6IiAgAMuXL6/3+gCg0Wjw7rvvYuvWrQgKCsL999+P+fPnw2AwiDG//vorAODOO++84bZu9piOHz8OQRDQqVMnBAQEOLx+/vlnFBYWArj8+QJAp06dHNZXqVRo3759vY+ZiK6NY3KIJGzkyJF45ZVXcObMGZhMJuzduxdLlixp9P3YbDY8/PDDuHTpEqZPn44uXbrAw8MDZ8+eRWJiIux2+x9u47vvvsMTTzyB+++/H8uWLUNISAhUKhVWrVrlMBi4PqZMmYLHH38cmzdvxrZt2/Dmm28iJSUF27dvx913393kx2S32yGTybB161YoFIo6yz09PRt0PER0c1jkEEnY8OHDkZycjH/+85+oqqqCSqXCsGHD6r1+27ZtcejQIQiC4NCac+zYMYe4gwcP4pdffsHq1asxcuRIcX5mZmadbV7vzsD//ve/4ebmhm3btkGj0YjzV61aVe98a+vQoQNeffVVvPrqqzh+/DgiIyOxYMECrFmzBh06dAAAHDp06LqtWg05pmvtWxAEhIeH44477rhuXNu2bQFcbvl58MEHxfkWiwV5eXno0aNHvY6ViK6N3VVEEubv749HHnkEa9asweeff45BgwbB39+/3us/+uijOHfunMO4nsrKSqxYscIhrqa1QhAEcZ4gCFi8eHGdbXp4eAAASkpK6mxDJpPBZrOJ806ePInNmzfXO9+a/Kqrqx3mdejQAV5eXuKl7wMHDoSXlxdSUlLqxNYcQ0OO6WpPPfUUFAoF3nrrLYf1a7Zx8eJFAECvXr0QEBCA1NRUmM1mMSYtLa3O50NEDceWHCKJGzlypDhweO7cuQ1ad9y4cViyZAlGjhyJnJwchISE4LPPPoO7u7tDXJcuXdChQwdMmzYNZ8+ehU6nw7///e9rDvyNiooCALz88suIi4uDQqHA8OHDER8fjw8++ACDBg3Cc889h8LCQixduhQdO3bETz/9VO+cf/nlFzz00EMYOnQoIiIioFQqsWnTJhQUFGD48OEAAJ1Oh4ULF2Ls2LHo3bs3nnvuOfj4+OB///sfKisrsXr16gYd09U6dOiAv/71r5g5cyZOnjyJwYMHw8vLC3l5edi0aRPGjx+PadOmQaVS4a9//StefPFFPPjggxg2bBjy8vKwatUqjskhagyuuaiLiJzFZDIJPj4+gl6vr3O5dH2cOnVKeOKJJwR3d3fB399feOWVV4SMjIw6l5AfOXJEiI2NFTw9PQV/f39h3Lhxwv/+9z8BgLBq1Soxzmq1CpMnTxYCAgIEmUzmcOn1p59+KnTq1EnQaDRCly5dhFWrVtW5PPuPXLhwQUhKShK6dOkieHh4CHq9XoiOjna4DL7Gl19+KfTr10/QarWCTqcT+vTpI/zzn/9s8DFdL8d///vfwr333it4eHgIHh4eQpcuXYSkpCTh2LFjDnHLli0TwsPDBY1GI/Tq1Uv49ttvhf79+/MScqJbJBOEq9pSiUhSrFYrQkND8fjjj+PTTz91dTpERE7DMTlEErd582YUFRU5DJ4lImoJ2JJDJFHff/89fvrpJ8ydOxf+/v4ON9Qzm824dOnSDdfX6/UON8O7HZSWlv7hc6OCg4OdlA0R3e448JhIopYvX441a9YgMjISaWlpDsv27NmDAQMG3HD9VatWITExsekSvAmvvPIKVq9efcMY/t1GRDXYkkPUAhUXFyMnJ+eGMd26dUNISIiTMqqfI0eO4Ny5czeMiY2NdVI2RHS7Y5FDREREksSBx0RERCRJLHKIiIhIkljkEBERkSSxyCEiIiJJYpFDREREksQih4iIiCSJRQ4RERFJEoscIiIikiQWOURERCRJLHKIiIhIkljkEBERkSSxyCEiIiJJYpFDREREksQih4iIiCSJRQ4RERFJEoscIiIikiQWOURERCRJLHKIiIhIkljkEBERkSSxyCEiIiJJYpFDREREksQih4iIiCSJRQ4RERFJEoscIiIikiQWOURERCRJLHKIiIhIkljkEBERkSQpXZ2AK9ntdpw7dw5eXl6QyWSuToeIiIjqQRAElJWVITQ0FHL59dtrWnSRc+7cOYSFhbk6DSIiIroJp0+fRuvWra+7vEUXOV5eXgAuf0g6nc7F2RARNb7CwkJs2LABQ4cOBRCIDRuAoUOBwEBXZ0Z084xGI8LCwsTv8euRCYIgOCmn247RaIRer0dpaSmLHCIiomaivt/fHHhMRCRhxcXF2LhxI4qLi1FcDGzcCBQXuzorIudgkUNEJGF5eXkYOnQo8vLykJd3uasqL8/VWRE5B4scIiIikqQWPfCYiIioNkEQYLVaYbPZXJ1Ki6ZQKKBUKm/59i4scoiIiACYzWacP38elZWVrk6FALi7uyMkJARqtfqmt8Eih4hIwrRaLe6++25otVoAwN13A1feUi12ux15eXlQKBQIDQ2FWq3mTWJdRBAEmM1mFBUVIS8vD506dbrhDf9uhEUOEZGEde3aFfv37xena72lWsxmM+x2O8LCwuDu7u7qdFo8rVYLlUqFU6dOwWw2w83N7aa2w4HHREREV9xsiwE1vsY4FzybREQSduDAAWg0Ghw4cAAHDgAaDXDggKuzInIOdlcREUlYzfiGmpvbm81Ay73PPbU0bMkhIiJqxhITEyGTyTBhwoQ6y5KSkiCTyZCYmOj8xG4DLHKIiIiaubCwMKxbtw5VVVXivOrqaqxduxZt2rRxYWauxSKHiIiomevZsyfCwsLwxRdfiPO++OILtGnTBnfffbc4z263IyUlBeHh4dBqtejRowf+9a9/icttNhvGjBkjLu/cuTMWL17ssK/ExEQMHjwY77//PkJCQuDn54ekpCRYLJamP9AG4pgcIiIJ69q1Kw4dOoT27dsDAA4dAq68JYkZPXo0Vq1ahREjRgAAVq5ciVGjRmHnzp1iTEpKCtasWYPU1FR06tQJ3377LZ5//nkEBASgf//+sNvtaN26NTZu3Ag/Pz/s2bMH48ePR0hICIYOHSpuZ8eOHQgJCcGOHTtw4sQJDBs2DJGRkRg3bpyzD/uGZILQcoeg1fdR7UREUtJuRrrD9Ml58S7K5PZRXV2NvLw8hIeH17kny/nz53H+/HmHeT4+PggPD0d1dTWOHDlSZ3s9e/YEABw7dgwVFRUOy9q1awdfX18UFRXh9OnTDstCQkIQEhLSoNwTExNRUlKCv//97wgLC8OxY8cAAF26dMHp06cxduxYeHt74+OPP4avry/+85//ICYmRlx/7NixqKysxNq1a6+5/UmTJsFgMIgtPomJidi5cyd+/fVXKBQKAMDQoUMhl8uxbt26BuV+Izc6J/X9/mZLDhGRhJ06dQpz587Fm2++CaAt5s4FrCotlPqqP1yXLvv444/x1ltvOcwbMWIE1qxZgzNnziAqKqrOOjXtB4mJidi7d6/Dss8++wzPP/88NmzYgEmTJjksmz17NubMmXNTeQYEBCA+Ph5paWkQBAHx8fHw9/cXl584cQKVlZV4+OGHHdYzm80OXVpLly7FypUrkZ+fj6qqKpjNZkRGRjqs061bN7HAAS4XZwcPHrypvJsSixwiIgm7ePEiPv30U7z00ksA2uLTT4HgBBWLnAZ48cUX8cQTTzjM8/HxAQC0bt0aOTk51103LS3tmi05wOXWj9otKgAa3IpztdGjR4uF09KlSx2WlZeXAwDS09PRqlUrh2UajQYAsG7dOkybNg0LFixATEwMvLy88N577+H77793iFepVA7TMpkMdrv9lnJvCixyiIhauNrdV+y6qutGXUhubm5i19S1dO7c+brLAgICEBAQcMv51TZo0CCYzWbIZDLExcU5LIuIiIBGo0F+fj769+9/zfV3796Nfv36XSmKL/v1118bNUdnYpFDRNQCxH/4HYAKAPe5OhVqQgqFAj///LP4vjYvLy9MmzYNU6dOhd1ux7333ovS0lLs3r0bOp0OCQkJ6NSpE/7xj39g27ZtCA8Px2effYYffvgB4eHhrjicW8Yih4iISEJuNBB37ty5CAgIQEpKCn777Td4e3ujZ8+e+POf/wzgctfcgQMHMGzYMMhkMjz77LN46aWXsHXrVmel36h4dRWvriIiCTt79iyWLFmCz4xdALRC2f528Op5Ekov0zXjW2p31Y2u5CHXaIyrq3gzQCIiCWvVqhVSUlKg9PKH0ssEn/7HrlvgEEkNu6uIiCSsrKwMOTk5sJsqAXjBXKCHOqgUco3N1akRNTm25BARSdjx48cxYMAAWIrPwVLsgYJ/xsBS7OHqtIicgi05REQS1W5GOkyGEw1ep0ZLHZ9D0sGWHCIiIpIkFjlEREQkSeyuIiKSMJlCCYWnH2QKJQABCs8qyBQt9s4h1MKwyCEikjB1QDu0Tlp9ZaoMrZO2uzQfImdidxURERFJEoscIiIJMxedxJmlCTAXnYS5yAtnlj4Ic5GXq9OiZmTnzp2QyWQoKSkBcPnJ6t7e3i7Nqb5Y5BARSZhgs8JWfhGCzQrBJoOtXAvBJqvXuu1mpDu86PaUmJgImUyGCRMm1FmWlJQEmUyGxMTERtvfsGHD8MsvvzTa9poSx+QQEUkEC5GWKywsDOvWrcPChQuh1WoBXH7209q1a9GmTZtG3ZdWqxX3cbtjSw4REVEz17NnT4SFheGLL74Q533xxRdo06YN7r77bnGe3W5HSkoKwsPDodVq0aNHD/zrX/9y2NbXX3+NO+64A1qtFgMGDMDJkycdll/dXfXrr7/iySefRFBQEDw9PdG7d2/85z//cVinXbt2eOeddzB69Gh4eXmhTZs2WLFiReN9ANfR4CLn7NmzeP755+Hn5wetVovu3bvjxx9/FJcLgoBZs2YhJCQEWq0WsbGxOH78uMM2Ll26hBEjRkCn08Hb2xtjxoxBeXm5Q8xPP/2E++67D25ubggLC8P8+fPr5LJx40Z06dIFbm5u6N69O77++uuGHg4REZEkjB49GqtWrRKnV65ciVGjRjnEpKSk4B//+AdSU1Nx+PBhTJ06Fc8//zx27doFADh9+jSeeuopPP7448jNzcXYsWMxY8aMG+63vLwcjz76KLKysnDgwAEMGjQIjz/+OPLz8x3iFixYgF69euHAgQN46aWXMHHiRBw7dqyRjv7aGtRdVVxcjHvuuQcDBgzA1q1bERAQgOPHj8PHx0eMmT9/Pj788EOsXr0a4eHhePPNNxEXF4cjR46Ij0ofMWIEzp8/j8zMTFgsFowaNQrjx4/H2rVrAVx+hPrAgQMRGxuL1NRUHDx4EKNHj4a3tzfGjx8PANizZw+effZZpKSk4LHHHsPatWsxePBg7N+/H3feeWdjfT5ERM2ayicUQc++A5VPKIAKBD2bDZVPhavTalbOn7/8qs3HBwgPB6qrgSNH6q7Ts+flf48dAyqu+rjbtQN8fYGiIuD0acdlISGXXzfj+eefx8yZM3Hq1CkAwO7du7Fu3Trs3LkTAGAymfDOO+/gP//5D2JiYgAA7du3x3//+198/PHH6N+/P5YvX44OHTpgwYIFAIDOnTvj4MGDePfdd6+73x49eqBHjx7i9Ny5c7Fp0yZ8+eWXmDRpkjj/0UcfxUsvvQQAmD59OhYuXIgdO3agc+fON3fA9dCgIufdd99FWFiYQ6UYHh4uvhcEAYsWLcIbb7yBJ598EgDwj3/8A0FBQdi8eTOGDx+On3/+GRkZGfjhhx/Qq1cvAMBHH32ERx99FO+//z5CQ0Px+eefw2w2Y+XKlVCr1ejWrRtyc3PxwQcfiEXO4sWLMWjQILz22msALn+omZmZWLJkCVJTU2/tUyEikgi5xh1ube66MmWDW5tLLs2nOfr4Y+CttxznjRgBrFkDnDkDREXVXUe4cr/FxERg717HZZ99Bjz/PLBhA1CrBgAAzJ4NzJlzc3kGBAQgPj4eaWlpEAQB8fHx8Pf3F5efOHEClZWVePjhhx3WM5vNYpfWzz//jOjoaIflNQXR9ZSXl2POnDlIT0/H+fPnYbVaUVVVVacl56677hLfy2QyBAcHo7Cw8KaOtb4aVOR8+eWXiIuLwzPPPINdu3ahVatWeOmllzBu3DgAQF5eHgwGA2JjY8V19Ho9oqOjkZ2djeHDhyM7Oxve3t5igQMAsbGxkMvl+P777/GnP/0J2dnZuP/++6FWq8WYuLg4vPvuuyguLoaPjw+ys7ORnJzskF9cXBw2b9583fxNJhNMJpM4bTQaG3L4RETNjrXsAsr2b4FXz8cAtELZ/nbw6nkSSi/TH65Ll734IvDEE47zajowWrcGcnKuv25a2rVbcgBg6FDg6vrhZltxaowePVpsPVm6dKnDspphIenp6WjVqpXDMo1Gc9P7nDZtGjIzM/H++++jY8eO0Gq1ePrpp2E2mx3iVCqVw7RMJoPdbr/p/dZHg4qc3377DcuXL0dycjL+/Oc/44cffsDLL78MtVqNhIQEGAwGAEBQUJDDekFBQeIyg8GAwMBAxySUSvj6+jrE1G4hqr1Ng8EAHx8fGAyGG+7nWlJSUvDW1eU4EZGE2SpKYNz7L7h3vhdAexj3doR75/MschrgRl1Ibm6/d01dy416YgICLr8a06BBg2A2myGTyRAXF+ewLCIiAhqNBvn5+ejfv/811+/atSu+/PJLh3l7r26Kusru3buRmJiIP/3pTwAuF1NXD1Z2lQYVOXa7Hb169cI777wDALj77rtx6NAhpKamIiEhoUkSbEwzZ850aP0xGo0ICwtzYUZERESNR6FQ4Oeffxbf1+bl5YVp06Zh6tSpsNvtuPfee1FaWordu3dDp9MhISEBEyZMwIIFC/Daa69h7NixyMnJQVpa2g332alTJ3zxxRd4/PHHIZPJ8OabbzZ5C019NejqqpCQEERERDjM69q1q9jvFhwcDAAoKChwiCkoKBCXXasPzmq14tKlSw4x19pG7X1cL6Zm+bVoNBrodDqHFxERkZTc6Ptt7ty5ePPNN5GSkoKuXbti0KBBSE9PF3tP2rRpg3//+9/YvHkzevTogdTUVLFh43o++OAD+Pj4oF+/fnj88ccRFxeHnjdq3nKiBrXk3HPPPXUu9/rll1/Qtm1bAJcHIQcHByMrKwuRkZEALreWfP/995g4cSKAywOYSkpKkJOTg6gro7W2b98Ou90uDnaKiYnBX/7yF1gsFrEPLzMzE507dxav5IqJiUFWVhamTJki5pKZmfmHA6SIiIik5I9aWmqPVZXJZHjllVfwyiuvXDf+sccew2OPPeYwr/al6ImJiQ53UG7Xrh22b3d88GtSUpLD9LW6r3Jzc2+Yd2NoUEvO1KlTsXfvXrzzzjs4ceIE1q5dixUrVogHI5PJMGXKFPz1r3/Fl19+iYMHD2LkyJEIDQ3F4MGDAUCsHMeNG4d9+/Zh9+7dmDRpEoYPH47Q0FAAwHPPPQe1Wo0xY8bg8OHDWL9+PRYvXuzQ1fTKK68gIyMDCxYswNGjRzFnzhz8+OOPDperERG1dAqtDp53DYRCq4NCa4HnXflQaC2uTovIKRrUktO7d29s2rQJM2fOxNtvv43w8HAsWrQII0aMEGNef/11VFRUYPz48SgpKcG9996LjIwM8R45APD5559j0qRJeOihhyCXyzFkyBB8+OGH4nK9Xo9vvvkGSUlJiIqKgr+/P2bNmiVePg4A/fr1w9q1a/HGG2/gz3/+Mzp16oTNmzfzHjlERLUo9YHwe+TlK1NV8HvkoEvzIXImmSDUXM3f8hiNRuj1epSWlnJ8DhE1e9d6dpXdYoK1xACldzAALawl7lB6V0KuavjA0JPz4hshy9tTdXU18vLyEB4e7vBHObnOjc5Jfb+/+ewqIiIJs1w8jfMrk2C5eBqWi544v7I/LBc9b2pbfCI5NTd8CjkRUTPGgoPo+tiSQ0REdEULHsFx22mMc8Eih4iIWrya25VUVla6OBOqUXMurn4cREOwu4qISMJkMhmgUF7+FwAUNtS8pd8pFAp4e3uLN6t1d3f//TMjpxIEAZWVlSgsLIS3t3edOzc3BIscIiIJUwd1QNtpm69MGdF2WoYr07mt1dwxv6mfjE314+3tfcOnGNQHixwiIiJcbvUKCQlBYGAgLBbeMNGVVCrVLbXg1GCRQ0QkYZYLp3Fhy/vwf2wagK64sCUS/o/lQuVf7urUblsKhaJRvmDJ9VjkEBFJmN1qgrngV9itJgBymAv0sFt5zQm1DPyfTkRERJLEIoeIiIgkiUUOERERSRLH5BARSZjSOxj+T8648oDOSvg/mQOlN294Ry0DixwiIglTuHnCo8u9V6as8OhicGk+RM7E7ioiIgmzVRTDuG8TbBXFsFWoYdwXDluF2tVpETkFixwiIgmzll1E8Y5PYS27CGuZG4p3RMBa5ubqtIicgt1VRETUYO1mpIvvT86Ld2EmRNfHIoeIqJmpXWAQ0fWxu4qIiIgkiS05REQSJtd4QNuxD+QaDwBWaDsWQK6xujotIqdgkUNEJGEqnxAEDpl1ZaoSgUN+dGk+RM7E7ioiIgkTbFbYKksh2KwQbDLYKtUQbDJXp0XkFCxyiIgkzFx0Emc+GgFz0UmYi7xw5qOHYS7ycnVaRE7BIoeIiIgkiUUOERERSRKLHCIiIpIkFjlEREQkSbyEnIhIwtSB4QibsgEylQaAEWFTtkGm4n1yqGVgkUNEJGEyuQIyjfvv07wRILUg7K4iIpIwy6WzKFj/JiyXzsJyyR0F6/vAcsn9j1ckkgC25BARSZjdXIXqkwdgN1cBUKL6ZADsZv7qp5aBLTlEREQkSSxyiIiISJLYZklERLek3Yx08f3JefEuzITIEYscIiIJU+oC4PvwBCh1AQCq4fvwISh11a5Oi8gpWOQQEUmYwl0Pr56PXZkyw6vnKZfmQ+RMLHKIiJqB2l1CDWGrKkPVbz9C274XAF9U/RYIbftCKLSWxk2Q6DbEgcdERBJmLS3AxS0LYC0tgLVUi4tbImEt1bo6LSKnYJFDREREksQih4iIiCSJRQ4RERFJEgceExFJmFzlBnVoZ8hVbgBsUIcWQ66yuTotIqdgkUNEJGEqv9YIeWHBlakKhLywx6X5EDkTu6uIiIhIkhpU5MyZMwcymczh1aVLF3F5dXU1kpKS4OfnB09PTwwZMgQFBQUO28jPz0d8fDzc3d0RGBiI1157DVar1SFm586d6NmzJzQaDTp27Ii0tLQ6uSxduhTt2rWDm5sboqOjsW/fvoYcChFRi2AynMCpdx+DyXACJoMOp96Nh8mgc3VaRE7R4Jacbt264fz58+Lrv//9r7hs6tSp+Oqrr7Bx40bs2rUL586dw1NPPSUut9lsiI+Ph9lsxp49e7B69WqkpaVh1qxZYkxeXh7i4+MxYMAA5ObmYsqUKRg7diy2bdsmxqxfvx7JycmYPXs29u/fjx49eiAuLg6FhYU3+zkQERGRxDS4yFEqlQgODhZf/v7+AIDS0lJ8+umn+OCDD/Dggw8iKioKq1atwp49e7B3714AwDfffIMjR45gzZo1iIyMxCOPPIK5c+di6dKlMJvNAIDU1FSEh4djwYIF6Nq1KyZNmoSnn34aCxcuFHP44IMPMG7cOIwaNQoRERFITU2Fu7s7Vq5c2RifCREREUlAg4uc48ePIzQ0FO3bt8eIESOQn58PAMjJyYHFYkFsbKwY26VLF7Rp0wbZ2dkAgOzsbHTv3h1BQUFiTFxcHIxGIw4fPizG1N5GTUzNNsxmM3Jychxi5HI5YmNjxZjrMZlMMBqNDi8iIiKSpgYVOdHR0UhLS0NGRgaWL1+OvLw83HfffSgrK4PBYIBarYa3t7fDOkFBQTAYDAAAg8HgUODULK9ZdqMYo9GIqqoqXLhwATab7ZoxNdu4npSUFOj1evEVFhbWkMMnIiKiZqRBl5A/8sgj4vu77roL0dHRaNu2LTZs2ACt9vZ/FsrMmTORnJwsThuNRhY6RCRpav82CB2/AkovfwDlCB2/A0qvalenReQUt3QJube3N+644w6cOHECwcHBMJvNKCkpcYgpKChAcHAwACA4OLjO1VY1038Uo9PpoNVq4e/vD4VCcc2Ymm1cj0ajgU6nc3gREUmZTKmGyicUMqUaMqUdKp9KyJR2V6dF5BS3VOSUl5fj119/RUhICKKioqBSqZCVlSUuP3bsGPLz8xETEwMAiImJwcGDBx2ugsrMzIROp0NERIQYU3sbNTE121Cr1YiKinKIsdvtyMrKEmOIiOgyS4kBF756H5YSAywlWlz4KhKWktu/5Z2oMTSou2ratGl4/PHH0bZtW5w7dw6zZ8+GQqHAs88+C71ejzFjxiA5ORm+vr7Q6XSYPHkyYmJi0LdvXwDAwIEDERERgRdeeAHz58+HwWDAG2+8gaSkJGg0GgDAhAkTsGTJErz++usYPXo0tm/fjg0bNiA9PV3MIzk5GQkJCejVqxf69OmDRYsWoaKiAqNGjWrEj4aIqPmzV5ej4shOePUeDECFiiOt4NX7NwBVTbK/djPSHaZPzotvkv0Q1UeDipwzZ87g2WefxcWLFxEQEIB7770Xe/fuRUBAAABg4cKFkMvlGDJkCEwmE+Li4rBs2TJxfYVCgS1btmDixImIiYmBh4cHEhIS8Pbbb4sx4eHhSE9Px9SpU7F48WK0bt0an3zyCeLi4sSYYcOGoaioCLNmzYLBYEBkZCQyMjLqDEYmIiKilksmCILg6iRcxWg0Qq/Xo7S0lONziOi2dnULSX2ZDCdgWD0FwQmLAPSEYfV9CE74Dppg59xCgy051BTq+/3NZ1cRERGRJPEp5EREt6Gbbbm5msLTF/p7noXC0xeACfp7foHC09Qo2ya63bHIISKSMKWnL7zvHXFlygTve4+7NB8iZ2J3FRGRhNlNlaj6LQd2UyXsJiWqfvOH3cS/b6llYJFDRCRhluJzKNw4G5bic7AUu6NwYzQsxe6uTovIKVjkEBERkSSxyCEiIiJJYpFDREREksTRZ0REEiZTqKD0DoFMoQJgh9K7AjIFH9BJLQOLHCIiCVMHtEWrF/9+ZaocrV7c6cp0iJyK3VVEREQkSSxyiIgkzFyYh9MfPgdzYR7MhV44/WEszIVerk6LyCnYXUVEJGGC3QZ7lRGC3QZABnuVBoJd5uq0iJyCLTlEREQkSSxyiIiISJJY5BAREZEkcUwOEZGEqXxbIfj596DybQWgAsHP74bKt8LVaRE5BYscIiIJk6u10LTqemXKBk2rElemQ+RU7K4iIpIwq/ECLmX9HVbjBViNbriU1RVWo5ur0yJyCrbkEBFJmK2yBGU//h88ug0A0B5lP7aHR7ezUOqqnbL/djPSxfcn58U7ZZ9ENdiSQ0RERJLEIoeIiIgkiUUOERERSRLH5BAR3SZqj19pLAp3HTzvjofCXQfADM+7T0Lhbm70/RDdjljkEBFJmFIXCL+BE69MVcNv4GGX5kPkTOyuIiKSMLulGibDCdgt1bBb5DAZdLBb+KufWgb+TycikjDLxTMwrJ4Cy8UzsFz0hGH1fbBc9HR1WkROwSKHiIiIJIlFDhEREUkSixwiIiKSJF5dRUQkYTKZHDK1FjLZ5b9pZWoLZDIXJ0XkJCxyiIgkTB3UHm2mbrwyZUSbqd+4NB8iZ2J3FREREUkSixwiIgkzX8jHuU9egvlCPswXPHHuk/thvsBLyKllYHcVEZGECVYzLBfzIVjNAOSwXPSCYOXft9Qy8H86ERERSRKLHCIiIpIkFjlEREQkSRyTQ0QkYSrvYAQ89SZU3sEAKhHw1A9QeVe6Oi0ip2CRQ0QkYXI3T7h3ir4yZYV7p0KX5kPkTOyuIiKSMFt5MUqzN8BWXgxbuQal2R1gK9e4Oi0ip2BLDhGRhFnLL6Lk23/ALbwngLYo+bYL3MKLoPA0OT2XdjPSxfcn58U7ff/U8rAlh4iIiCTploqcefPmQSaTYcqUKeK86upqJCUlwc/PD56enhgyZAgKCgoc1svPz0d8fDzc3d0RGBiI1157DVar1SFm586d6NmzJzQaDTp27Ii0tLQ6+1+6dCnatWsHNzc3REdHY9++fbdyOERERCQhN13k/PDDD/j4449x1113OcyfOnUqvvrqK2zcuBG7du3CuXPn8NRTT4nLbTYb4uPjYTabsWfPHqxevRppaWmYNWuWGJOXl4f4+HgMGDAAubm5mDJlCsaOHYtt27aJMevXr0dycjJmz56N/fv3o0ePHoiLi0NhIQfVERER0U0WOeXl5RgxYgT+/ve/w8fHR5xfWlqKTz/9FB988AEefPBBREVFYdWqVdizZw/27t0LAPjmm29w5MgRrFmzBpGRkXjkkUcwd+5cLF26FGazGQCQmpqK8PBwLFiwAF27dsWkSZPw9NNPY+HCheK+PvjgA4wbNw6jRo1CREQEUlNT4e7ujpUrV97K50FEJClyN0+4d74HcjdPyN0scO98HnI3i6vTInKKmypykpKSEB8fj9jYWIf5OTk5sFgsDvO7dOmCNm3aIDs7GwCQnZ2N7t27IygoSIyJi4uD0WjE4cOHxZirtx0XFyduw2w2IycnxyFGLpcjNjZWjCEioiv3yRk8EyrvYKi8qxAweD9U3lWuTovIKRp8ddW6deuwf/9+/PDDD3WWGQwGqNVqeHt7O8wPCgqCwWAQY2oXODXLa5bdKMZoNKKqqgrFxcWw2WzXjDl69Oh1czeZTDCZfr+iwGg0/sHREhE1rdpXHDUFwWaBraIUCg89ADVsFRooPEyQKYQm3S/R7aBBLTmnT5/GK6+8gs8//xxubm5NlVOTSUlJgV6vF19hYWGuTomIqEmZi07h7PJEmItOwVzkhbPLH4K5yMvVaRE5RYOKnJycHBQWFqJnz55QKpVQKpXYtWsXPvzwQyiVSgQFBcFsNqOkpMRhvYKCAgQHBwMAgoOD61xtVTP9RzE6nQ5arRb+/v5QKBTXjKnZxrXMnDkTpaWl4uv06dMNOXwiIiJqRhpU5Dz00EM4ePAgcnNzxVevXr0wYsQI8b1KpUJWVpa4zrFjx5Cfn4+YmBgAQExMDA4ePOhwFVRmZiZ0Oh0iIiLEmNrbqImp2YZarUZUVJRDjN1uR1ZWlhhzLRqNBjqdzuFFRERE0tSgMTleXl648847HeZ5eHjAz89PnD9mzBgkJyfD19cXOp0OkydPRkxMDPr27QsAGDhwICIiIvDCCy9g/vz5MBgMeOONN5CUlASN5vKtxidMmIAlS5bg9ddfx+jRo7F9+3Zs2LAB6em/910nJycjISEBvXr1Qp8+fbBo0SJUVFRg1KhRt/SBEBERkTQ0+mMdFi5cCLlcjiFDhsBkMiEuLg7Lli0TlysUCmzZsgUTJ05ETEwMPDw8kJCQgLfffluMCQ8PR3p6OqZOnYrFixejdevW+OSTTxAXFyfGDBs2DEVFRZg1axYMBgMiIyORkZFRZzAyERERtUwyQRBa7BB7o9EIvV6P0tJSdl0RkUs0+dVVgh2w2QCFAoAcsMkBhR0yWZPu9g/x2VV0K+r7/c0HdBIRSZhMJgeUtYZfKu2uS4bIyfiATiIiCbNcOgvD2hmwXDoLyyUPGNb2heWSh6vTInIKtuQQEUmY3VwF0+lDsJurAChgOu0Hu1nh6rSInIItOURERCRJLHKIiIhIkljkEBERkSRxTA4RkYQpdQHwHTQZSl0AgCr4DvoJSh2fQk4tA4scIiIJU7jr4dWj5kaqFnj14DP7qOVgkUNEJGG2ylJUHt8L9059Afij8ngw3DsZoHC3uDSvq2+CyJsDUlPgmBwiIgmzGotwKeMjWI1FsBq1uJRxF6xGravTInIKFjlEREQkSSxyiIiISJJY5BAREZEkceAxEZGEydVaaMLuhFytBWCDJuwi5Gqbq9MicgoWOUREEqbybYXg5+ZdmapA8HN7XZoPkTOxyCEicqKrL51uaoJgB2w2QKEAIAdsckBhh0zm1DSIXIJjcoiIJMxc8BvyF/wJ5oLfYC7QIX/BIzAX6FydFpFTsMghIiIiSWKRQ0RERJLEIoeIiIgkiUUOERERSRKvriIikjB1QFu0mpgGhYceQBlaTcyCwsPk6rSInIJFDhGRhMkUKih1/lemBCh11S7Nh8iZ2F1FRCRhlhIDijanwFJigKVEi6LNPWEp4VPIqWVgkUNEJGH26nJUHtsNe3U57NUqVB4Lgb1a5eq0iJyCRQ4RERFJEoscIiIikiQOPCYiIper/Uyvk/PiXZgJSQmLHCIiCVN6+sH7/pFQevoBMMH7/qNQevIScmoZWOQQEUmYwtMH+pihV6ZM0Mf86tJ8iJyJY3KIiCTMXl2OyuPfX7m6SonK44GwV/PvW2oZWOQQEUmYpcSAoi/mXrlPjjuKvugNS4m7q9MicgoWOURERCRJLHKIiIhIkljkEBERkSRx9BkRkYTJlGqo/NpAplQDsEPlVwaZ0u7qtIicgkUOEVETq32jO2dT+7dB6NhlV6bKETr2W5flQuRs7K4iIiIiSWKRQ0QkYeaC35C/8BmYC36DuUCH/IUDYS7QuTotIqdgdxURkYQJgh2CuQqCcHkcjmBWQRBcnBSRk7Alh4iIiCSJRQ4RERFJEoscIiIikiSOySEikjCVX2sEJyyCyq81gHIEJ3wHlV+5q9MicooGteQsX74cd911F3Q6HXQ6HWJiYrB161ZxeXV1NZKSkuDn5wdPT08MGTIEBQUFDtvIz89HfHw83N3dERgYiNdeew1Wq9UhZufOnejZsyc0Gg06duyItLS0OrksXboU7dq1g5ubG6Kjo7Fv376GHAoRUYsgV7lBE9wRcpUb5Co7NMFGyFW8GSC1DA0qclq3bo158+YhJycHP/74Ix588EE8+eSTOHz4MABg6tSp+Oqrr7Bx40bs2rUL586dw1NPPSWub7PZEB8fD7PZjD179mD16tVIS0vDrFmzxJi8vDzEx8djwIAByM3NxZQpUzB27Fhs27ZNjFm/fj2Sk5Mxe/Zs7N+/Hz169EBcXBwKCwtv9fMgIpIUq7EQF79ZDquxEFajGy5+0w1Wo5ur0yJyCpkg3NrFhL6+vnjvvffw9NNPIyAgAGvXrsXTTz8NADh69Ci6du2K7Oxs9O3bF1u3bsVjjz2Gc+fOISgoCACQmpqK6dOno6ioCGq1GtOnT0d6ejoOHTok7mP48OEoKSlBRkYGACA6Ohq9e/fGkiVLAAB2ux1hYWGYPHkyZsyYUe/cjUYj9Ho9SktLodPxvhFE1DRcecdjk+EEDKunIDhhEYCeMKy+D8EJ30ETbHRZTn/k5Lx4V6dAt7n6fn/f9MBjm82GdevWoaKiAjExMcjJyYHFYkFsbKwY06VLF7Rp0wbZ2dkAgOzsbHTv3l0scAAgLi4ORqNRbA3Kzs522EZNTM02zGYzcnJyHGLkcjliY2PFmOsxmUwwGo0OLyIiur20m5EuvohuRYOLnIMHD8LT0xMajQYTJkzApk2bEBERAYPBALVaDW9vb4f4oKAgGAwGAIDBYHAocGqW1yy7UYzRaERVVRUuXLgAm812zZiabVxPSkoK9Hq9+AoLC2vo4RMREVEz0eAip3PnzsjNzcX333+PiRMnIiEhAUeOHGmK3BrdzJkzUVpaKr5Onz7t6pSIiIioiTT4EnK1Wo2OHTsCAKKiovDDDz9g8eLFGDZsGMxmM0pKShxacwoKChAcHAwACA4OrnMVVM3VV7Vjrr4iq6CgADqdDlqtFgqFAgqF4poxNdu4Ho1GA41G09BDJiJqthTu3vDq9SQU7t4AzPDq9RsU7mZXp0XkFLd8M0C73Q6TyYSoqCioVCpkZWWJy44dO4b8/HzExMQAAGJiYnDw4EGHq6AyMzOh0+kQEREhxtTeRk1MzTbUajWioqIcYux2O7KyssQYIiK6TKnzh+9D46DU+UOpq4bvQz9Dqat2dVpETtGglpyZM2fikUceQZs2bVBWVoa1a9di586d2LZtG/R6PcaMGYPk5GT4+vpCp9Nh8uTJiImJQd++fQEAAwcOREREBF544QXMnz8fBoMBb7zxBpKSksQWlgkTJmDJkiV4/fXXMXr0aGzfvh0bNmxAevrvA9CSk5ORkJCAXr16oU+fPli0aBEqKiowatSoRvxoiIiaP7u5Cpaik1AFtAPgCUuRF1QBZZCrba5OjajJNajIKSwsxMiRI3H+/Hno9Xrcdddd2LZtGx5++GEAwMKFCyGXyzFkyBCYTCbExcVh2bJl4voKhQJbtmzBxIkTERMTAw8PDyQkJODtt98WY8LDw5Geno6pU6di8eLFaN26NT755BPExcWJMcOGDUNRURFmzZoFg8GAyMhIZGRk1BmMTETU0lkunYVhzWu/X0K+5p7b/hJyosZyy/fJac54nxwiaiq3y+XPzfE+ObXxnjl0LU1+nxwiIiKi2xmLHCIiIpIkPoWciEjCZHIF5FodZHIFAAFyrQkyeYsdpUAtDIscIiIJUweGI+zltVemyhD28n9cmg+RM7G7ioiIiCSJRQ4RkYSZi07h7MfjYC46BXORJ85+/ADMRZ6uTovIKdhdRUQkYYLNAmvJeQg2CwA5rCUeEGz8+5ZaBv5PJyIiIklikUNERESSxCKHiIiIJIljcoiIJEzlE4rAZ96CyicUQCUCn/keKp9KV6dF5BQscoiIJEyucYe2fdSVKSu07S+4NB8iZ2KRQ0QkYdbySyjP3QrPyEcAhKA8tw08I/Oh9DS5OrV6qf2gUz6skxqKY3KIiCTMVn4Jpbv/CVv5JdjKNSjdfQds5RpXp0XkFCxyiIiISJJY5BAREZEkscghIiIiSeLAYyIiCZO7ecIj4gHI3TwBWOARcRZyN4ur0yJyChY5REQSpvIOhv/j065MVcH/8VxXpkPkVCxyiIgaSe3LnW8XgtUMa9kFKL38AbjBWuYGpVc1ZEq7q1MjanIck0NEJGHmC/k4t2I8zBfyYb7giXMrBsB8wdPVaRE5BYscIiIikiQWOURERCRJLHKIiIhIkljkEBERkSTx6ioiIgnTBHdE2+lbrkwZ0Xb67XcFGFFTYUsOERERSRKLHCIiCbNcPIPzn70Ky8UzsFz0wPnP+sFy0cPVaRE5BburiIgkzG6phvncMdgt1QAUMJ/zgd2icHVaRE7BIoeIiJqFq+8ofXJevIsyoeaC3VVEREQkSSxyiIiISJLYXUVEJGFKfRD8HnsVSn0QgCr4PZYLpb7K1WkROQWLHCIiCVNoveDZbcCVKQs8u511aT5EzsTuKiIiCbNVlqJs/xbYKkthq1SjbH9b2CrVrk6LyCnYkkNEdJOuvtrndmQ1FuFSZirUoV0AhOFS5p1QhxZD4W52dWpETY4tOURERCRJLHKIiIhIkljkEBERkSRxTA4RkYTJ1Vq4tbsbcrUWgBVu7YogV1tdnRaRU7DIISKSMJVvKwQNm3tlqhJBw/a5NB8iZ2J3FRGRhAl2G+ymSgh2GwQ7YDcpIdhdnRWRc7DIISKSMHNhHk4vGgpzYR7MhTqcXhQHc6HO1WkROQWLHCIiIpKkBhU5KSkp6N27N7y8vBAYGIjBgwfj2LFjDjHV1dVISkqCn58fPD09MWTIEBQUFDjE5OfnIz4+Hu7u7ggMDMRrr70Gq9VxINzOnTvRs2dPaDQadOzYEWlpaXXyWbp0Kdq1awc3NzdER0dj3z72NRMREdFlDSpydu3ahaSkJOzduxeZmZmwWCwYOHAgKioqxJipU6fiq6++wsaNG7Fr1y6cO3cOTz31lLjcZrMhPj4eZrMZe/bswerVq5GWloZZs2aJMXl5eYiPj8eAAQOQm5uLKVOmYOzYsdi2bZsYs379eiQnJ2P27NnYv38/evTogbi4OBQWFt7K50FEREQSIRMEQbjZlYuKihAYGIhdu3bh/vvvR2lpKQICArB27Vo8/fTTAICjR4+ia9euyM7ORt++fbF161Y89thjOHfuHIKCggAAqampmD59OoqKiqBWqzF9+nSkp6fj0KFD4r6GDx+OkpISZGRkAACio6PRu3dvLFmyBABgt9sRFhaGyZMnY8aMGfXK32g0Qq/Xo7S0FDod+6iJqGGaw2MdTIYTMKyeguCERQB6wrD6PgQnfAdNsNHVqTWqk/PiXZ0COVF9v79vaUxOaWkpAMDX1xcAkJOTA4vFgtjYWDGmS5cuaNOmDbKzswEA2dnZ6N69u1jgAEBcXByMRiMOHz4sxtTeRk1MzTbMZjNycnIcYuRyOWJjY8WYazGZTDAajQ4vIiIpUwe0Q+vJn0Md0A7qgDK0npwJdUCZq9MicoqbLnLsdjumTJmCe+65B3feeScAwGAwQK1Ww9vb2yE2KCgIBoNBjKld4NQsr1l2oxij0YiqqipcuHABNpvtmjE127iWlJQU6PV68RUWFtbwAyciakZkCiUU7nrIFErIFAIU7mbIFDfdgE/UrNz0zQCTkpJw6NAh/Pe//23MfJrUzJkzkZycLE4bjUYWOkTUIM2hi6o2S/F5FG//O3weHAegA4q3R8DnwSNQ+VS6OjWiJndTLTmTJk3Cli1bsGPHDrRu3VqcHxwcDLPZjJKSEof4goICBAcHizFXX21VM/1HMTqdDlqtFv7+/lAoFNeMqdnGtWg0Guh0OocXEZGU2U0VqDqxD3ZTBewmJapOBMFu4s3uqWVoUJEjCAImTZqETZs2Yfv27QgPD3dYHhUVBZVKhaysLHHesWPHkJ+fj5iYGABATEwMDh486HAVVGZmJnQ6HSIiIsSY2tuoianZhlqtRlRUlEOM3W5HVlaWGENEREQtW4PK+aSkJKxduxb/93//By8vL3H8i16vh1arhV6vx5gxY5CcnAxfX1/odDpMnjwZMTEx6Nu3LwBg4MCBiIiIwAsvvID58+fDYDDgjTfeQFJSEjQaDQBgwoQJWLJkCV5//XWMHj0a27dvx4YNG5Ce/nszcXJyMhISEtCrVy/06dMHixYtQkVFBUaNGtVYnw0RERE1Yw0qcpYvXw4AeOCBBxzmr1q1ComJiQCAhQsXQi6XY8iQITCZTIiLi8OyZcvEWIVCgS1btmDixImIiYmBh4cHEhIS8Pbbb4sx4eHhSE9Px9SpU7F48WK0bt0an3zyCeLi4sSYYcOGoaioCLNmzYLBYEBkZCQyMjLqDEYmIiKilumW7pPT3PE+OUTUUM1t4LGtohgVh3fCo9sDAIJQcbgVPLqdhcLD7OrUGhXvk9Oy1Pf7m6PPiIgkTOHhA12fP12ZMkPXJ8+l+RA5Ex/QSUQkYbbqclQc/S9s1eWwVStRcTQYtmr+fUstA4scIiIJs5YYcOH/5sFaYoC1xB0X/i8K1hJ3V6dF5BQscoiIiEiSWOQQERGRJLHIISIiIkni6DMiIgmTKzVQB3WAXKkBYIc6qBRypd3VaRE5Be+Tw/vkENEfaG73xmnpeM8c6avv9ze7q4iIiEiSWOQQEUmYueBXnHp/MMwFv8JcoMOp9wfBXMCWa2oZOCaHiEjCBEEAbFaIIxNsCrTcQQrU0rAlh4iIiCSJRQ4RERFJEoscIiIikiSOySEikjCVXxhCRi+F0jsYQDlCRu+C0rvS1WkROQWLHCIiCZOrNFAHtL0yZYc6oNyl+RA5E4scIqKrSOnmf9bSQpTuWQd9v+EA2qJ0T0fo+52AUl/l6tSImhzH5BARSZityojyn76BrcoIW5UK5T+1ga1K5eq0iJyCRQ4RERFJEoscIiIikiQWOURERCRJHHhMRCRhCg9v6Po+DYWHNwATdH1PQOFhcnVaTerqgeN8KnnLxSKHiEjClF7+8OmfeGXKBJ/+x1yZDpFTsbuKiEjC7KZKVOf/BLupEnaTAtX5vrCbFK5Oi8gp2JJDRARp3RunNkvxORT8888ITlgEoCcK/hmD4ITvoAk2ujo1oibHlhwiIiKSJBY5REREJEkscoiIiEiSOCaHiEjCZAolFJ5+kCmUAAQoPKsgUwiuTovIKVjkEBFJmDqgHVonrb4yVYbWSdtdmg+RM7G7ioiIiCSJLTlE1GJJ9bLx2sxFJ1G4YTYCh74FoDsKN/RG4NAfoA4oc3VqRE2ORQ4RkYQJNits5Rch2KwAZLCVayHYZK5Oi8gp2F1FREREksSWHCIikrTa3ZJ8WGfLwpYcIiIikiS25BBRi9ISBhvXpvIJRdCz70DlEwqgAkHPZkPlU+HqtIicgkUOEZGEyTXucGtz15UpG9zaXHJpPkTOxO4qIiIJs5ZdQPGuNFjLLsBapkHxrs6wlmlcnRaRU7DIISKSMFtFCYx7/wVbRQlsFRoY93aErYJFDrUMLHKIiIhIkjgmh4gkraUNNCai37Elh4iIiCSpwUXOt99+i8cffxyhoaGQyWTYvHmzw3JBEDBr1iyEhIRAq9UiNjYWx48fd4i5dOkSRowYAZ1OB29vb4wZMwbl5eUOMT/99BPuu+8+uLm5ISwsDPPnz6+Ty8aNG9GlSxe4ubmhe/fu+Prrrxt6OEREkqbQ6uB510AotDootBZ43pUPhdbi6rSInKLBRU5FRQV69OiBpUuXXnP5/Pnz8eGHHyI1NRXff/89PDw8EBcXh+rqajFmxIgROHz4MDIzM7FlyxZ8++23GD9+vLjcaDRi4MCBaNu2LXJycvDee+9hzpw5WLFihRizZ88ePPvssxgzZgwOHDiAwYMHY/DgwTh06FBDD4mISLKU+kD4PfIylPpAKPVV8HvkIJT6KlenReQUMkEQhJteWSbDpk2bMHjwYACXW3FCQ0Px6quvYtq0aQCA0tJSBAUFIS0tDcOHD8fPP/+MiIgI/PDDD+jVqxcAICMjA48++ijOnDmD0NBQLF++HH/5y19gMBigVqsBADNmzMDmzZtx9OhRAMCwYcNQUVGBLVu2iPn07dsXkZGRSE1NrVf+RqMRer0epaWl0Ol0N/sxENFthuNwfme3mGAtMUDpHQxAC2uJO5TelZCr7K5OzeX4iIfmq77f3406JicvLw8GgwGxsbHiPL1ej+joaGRnZwMAsrOz4e3tLRY4ABAbGwu5XI7vv/9ejLn//vvFAgcA4uLicOzYMRQXF4sxtfdTE1OzHyIiAiwXT+P8yiRYLp6G5aInzq/sD8tFT1enReQUjXp1lcFgAAAEBQU5zA8KChKXGQwGBAYGOiahVMLX19chJjw8vM42apb5+PjAYDDccD/XYjKZYDKZxGmj0diQwyMiIqJmpEVdXZWSkgK9Xi++wsLCXJ0SERERNZFGbckJDg4GABQUFCAkJEScX1BQgMjISDGmsLDQYT2r1YpLly6J6wcHB6OgoMAhpmb6j2Jqll/LzJkzkZycLE4bjUYWOkQSwXE4RHS1Rm3JCQ8PR3BwMLKyssR5RqMR33//PWJiYgAAMTExKCkpQU5Ojhizfft22O12REdHizHffvstLJbfL3PMzMxE586d4ePjI8bU3k9NTM1+rkWj0UCn0zm8iIikTCaTAQolZDIZZDIACtvlf4lagAYXOeXl5cjNzUVubi6Ay4ONc3NzkZ+fD5lMhilTpuCvf/0rvvzySxw8eBAjR45EaGioeAVW165dMWjQIIwbNw779u3D7t27MWnSJAwfPhyhoaEAgOeeew5qtRpjxozB4cOHsX79eixevNihFeaVV15BRkYGFixYgKNHj2LOnDn48ccfMWnSpFv/VIiIJEId1AFtp22GOqgD1EFGtJ2WAXUQxyNSy9DgS8h37tyJAQMG1JmfkJCAtLQ0CIKA2bNnY8WKFSgpKcG9996LZcuW4Y477hBjL126hEmTJuGrr76CXC7HkCFD8OGHH8LT8/cR/z/99BOSkpLwww8/wN/fH5MnT8b06dMd9rlx40a88cYbOHnyJDp16oT58+fj0Ucfrfex8BJyouaL3VN0q3gJefNV3+/vW7pPTnPHIoeo+WKRUz+WC6dxYcv78H9sGoCuuLAlEv6P5ULlX/6H60odi5zmq77f33xAJxGRhNmtJpgLfoXdagIgh7lAD7u1RV1YSy0Y/6cTERGRJLElh4iaDXZRUWOq/f+JXVfSxJYcIiIikiS25BARSZjSOxj+T8648oDOSvg/mQOld6Wr0yJyChY5RHRbYxfVrVG4ecKjy71Xpqzw6HL95/sRSQ27q4iIJMxWUQzjvk2wVRTDVqGGcV84bBVqV6dF5BRsySGi2wpbbhqXtewiind8Ck2b7gDaonhHBDRtLkLhYXZ1akRNji05REREJElsySEiohbv6hZEXlIuDSxyiMjl2EVFRE2BRQ4RkYTJNR7QduwDucYDgBXajgWQa6yuTovIKVjkEBFJmMonBIFDZl2ZqkTgkB9dmg+RM7HIISKXYBeVcwg2K+ymiistOSrYTSrINRbIFIKrUyNqcry6iohIwsxFJ3HmoxEwF52EucgLZz56GOYiL1enReQULHKIiIhIkthdRUROwy4qInImtuQQERGRJLHIISIiIkmSCYLQYofYG41G6PV6lJaWQqfTuTodIslh95TrCXYbBIsJMpUGgAKCRQmZygoZ/8StN979+PZT3+9vjskhIpIwmVwBmcb992neCJBaENbyREQSZrl0FgXr34Tl0llYLrmjYH0fWC65//GKRBLAlhwialTsorq92M1VqD55AHZzFQAlqk8GwG7mr35qGdiSQ0RERJLEcp6IiOgGardOchBy88KWHCIiIpIktuQQ0S3jOJzbl1IXAN+HJ0CpCwBQDd+HD0Gpq3Z1WkROwSKHiEjCFO56ePV87MqUGV49T7k0HyJnYpFDRA3Glpvmw1ZVhqrffoS2fS8Avqj6LRDa9oVQaC2uTo2oybHIISKSMGtpAS5uWYDghEUAWuHilkgEJ3zHIucmXV3gcyDy7Y1FDhHVC1tviKi54dVVREREJElsySGi62LrDRE1ZyxyiIgkTK5ygzq0M+QqNwA2qEOLIVfZXJ0WkVPIBEEQXJ2Eq9T3Ue1ELQlbb4huDgchO099v785JoeIiIgkid1VRC0cW26kzWQ4AcPqKVcuIe8Jw+r7EJzwHTTBRlenRtTkWOQQERE1Aj7I8/bDIoeoBWLrDRG1BCxyiFoIFjZE1NKwyCGSKBY1RK7Dxz/cHljkEEkICxu6mtq/DULHr4DSyx9AOULH74DSq9rVaRE5BYscomaOhQ3diEyphson9MqUHSqfSpfm01JxULJrsMghagZYyNDNspQYUPrdGujvex5AOEq/6wz9fceg8q5ydWotFgse52n2Rc7SpUvx3nvvwWAwoEePHvjoo4/Qp08fV6dFVC8sXqip2avLUXFkJ7x6DwagQsWRVvDq/RsAFjkkfc26yFm/fj2Sk5ORmpqK6OhoLFq0CHFxcTh27BgCAwNdnR61cCxgiOiPsFWnaTXrIueDDz7AuHHjMGrUKABAamoq0tPTsXLlSsyYMcPF2ZFUsFghIme40e8aFkA3p9kWOWazGTk5OZg5c6Y4Ty6XIzY2FtnZ2ddcx2QywWQyidOlpaUALj/oi5q3O2dvc3UKRLclu7m61r8VAIywmytgN3EAcnPSZurG6y479FacEzO5PdR8b//RM8abbZFz4cIF2Gw2BAUFOcwPCgrC0aNHr7lOSkoK3nrrrTrzw8LCmiRHIqLbReE/Z9R678JEqNHpF7k6A9cpKyuDXq+/7vJmW+TcjJkzZyI5OVmcttvtuHTpEvz8/CCTyVyYWdMwGo0ICwvD6dOnb/goenIdnqPmgeepeeB5uv011jkSBAFlZWUIDQ29YVyzLXL8/f2hUChQUFDgML+goADBwcHXXEej0UCj0TjM8/b2bqoUbxs6nY4/8Lc5nqPmgeepeeB5uv01xjm6UQtODfkt7cGF1Go1oqKikJWVJc6z2+3IyspCTEyMCzMjIiKi20GzbckBgOTkZCQkJKBXr17o06cPFi1ahIqKCvFqKyIiImq5mnWRM2zYMBQVFWHWrFkwGAyIjIxERkZGncHILZVGo8Hs2bPrdNHR7YPnqHngeWoeeJ5uf84+RzLhj66/IiIiImqGmu2YHCIiIqIbYZFDREREksQih4iIiCSJRQ4RERFJEoucZu7kyZMYM2YMwsPDodVq0aFDB8yePRtms9kh7qeffsJ9990HNzc3hIWFYf78+XW2tXHjRnTp0gVubm7o3r07vv76a2cdRovwt7/9Df369YO7u/t1b0Ipk8nqvNatW+cQs3PnTvTs2RMajQYdO3ZEWlpa0yffgtTnPOXn5yM+Ph7u7u4IDAzEa6+9BqvV6hDD8+Rc7dq1q/OzM2/ePIeY+vwepKa1dOlStGvXDm5uboiOjsa+ffuadH8scpq5o0ePwm634+OPP8bhw4excOFCpKam4s9//rMYYzQaMXDgQLRt2xY5OTl47733MGfOHKxYsUKM2bNnD5599lmMGTMGBw4cwODBgzF48GAcOnTIFYclSWazGc888wwmTpx4w7hVq1bh/Pnz4mvw4MHisry8PMTHx2PAgAHIzc3FlClTMHbsWGzbxgeUNpY/Ok82mw3x8fEwm83Ys2cPVq9ejbS0NMyaNUuM4XlyjbffftvhZ2fy5Mnisvr8HqSmtX79eiQnJ2P27NnYv38/evTogbi4OBQWFjbdTgWSnPnz5wvh4eHi9LJlywQfHx/BZDKJ86ZPny507txZnB46dKgQHx/vsJ3o6GjhxRdfbPqEW5hVq1YJer3+mssACJs2bbruuq+//rrQrVs3h3nDhg0T4uLiGjFDEoTrn6evv/5akMvlgsFgEOctX75c0Ol04s8Yz5PztW3bVli4cOF1l9fn9yA1rT59+ghJSUnitM1mE0JDQ4WUlJQm2ydbciSotLQUvr6+4nR2djbuv/9+qNVqcV5cXByOHTuG4uJiMSY2NtZhO3FxccjOznZO0iRKSkqCv78/+vTpg5UrV0KodSsrnifXy87ORvfu3R1uOhoXFwej0YjDhw+LMTxPzjdv3jz4+fnh7rvvxnvvvefQhVif34PUdMxmM3Jychx+LuRyOWJjY5v056JZ3/GY6jpx4gQ++ugjvP/+++I8g8GA8PBwh7iaX9AGgwE+Pj4wGAx17hQdFBQEg8HQ9EmT6O2338aDDz4Id3d3fPPNN3jppZdQXl6Ol19+GQCue56MRiOqqqqg1WpdkXaLcr1zULPsRjE8T03n5ZdfRs+ePeHr64s9e/Zg5syZOH/+PD744AMA9fs9SE3nwoULsNls1/y5OHr0aJPtly05t6kZM2ZccxBq7dfV/zHOnj2LQYMG4ZlnnsG4ceNclHnLcjPn6UbefPNN3HPPPbj77rsxffp0vP7663jvvfea8AhahsY+T+QcDTlvycnJeOCBB3DXXXdhwoQJWLBgAT766COYTCYXHwW5EltyblOvvvoqEhMTbxjTvn178f25c+cwYMAA9OvXr85AuuDgYBQUFDjMq5kODg6+YUzNcrq2hp6nhoqOjsbcuXNhMpmg0Wiue550Oh1bB26gMc9TcHBwnStC6vvzxPPUMLdy3qKjo2G1WnHy5El07ty5Xr8Hqen4+/tDoVA4/XuGRc5tKiAgAAEBAfWKPXv2LAYMGICoqCisWrUKcrljA11MTAz+8pe/wGKxQKVSAQAyMzPRuXNnsYk2JiYGWVlZmDJlirheZmYmYmJiGueAJKoh5+lm5ObmwsfHR3yYXUxMTJ1L+3me/lhjnqeYmBj87W9/Q2FhIQIDAwFcPgc6nQ4RERFiDM/TrbuV85abmwu5XC6eo/r8HqSmo1arERUVhaysLPGKUbvdjqysLEyaNKnpdtxkQ5rJKc6cOSN07NhReOihh4QzZ84I58+fF181SkpKhKCgIOGFF14QDh06JKxbt05wd3cXPv74YzFm9+7dglKpFN5//33h559/FmbPni2oVCrh4MGDrjgsSTp16pRw4MAB4a233hI8PT2FAwcOCAcOHBDKysoEQRCEL7/8Uvj73/8uHDx4UDh+/LiwbNkywd3dXZg1a5a4jd9++01wd3cXXnvtNeHnn38Wli5dKigUCiEjI8NVhyU5f3SerFarcOeddwoDBw4UcnNzhYyMDCEgIECYOXOmuA2eJ+fas2ePsHDhQiE3N1f49ddfhTVr1ggBAQHCyJEjxZj6/B6kprVu3TpBo9EIaWlpwpEjR4Tx48cL3t7eDlcqNjYWOc3cqlWrBADXfNX2v//9T7j33nsFjUYjtGrVSpg3b16dbW3YsEG44447BLVaLXTr1k1IT0931mG0CAkJCdc8Tzt27BAEQRC2bt0qREZGCp6enoKHh4fQo0cPITU1VbDZbA7b2bFjhxAZGSmo1Wqhffv2wqpVq5x/MBL2R+dJEATh5MmTwiOPPCJotVrB399fePXVVwWLxeKwHZ4n58nJyRGio6MFvV4vuLm5CV27dhXeeecdobq62iGuPr8HqWl99NFHQps2bQS1Wi306dNH2Lt3b5PuTyYIta5PJSIiIpIIXl1FREREksQih4iIiCSJRQ4RERFJEoscIiIikiQWOURERCRJLHKIiIhIkljkEBERkSSxyCEiIiJJYpFDREREksQih4iIiCSJRQ4RERFJEoscIiIikqT/B70zRnhwGk7MAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Example usage\n",
        "x_data, y_data = get_x_data_y_data(_rawData, 'fixed', 0)\n",
        "plot_data_statistics(x_data,'x_data_scaled')\n",
        "plot_data_statistics(y_data,'y_data_scaled')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNvka2cYX0cH"
      },
      "source": [
        "# Scaling Data Using Different Scalers in Python\n",
        "\n",
        "This code snippet demonstrates the use of different scaling techniques from the `sklearn.preprocessing` module to normalize or standardize data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RL4IRI6ZF0ZL",
        "outputId": "7bd4e632-3af7-4cc0-a64c-aad49ecd4361"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "def _getScaledData(data):\n",
        "    \"\"\"\n",
        "    Scale the input data using the MinMaxScaler.\n",
        "\n",
        "    This function creates an instance of MinMaxScaler, fits it to the input data,\n",
        "    and then transforms the data. It scales the features to have a mean of 0 and a\n",
        "    variance of 1, which is a standard practice for many machine learning algorithms\n",
        "    to work effectively. The function returns the scaled data along with the scaler\n",
        "    instance used. This scaler instance can be used later for inverse transformations\n",
        "    or to apply the same scaling to new datasets.\n",
        "\n",
        "    Parameters:\n",
        "    data (array-like): The input data that needs to be scaled. This could be a list,\n",
        "                       numpy array, or a pandas DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the scaled dataset and the scaler instance. The first\n",
        "           element of the tuple is the scaled data, and the second element is the\n",
        "           MinMaxScaler instance that was used for scaling.\n",
        "    \"\"\"\n",
        "    # Create an instance of the MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Fit the scaler to the data. This computes the mean and standard deviation to be used for later scaling.\n",
        "    scaler.fit(data)\n",
        "\n",
        "    # Transform the data using the fitted scaler. This scales the data to a standard scale.\n",
        "    scaledDataSet = scaler.transform(data)\n",
        "\n",
        "    # Return the scaled data and the scaler instance.\n",
        "    # This is useful if you need to inverse transform the scaled data later or apply the same transformation to new data.\n",
        "    return scaledDataSet, scaler\n",
        "\n",
        "\n",
        "plot_data_statistics(_rawData,'Raw Data')\n",
        "\n",
        "scaled_y_data_standard, scaler_y_org = _getScaledData(y_data)\n",
        "print(type(scaled_y_data_standard))\n",
        "plot_data_statistics(scaled_y_data_standard,'scaled_y_data_standard')\n",
        "\n",
        "scaled_x_data_standard, scaler_x_org = _getScaledData(x_data)\n",
        "print(type(scaled_x_data_standard))\n",
        "plot_data_statistics(scaled_x_data_standard,'scaled_x_data_standard')\n",
        "\n",
        "scaled_raw_data_standard, scaler_raw_org = _getScaledData(_rawData)\n",
        "plot_data_statistics(scaled_raw_data_standard,'scaled_raw_data_standard')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4ITlnHoaJPN"
      },
      "source": [
        "# Function for Saving and Downloading DataFrames as Excel Files\n",
        "\n",
        "This Python code snippet saves data sets as Excel files and downloads them, including both the full data sets and selected entries from each data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "p0HWdWy9LDlm",
        "outputId": "db2e57a0-cab6-4451-c6ff-5f5d2cb3591c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_71cabbf6-e712-425b-9031-33ee5c8291ad\", \"raw_data_actual_L1-RSRP_first_AND_last_10_samples.xlsx\", 13540)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_038b68a0-17ec-4da5-b883-31068108d042\", \"x_data_rsrp_first_AND_last_10_samples.xlsx\", 8772)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_afc6f6b0-1283-48ee-8d6c-92250f8a196b\", \"scaled_x_data_standard_first_AND_last_10_samples.xlsx\", 8583)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8a4bd969-59ec-4685-9dd6-3dc7100e474e\", \"y_data_rsrp_first_AND_last_10_samples.xlsx\", 13541)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_714697e6-b173-47bb-a846-a0b7576093c9\", \"scaled_y_data_standard_first_AND_last_10_samples.xlsx\", 13140)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @author: jalal\n",
        "# \"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "def save_and_download_dataframes(data_sets, file_names, num_entries):\n",
        "    \"\"\"\n",
        "    Saves each data set in the data_sets list as an Excel file with specified file names and downloads them.\n",
        "    Additionally, saves and downloads the first and last specified number of entries of each data set as separate Excel files.\n",
        "\n",
        "    Args:\n",
        "    data_sets (list of iterables): List of data sets to be saved and downloaded.\n",
        "    file_names (list of str): List of file names for the full data sets.\n",
        "    num_entries (int): Number of first and last entries to be saved and downloaded for each data set.\n",
        "    \"\"\"\n",
        "\n",
        "    for i, data in enumerate(data_sets):\n",
        "        # Create DataFrame from the provided data set\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Ensure the file names end with '.xlsx'\n",
        "        first_last_file_name = f\"{file_names[i].rstrip('.xlsx')}_first_AND_last_{num_entries}_samples.xlsx\"\n",
        "\n",
        "        # Extract the first and last 'num_entries' from the DataFrame\n",
        "        first_last_entries = pd.concat([df.head(num_entries), df.tail(num_entries)])\n",
        "\n",
        "        # Save the combined first and last entries to a separate Excel file\n",
        "        first_last_entries.to_excel(first_last_file_name, index=False)\n",
        "\n",
        "        # Download the Excel files\n",
        "        files.download(first_last_file_name)\n",
        "\n",
        "# Example usage of the function:\n",
        "data_sets = [_rawData, x_data, scaled_x_data_standard, y_data, scaled_y_data_standard]\n",
        "file_names = ['raw_data_actual_L1-RSRPs', 'x_data_rsrp', 'scaled_x_data_standard', 'y_data_rsrp', 'scaled_y_data_standard']\n",
        "num_entries = 10\n",
        "save_and_download_dataframes(data_sets, file_names, num_entries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnyZOiBbe9E6"
      },
      "source": [
        "# KPIs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VfjTVar7D8bw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class CustomKPI:\n",
        "    def custom_loss(self, ytrue, yhat):\n",
        "        \"\"\"\n",
        "        Custom loss function that weighs the loss based on the frequency of each class in ytrue.\n",
        "        \"\"\"\n",
        "        # Calculating weights for each class based on their frequency in ytrue\n",
        "        weights = ytrue / K.sum(ytrue)\n",
        "        # Normalizing yhat so that the class probabilities of each sample sum to 1\n",
        "        yhat /= K.sum(yhat, axis=-1, keepdims=True)\n",
        "        # Clipping yhat to prevent NaN's and Inf's\n",
        "        yhat = K.clip(yhat, K.epsilon(), 1 - K.epsilon())\n",
        "        # Calculating the weighted loss\n",
        "        loss = ytrue * K.log(yhat) * 100 * weights\n",
        "        loss = -K.sum(loss, -1)\n",
        "        return loss\n",
        "\n",
        "    def top_k_accuracies(self, y_hat, y_true, top_k):\n",
        "        accuracies = {k: 0 for k in top_k}\n",
        "\n",
        "        for predicted_vector, true_vector in zip(y_hat, y_true):\n",
        "            for k in top_k:\n",
        "                # Get top-k indices for both predicted and true vectors\n",
        "                predicted_top_k = set(np.argsort(predicted_vector)[-k:])\n",
        "                true_top_k = set(np.argsort(true_vector)[-k:])\n",
        "\n",
        "                # Check for any overlap between the sets\n",
        "                if len(predicted_top_k.intersection(true_top_k)) > 0:\n",
        "                    accuracies[k] += 1\n",
        "\n",
        "        return {k: accuracy / len(y_true) for k, accuracy in accuracies.items()}\n",
        "\n",
        "\n",
        "\n",
        "    def _rsrpDiff(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Simplified function to calculate the difference in RSRP (Reference Signal Received Power).\n",
        "        \"\"\"\n",
        "        # Get the number of samples\n",
        "        num_samples = y_pred.shape[0]\n",
        "\n",
        "        # Define the k values for which to calculate differences\n",
        "        k_values = [1, 2, 4, 6, 8]\n",
        "\n",
        "        # Initialize a dictionary to store RSRP differences for each k value\n",
        "        topRSRPdiffs = {k: np.zeros(num_samples) for k in k_values}\n",
        "\n",
        "        # Iterate over each sample\n",
        "        for s in range(num_samples):\n",
        "            # Get the corresponding true maximum RSRP value\n",
        "            true_max_RSRP = y_true[s, :].max()\n",
        "\n",
        "            # Iterate over each k value\n",
        "            for k in k_values:\n",
        "                # Get the indices of the top k predictions for the current sample\n",
        "                top_k_idxs = np.argsort(y_pred[s, :])[-k:][::-1]\n",
        "\n",
        "                # Get the maximum RSRP value among the top k predictions\n",
        "                top_k_max_RSRP = y_true[s, top_k_idxs].max()\n",
        "\n",
        "                # Calculate and store the absolute difference between true max RSRP and top k max RSRP\n",
        "                topRSRPdiffs[k][s] = abs(true_max_RSRP - top_k_max_RSRP)\n",
        "        # print the best beam ID!!!\n",
        "        print('true_max_RSRP',true_max_RSRP)\n",
        "        # Return the mean of the RSRP differences for each k value\n",
        "        return [np.mean(topRSRPdiffs[k]) for k in k_values]\n",
        "\n",
        "    def _L1_rsrpDiffs(self, y_pred, y_true, k_values):\n",
        "        \"\"\"\n",
        "        Function to calculate the difference in RSRP (Reference Signal Received Power) for multiple top k values.\n",
        "        \"\"\"\n",
        "        # Initialize a dictionary to store RSRP differences for each k value\n",
        "        topRSRPdiffs = {k: [] for k in k_values}\n",
        "\n",
        "        # Iterate over each sample\n",
        "        for s in range(y_pred.shape[0]):\n",
        "            # Get the corresponding true maximum RSRP value\n",
        "            true_max_RSRP = y_true[s, :].max()\n",
        "\n",
        "            # Iterate over each k value\n",
        "            for k in k_values:\n",
        "                # Get the indices of the top k predictions for the current sample\n",
        "                top_k_idxs = np.argsort(y_pred[s, :])[-k:][::-1]\n",
        "\n",
        "                # Get the maximum RSRP value among the top k predictions\n",
        "                top_k_max_RSRP = y_true[s, top_k_idxs].max()\n",
        "\n",
        "                # Calculate and store the absolute difference between true max RSRP and top k max RSRP\n",
        "                topRSRPdiffs[k].append(abs(true_max_RSRP - top_k_max_RSRP))\n",
        "\n",
        "        # Return the mean of the RSRP differences for each k value\n",
        "        return {k: np.mean(diffs) for k, diffs in topRSRPdiffs.items()}\n",
        "\n",
        "\n",
        "    def _rsrpDiffNaiveBaseline(self, trueRSRPs, samplingPatternStr):\n",
        "        \"\"\"\n",
        "        Function to calculate RSRP difference for a naive baseline method.\n",
        "        \"\"\"\n",
        "        # Initialization\n",
        "        nsamples, total_beams = trueRSRPs.shape\n",
        "        topRSRPdiff = np.zeros(trueRSRPs.shape[0])\n",
        "        topAccuracy = np.zeros(trueRSRPs.shape[0])\n",
        "\n",
        "        if samplingPatternStr=='fixed':\n",
        "\n",
        "            idxs = prepData.GetBPatternIdxs(total_beams, SET_B_LEN, prepData.PRESET_K_FOR_FIXED_BPATTERN)\n",
        "\n",
        "            for s in range(trueRSRPs.shape[0]):\n",
        "                ySweeped = trueRSRPs[s, idxs]\n",
        "                topRSRPdiff[s] = abs(trueRSRPs[s,:].max() - ySweeped.max())\n",
        "\n",
        "                if trueRSRPs[s,:].max()==ySweeped.max():\n",
        "                    topAccuracy[s] = 1\n",
        "\n",
        "        elif samplingPatternStr=='preset':\n",
        "            K1= 16 if trueRSRPs.shape[1] == 256 else 8 if trueRSRPs.shape[1] == 32 else 32 if trueRSRPs.shape[1] == 64*8 else 8\n",
        "            K2 = total_beams//K1 # = int(np.sqrt(total_beams))\n",
        "            #k = int(np.sqrt(total_beams))\n",
        "            allBeamIndices = np.arange(total_beams).reshape(K1, K2) #16, 16)\n",
        "            totalPreset = 5\n",
        "            presets = np.zeros((totalPreset, SET_B_LEN), dtype=int)\n",
        "            for k in range(totalPreset):\n",
        "                presets[k, :] = prepData.GetBPatternIdxs(total_beams, SET_B_LEN, k)\n",
        "            for sample in range(trueRSRPs.shape[0]):\n",
        "                idxs = presets[sample % totalPreset]\n",
        "                ySweeped = trueRSRPs[sample, idxs]\n",
        "                topRSRPdiff[sample] = abs(trueRSRPs[sample, :].max() - ySweeped.max())\n",
        "                if trueRSRPs[sample, :].max() == ySweeped.max():\n",
        "                    topAccuracy[sample] = 1\n",
        "\n",
        "        elif samplingPatternStr=='random':\n",
        "            for s in range(trueRSRPs.shape[0]):\n",
        "                idxs = np.random.choice(total_beams, SET_B_LEN).astype(int)\n",
        "                ySweeped = trueRSRPs[s, idxs]\n",
        "                topRSRPdiff[s] = abs(trueRSRPs[s,:].max() - ySweeped.max())\n",
        "\n",
        "                if trueRSRPs[s,:].max()==ySweeped.max():\n",
        "                    topAccuracy[s] = 1\n",
        "\n",
        "        return topRSRPdiff.mean(), topAccuracy.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWAe3EKRsGRZ",
        "outputId": "9d76631f-59e6-4fc9-cf2a-682f844768a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model custom loss is: tf.Tensor([0.00041745 0.00049366 0.00017353 ... 0.00050402 0.0005514  0.00012076], shape=(524288,), dtype=float64)\n",
            "The Top-k (k=[1,2,4,8]) accurancies are: {1: 0.21852684020996094, 2: 0.3579597473144531, 4: 0.5900382995605469, 8: 0.8606204986572266}\n",
            "true_max_RSRP 0.4406887347692775\n",
            "The RSRP difference is: [0.5482358698916323, 0.4489982156817906, 0.2847460928709124, 0.16618984529185504, 0.09506695778750245]\n",
            "The naive RSRP difference is: (6.245991220563758, 0.24419784545898438)\n",
            "Is my y_data the same is my raw data? True\n",
            "Is my y_data the same is x_data? False\n"
          ]
        }
      ],
      "source": [
        "# Usage example:\n",
        "print(\"The model custom loss is:\", CustomKPI().custom_loss(scaled_y_data_standard, scaled_y_data_standard))\n",
        "print(\"The Top-k (k=[1,2,4,8]) accurancies are:\", CustomKPI().top_k_accuracies(scaled_y_data_standard, scaled_x_data_standard, [1,2,4,8]))\n",
        "print(\"The RSRP difference is:\",CustomKPI()._rsrpDiff(scaled_y_data_standard, scaled_x_data_standard))\n",
        "print(\"The naive RSRP difference is:\",CustomKPI()._rsrpDiffNaiveBaseline(y_data, 'fixed'))\n",
        "print(\"Is my y_data the same is my raw data?\",np.all(scaled_y_data_standard == scaled_raw_data_standard))\n",
        "print(\"Is my y_data the same is x_data?\",np.all(scaled_y_data_standard == scaled_x_data_standard))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hB_h5UhXMs9P",
        "outputId": "bdb0f269-a3ff-4137-81fd-13e8807205cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ==> after reshape: \n",
            "     -> xDataTrain,xScaledData ==> xTrainData=(424673, 32, 1), xDataVal==> xValData=(47186, 32, 1), xTestData=(52429, 32, 1)\n",
            "\n",
            "\n",
            " ==> after reshape: \n",
            "     -> yDataTrain,yScaledData ==> yTrainData=(424673, 32, 1), yDataVal==> yValData=(47186, 32, 1), yTestData=(52429, 32, 1)\n",
            "\n",
            "\n",
            "Indices: [2, 3, 5, 10, 11, 17, 19, 31]\n",
            "yTrainData:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6   \\\n",
              "0       0.406902  0.392012  0.390028  0.457215  0.401168  0.386982  0.450856   \n",
              "1       0.777527  0.711429  0.645584  0.685337  0.789529  0.687604  0.648294   \n",
              "2       0.711286  0.681307  0.752972  0.684181  0.666503  0.661112  0.704636   \n",
              "3       0.599443  0.656779  0.652438  0.689942  0.735209  0.684422  0.667135   \n",
              "4       0.638695  0.594968  0.636943  0.707400  0.634925  0.736688  0.769632   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "424668  0.772937  0.701956  0.600883  0.683433  0.824450  0.759644  0.693519   \n",
              "424669  0.682404  0.692280  0.667273  0.624564  0.654651  0.664439  0.613516   \n",
              "424670  0.632162  0.737093  0.711464  0.671408  0.631695  0.680436  0.646090   \n",
              "424671  0.672567  0.690898  0.696021  0.768375  0.643788  0.622659  0.654690   \n",
              "424672  0.770945  0.715777  0.710649  0.704004  0.808796  0.566945  0.645552   \n",
              "\n",
              "              7         8         9   ...        22        23        24  \\\n",
              "0       0.430801  0.436325  0.387880  ...  0.411647  0.437074  0.399029   \n",
              "1       0.708204  0.771874  0.689922  ...  0.644622  0.732900  0.815446   \n",
              "2       0.668516  0.733276  0.701697  ...  0.721700  0.712694  0.687341   \n",
              "3       0.786497  0.648669  0.604740  ...  0.631862  0.720130  0.701764   \n",
              "4       0.842367  0.614178  0.549434  ...  0.706250  0.755031  0.570652   \n",
              "...          ...       ...       ...  ...       ...       ...       ...   \n",
              "424668  0.686911  0.726741  0.671068  ...  0.566615  0.650046  0.802510   \n",
              "424669  0.631959  0.695639  0.672653  ...  0.619481  0.648907  0.617844   \n",
              "424670  0.609568  0.650943  0.677125  ...  0.685677  0.681701  0.617377   \n",
              "424671  0.674555  0.640582  0.610347  ...  0.670599  0.721592  0.642292   \n",
              "424672  0.728701  0.821590  0.668461  ...  0.641548  0.761739  0.796355   \n",
              "\n",
              "              25        26        27        28        29        30        31  \n",
              "0       0.378055  0.483082  0.452112  0.405860  0.403140  0.440933  0.484721  \n",
              "1       0.739966  0.691955  0.715568  0.736122  0.669998  0.651836  0.651237  \n",
              "2       0.669075  0.743057  0.667719  0.710833  0.691024  0.790427  0.683787  \n",
              "3       0.654651  0.662740  0.749254  0.604362  0.624862  0.657597  0.662135  \n",
              "4       0.664444  0.702813  0.766688  0.727392  0.662770  0.757457  0.805198  \n",
              "...          ...       ...       ...       ...       ...       ...       ...  \n",
              "424668  0.738192  0.675667  0.657159  0.684970  0.630829  0.571534  0.602619  \n",
              "424669  0.643519  0.630881  0.620515  0.669455  0.683276  0.686585  0.590760  \n",
              "424670  0.665863  0.646974  0.606469  0.634668  0.720018  0.694595  0.642989  \n",
              "424671  0.628853  0.674823  0.638445  0.644152  0.671320  0.706751  0.745216  \n",
              "424672  0.565831  0.636776  0.710073  0.745206  0.693924  0.709269  0.683709  \n",
              "\n",
              "[424673 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8dc545b5-b92d-48a3-bf0d-56fb8865a1cd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.406902</td>\n",
              "      <td>0.392012</td>\n",
              "      <td>0.390028</td>\n",
              "      <td>0.457215</td>\n",
              "      <td>0.401168</td>\n",
              "      <td>0.386982</td>\n",
              "      <td>0.450856</td>\n",
              "      <td>0.430801</td>\n",
              "      <td>0.436325</td>\n",
              "      <td>0.387880</td>\n",
              "      <td>...</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.437074</td>\n",
              "      <td>0.399029</td>\n",
              "      <td>0.378055</td>\n",
              "      <td>0.483082</td>\n",
              "      <td>0.452112</td>\n",
              "      <td>0.405860</td>\n",
              "      <td>0.403140</td>\n",
              "      <td>0.440933</td>\n",
              "      <td>0.484721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.777527</td>\n",
              "      <td>0.711429</td>\n",
              "      <td>0.645584</td>\n",
              "      <td>0.685337</td>\n",
              "      <td>0.789529</td>\n",
              "      <td>0.687604</td>\n",
              "      <td>0.648294</td>\n",
              "      <td>0.708204</td>\n",
              "      <td>0.771874</td>\n",
              "      <td>0.689922</td>\n",
              "      <td>...</td>\n",
              "      <td>0.644622</td>\n",
              "      <td>0.732900</td>\n",
              "      <td>0.815446</td>\n",
              "      <td>0.739966</td>\n",
              "      <td>0.691955</td>\n",
              "      <td>0.715568</td>\n",
              "      <td>0.736122</td>\n",
              "      <td>0.669998</td>\n",
              "      <td>0.651836</td>\n",
              "      <td>0.651237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.711286</td>\n",
              "      <td>0.681307</td>\n",
              "      <td>0.752972</td>\n",
              "      <td>0.684181</td>\n",
              "      <td>0.666503</td>\n",
              "      <td>0.661112</td>\n",
              "      <td>0.704636</td>\n",
              "      <td>0.668516</td>\n",
              "      <td>0.733276</td>\n",
              "      <td>0.701697</td>\n",
              "      <td>...</td>\n",
              "      <td>0.721700</td>\n",
              "      <td>0.712694</td>\n",
              "      <td>0.687341</td>\n",
              "      <td>0.669075</td>\n",
              "      <td>0.743057</td>\n",
              "      <td>0.667719</td>\n",
              "      <td>0.710833</td>\n",
              "      <td>0.691024</td>\n",
              "      <td>0.790427</td>\n",
              "      <td>0.683787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.599443</td>\n",
              "      <td>0.656779</td>\n",
              "      <td>0.652438</td>\n",
              "      <td>0.689942</td>\n",
              "      <td>0.735209</td>\n",
              "      <td>0.684422</td>\n",
              "      <td>0.667135</td>\n",
              "      <td>0.786497</td>\n",
              "      <td>0.648669</td>\n",
              "      <td>0.604740</td>\n",
              "      <td>...</td>\n",
              "      <td>0.631862</td>\n",
              "      <td>0.720130</td>\n",
              "      <td>0.701764</td>\n",
              "      <td>0.654651</td>\n",
              "      <td>0.662740</td>\n",
              "      <td>0.749254</td>\n",
              "      <td>0.604362</td>\n",
              "      <td>0.624862</td>\n",
              "      <td>0.657597</td>\n",
              "      <td>0.662135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.638695</td>\n",
              "      <td>0.594968</td>\n",
              "      <td>0.636943</td>\n",
              "      <td>0.707400</td>\n",
              "      <td>0.634925</td>\n",
              "      <td>0.736688</td>\n",
              "      <td>0.769632</td>\n",
              "      <td>0.842367</td>\n",
              "      <td>0.614178</td>\n",
              "      <td>0.549434</td>\n",
              "      <td>...</td>\n",
              "      <td>0.706250</td>\n",
              "      <td>0.755031</td>\n",
              "      <td>0.570652</td>\n",
              "      <td>0.664444</td>\n",
              "      <td>0.702813</td>\n",
              "      <td>0.766688</td>\n",
              "      <td>0.727392</td>\n",
              "      <td>0.662770</td>\n",
              "      <td>0.757457</td>\n",
              "      <td>0.805198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424668</th>\n",
              "      <td>0.772937</td>\n",
              "      <td>0.701956</td>\n",
              "      <td>0.600883</td>\n",
              "      <td>0.683433</td>\n",
              "      <td>0.824450</td>\n",
              "      <td>0.759644</td>\n",
              "      <td>0.693519</td>\n",
              "      <td>0.686911</td>\n",
              "      <td>0.726741</td>\n",
              "      <td>0.671068</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566615</td>\n",
              "      <td>0.650046</td>\n",
              "      <td>0.802510</td>\n",
              "      <td>0.738192</td>\n",
              "      <td>0.675667</td>\n",
              "      <td>0.657159</td>\n",
              "      <td>0.684970</td>\n",
              "      <td>0.630829</td>\n",
              "      <td>0.571534</td>\n",
              "      <td>0.602619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424669</th>\n",
              "      <td>0.682404</td>\n",
              "      <td>0.692280</td>\n",
              "      <td>0.667273</td>\n",
              "      <td>0.624564</td>\n",
              "      <td>0.654651</td>\n",
              "      <td>0.664439</td>\n",
              "      <td>0.613516</td>\n",
              "      <td>0.631959</td>\n",
              "      <td>0.695639</td>\n",
              "      <td>0.672653</td>\n",
              "      <td>...</td>\n",
              "      <td>0.619481</td>\n",
              "      <td>0.648907</td>\n",
              "      <td>0.617844</td>\n",
              "      <td>0.643519</td>\n",
              "      <td>0.630881</td>\n",
              "      <td>0.620515</td>\n",
              "      <td>0.669455</td>\n",
              "      <td>0.683276</td>\n",
              "      <td>0.686585</td>\n",
              "      <td>0.590760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424670</th>\n",
              "      <td>0.632162</td>\n",
              "      <td>0.737093</td>\n",
              "      <td>0.711464</td>\n",
              "      <td>0.671408</td>\n",
              "      <td>0.631695</td>\n",
              "      <td>0.680436</td>\n",
              "      <td>0.646090</td>\n",
              "      <td>0.609568</td>\n",
              "      <td>0.650943</td>\n",
              "      <td>0.677125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.685677</td>\n",
              "      <td>0.681701</td>\n",
              "      <td>0.617377</td>\n",
              "      <td>0.665863</td>\n",
              "      <td>0.646974</td>\n",
              "      <td>0.606469</td>\n",
              "      <td>0.634668</td>\n",
              "      <td>0.720018</td>\n",
              "      <td>0.694595</td>\n",
              "      <td>0.642989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424671</th>\n",
              "      <td>0.672567</td>\n",
              "      <td>0.690898</td>\n",
              "      <td>0.696021</td>\n",
              "      <td>0.768375</td>\n",
              "      <td>0.643788</td>\n",
              "      <td>0.622659</td>\n",
              "      <td>0.654690</td>\n",
              "      <td>0.674555</td>\n",
              "      <td>0.640582</td>\n",
              "      <td>0.610347</td>\n",
              "      <td>...</td>\n",
              "      <td>0.670599</td>\n",
              "      <td>0.721592</td>\n",
              "      <td>0.642292</td>\n",
              "      <td>0.628853</td>\n",
              "      <td>0.674823</td>\n",
              "      <td>0.638445</td>\n",
              "      <td>0.644152</td>\n",
              "      <td>0.671320</td>\n",
              "      <td>0.706751</td>\n",
              "      <td>0.745216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424672</th>\n",
              "      <td>0.770945</td>\n",
              "      <td>0.715777</td>\n",
              "      <td>0.710649</td>\n",
              "      <td>0.704004</td>\n",
              "      <td>0.808796</td>\n",
              "      <td>0.566945</td>\n",
              "      <td>0.645552</td>\n",
              "      <td>0.728701</td>\n",
              "      <td>0.821590</td>\n",
              "      <td>0.668461</td>\n",
              "      <td>...</td>\n",
              "      <td>0.641548</td>\n",
              "      <td>0.761739</td>\n",
              "      <td>0.796355</td>\n",
              "      <td>0.565831</td>\n",
              "      <td>0.636776</td>\n",
              "      <td>0.710073</td>\n",
              "      <td>0.745206</td>\n",
              "      <td>0.693924</td>\n",
              "      <td>0.709269</td>\n",
              "      <td>0.683709</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>424673 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8dc545b5-b92d-48a3-bf0d-56fb8865a1cd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8dc545b5-b92d-48a3-bf0d-56fb8865a1cd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8dc545b5-b92d-48a3-bf0d-56fb8865a1cd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7d139f0a-d4a5-43e1-b222-92b6ffd1c6b5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7d139f0a-d4a5-43e1-b222-92b6ffd1c6b5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7d139f0a-d4a5-43e1-b222-92b6ffd1c6b5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xTrainData:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         0    1         2         3    4         5    6    7    8    9   ...  \\\n",
              "0       0.0  0.0  0.390028  0.457215  0.0  0.386982  0.0  0.0  0.0  0.0  ...   \n",
              "1       0.0  0.0  0.645584  0.685337  0.0  0.687604  0.0  0.0  0.0  0.0  ...   \n",
              "2       0.0  0.0  0.752972  0.684181  0.0  0.661112  0.0  0.0  0.0  0.0  ...   \n",
              "3       0.0  0.0  0.652438  0.689942  0.0  0.684422  0.0  0.0  0.0  0.0  ...   \n",
              "4       0.0  0.0  0.636943  0.707400  0.0  0.736688  0.0  0.0  0.0  0.0  ...   \n",
              "...     ...  ...       ...       ...  ...       ...  ...  ...  ...  ...  ...   \n",
              "424668  0.0  0.0  0.600883  0.683433  0.0  0.759644  0.0  0.0  0.0  0.0  ...   \n",
              "424669  0.0  0.0  0.667273  0.624564  0.0  0.664439  0.0  0.0  0.0  0.0  ...   \n",
              "424670  0.0  0.0  0.711464  0.671408  0.0  0.680436  0.0  0.0  0.0  0.0  ...   \n",
              "424671  0.0  0.0  0.696021  0.768375  0.0  0.622659  0.0  0.0  0.0  0.0  ...   \n",
              "424672  0.0  0.0  0.710649  0.704004  0.0  0.566945  0.0  0.0  0.0  0.0  ...   \n",
              "\n",
              "         22   23   24   25   26   27   28   29   30        31  \n",
              "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.484721  \n",
              "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.651237  \n",
              "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.683787  \n",
              "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.662135  \n",
              "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.805198  \n",
              "...     ...  ...  ...  ...  ...  ...  ...  ...  ...       ...  \n",
              "424668  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.602619  \n",
              "424669  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.590760  \n",
              "424670  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.642989  \n",
              "424671  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.745216  \n",
              "424672  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.683709  \n",
              "\n",
              "[424673 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c18a0a08-a67b-44c8-acf0-75c1f2856331\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.390028</td>\n",
              "      <td>0.457215</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.386982</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.484721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.645584</td>\n",
              "      <td>0.685337</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.687604</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.651237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.752972</td>\n",
              "      <td>0.684181</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.661112</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.683787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.652438</td>\n",
              "      <td>0.689942</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.684422</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.662135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.636943</td>\n",
              "      <td>0.707400</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.736688</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.805198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424668</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.600883</td>\n",
              "      <td>0.683433</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.759644</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.602619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424669</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.667273</td>\n",
              "      <td>0.624564</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.664439</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.590760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424670</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.711464</td>\n",
              "      <td>0.671408</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.680436</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.642989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424671</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.696021</td>\n",
              "      <td>0.768375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.622659</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.745216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424672</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.710649</td>\n",
              "      <td>0.704004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.566945</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.683709</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>424673 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c18a0a08-a67b-44c8-acf0-75c1f2856331')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c18a0a08-a67b-44c8-acf0-75c1f2856331 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c18a0a08-a67b-44c8-acf0-75c1f2856331');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-96687752-7595-43d9-a849-a188372b4875\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-96687752-7595-43d9-a849-a188372b4875')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-96687752-7595-43d9-a849-a188372b4875 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xTest:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHNCAYAAADyqRSQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOE0lEQVR4nO3de1xUdeI+8Ge4zHAdQJGroKgIXvCCroRa1saKu6zl1qZr6qppVuJvS7Y0Ky/VfsXVtLay7LJlW5npfq1MTSMvsSlZcVEEREwQRQEVmAHlMjCf3x9+OeuIItjAZ5zzvF+veeGc85lznjmY83TmXDRCCAEiIiIiFXKQHYCIiIhIFhYhIiIiUi0WISIiIlItFiEiIiJSLRYhIiIiUi0WISIiIlItFiEiIiJSLRYhIiIiUi0WISIiIlItFiEiIiJSLRYhIrolFBUVQaPRtOlRVFT0i9d35swZLFu2DFlZWb94WURku5xkByAiaotu3brhww8/tJi2evVqnD59Gi+//HKLsb/UmTNn8Pzzz6Nnz54YMmTIL14eEdkmFiEiuiW4u7tj6tSpFtM2btyIysrKFtOJiNqKX40RkU2ora1FZGQkIiMjUVtbq0yvqKhAYGAgRo4ciaamphsup76+HkuXLkWfPn2g0+kQEhKCBQsWoL6+3mJcSkoKRo8eDW9vb3h4eCAiIgLPPPMMAGDfvn341a9+BQCYOXOm8pXb+vXrrfeGicgmcI8QEdkEV1dXfPDBBxg1ahSeffZZrFmzBgCQmJgIg8GA9evXw9HRsdVlmM1m3HPPPfjuu+8wZ84c9OvXD9nZ2Xj55Zdx7NgxfP755wCAnJwc/P73v8egQYPwwgsvQKfT4fjx49i/fz8AoF+/fnjhhRewZMkSzJkzB7fffjsAYOTIkR23AYhIChYhIrIZMTExWLBgAf7+97/jD3/4A8rKyrBx40a88sor6Nu37w1fv2HDBnzzzTf49ttvMXr0aGX6wIED8eijj+LAgQMYOXIkUlJS0NDQgK+++gq+vr4tluPv74/f/va3WLJkCWJjY/nVG5Ed41djRGRTli1bhgEDBmD69OmYO3cuxowZg7/85S9teu3mzZvRr18/REZG4vz588rj17/+NQBg7969AABvb28AwBdffAGz2dwh74OIbg0sQkRkU7RaLd577z0UFhaiuroa77//PjQaTZteW1BQgJycHHTr1s3i0bw3qby8HAAwadIkjBo1CrNnz4a/vz/+9Kc/YdOmTSxFRCrEr8aIyObs2rULAFBXV4eCggKEhYW16XVmsxlRUVHK8UVXCwkJAXD5eKTU1FTs3bsX27dvx86dO/Hpp5/i17/+Nb7++usbHotERPaDRYiIbMrhw4fxwgsvYObMmcjKysLs2bORnZ0NLy+vG762d+/eOHToEO6+++4b7kVycHDA3Xffjbvvvhtr1qzB8uXL8eyzz2Lv3r2Ii4tr814oIrq18asxIrIZJpMJM2bMQFBQEP7xj39g/fr1KCsrw/z589v0+okTJ6KkpATvvPNOi3m1tbW4ePEigMun5F+t+aKJzafZu7u7AwCqqqpu4p0Q0a2Ce4SIyGb87W9/Q1ZWFnbv3g1PT08MGjQIS5YswXPPPYc//vGP+N3vftfq66dNm4ZNmzbh0Ucfxd69ezFq1Cg0NTXh6NGj2LRpE3bt2oXhw4fjhRdeQGpqKhISEtCjRw+Ul5fjjTfeQPfu3ZWzzXr37g1vb2+sW7cOnp6ecHd3R0xMTJu/piOiW4NGCCFkhyAiysjIQExMDB577DG8+uqryvSmpibExsaipKQEOTk5yhlfAPD73/8eR44csbi3mMlkwssvv4x//etfOH78ONzc3NCrVy/cc889eOKJJ6DX67Fnzx68+uqr+PHHH3H+/Hn4+vpizJgxeP755xEeHq4sa+vWrVi0aBGOHTuGxsZGvP/++5gxY0YnbA0i6iwsQkRERKRaPEaIiIiIVItFiIiIiFSLRYiIiIhUi0WIiIiIVItFiIiIiFSLRYiIiIhUi0WIiIiIVItFiIiIiFSLRYiIiIhUi0WIiIiIVItFiIiIiFSLRYiIiIhUi0WIiIiIVItFiIiIiFSLRYiIiIhUi0WIiIiIVItFiIiIiFSLRYiIiIhUi0WIiIiIVItFiIiIiFSLRYiIiIhUi0WIiIiIVItFiIiIiFSLRYiIiIhUi0WIiIiIVItFiIiIiFSLRYiIiIhUi0WIiIiIVItFiIiIiFSLRYiIiIhUi0WIiIiIVItFiIiIiFTLSXYAW2Y2m3HmzBl4enpCo9HIjkNERERtIIRAdXU1goKC4ODQ+j4fFqFWnDlzBiEhIbJjEBER0U04deoUunfv3uoYFqFWeHp6Ari8IfV6vVWXXV4ObNoETJwI+PlZddF2pby8HJs2bcLEiRPhxw1FRERtYDQaERISonyOt0YjhBCdkOmWZDQa4eXlBYPBYPUiRERERB2jPZ/fPFhakspKYPPmyz/p+iorK7F582ZUckMREVEHYBGSpLDw8tdihYWyk9i2wsJCTJw4EYXcUERE1AFYhIiIiEi1eLA0ERFROwgh0NjYiKamJtlRVM3Z2RmOjo6/eDksQkRERG3U0NCAs2fP4tKlS7KjqJ5Go0H37t3h4eHxi5bDIiSJqyswdOjln3R9rq6uGDp0KFy5oYhIMrPZjMLCQjg6OiIoKAharZYX25VECIFz587h9OnTCA8P/0V7hliEJOnXD8jIkJ3C9vXr1w8Z3FBEZAMaGhpgNpsREhICNzc32XFUr1u3bigqKoLJZPpFRYgHSxMREbXDjW7ZQJ3DWnvj+NuUJDMT0Oku/6Try8zMhE6nQyY3FBERdQAWIUmEABoaLv+k6xNCoKGhAbwAOhERdQQWISIiIjs3Y8YMaDQaPProoy3mJSYmQqPRYMaMGZ0fzAawCBEREalASEgINm7ciNraWmVaXV0dNmzYgNDQUInJ5GIRIiIiUoHo6GiEhIRgy5YtyrQtW7YgNDQUQ4cOVaaZzWYkJycjLCwMrq6uGDx4MP79738r85uamjBr1ixlfkREBP7xj39YrGvGjBmYMGECXnrpJQQGBqJr165ITEyEyWTq+DfaTjx9XpJ+/YAjR4BevWQnsW39+vXDkSNH0IsbiojoF3vooYfw/vvvY8qUKQCA9957DzNnzsS+ffuUMcnJyfjoo4+wbt06hIeHIzU1FVOnTkW3bt0wZswYmM1mdO/eHZs3b0bXrl1x4MABzJkzB4GBgZg4caKynL179yIwMBB79+7F8ePHMWnSJAwZMgQPP/xwZ7/tVmkEj0K9LqPRCC8vLxgMBuj1eqsvv+fT25U/F61IsPryiYjIeurq6lBYWIiwsDC4uLgo08+ePYuzZ89ajPXx8UFYWBjq6uqQm5vbYlnR0dEAgPz8fFy8eNFiXs+ePdGlSxecO3cOp06dspjn6emJ8PDwdmefMWMGqqqq8M477yAkJAT5+fkAgMjISJw6dQqzZ8+Gt7c33nrrLXTp0gXffPMNYmNjldfPnj0bly5dwoYNG665/Hnz5qG0tFTZczRjxgzs27cPP//8s3KNn4kTJ8LBwQEbN25sd/5rud7vA2jf5zf3CEly8iRw4asoeI08Diev2hu/QKVOnjyJF198EYsXL0aPHj1kxyEiauGtt97C888/bzFtypQp+Oijj3D69GkMGzasxWua90HMmDED33//vcW8Dz/8EFOnTsWmTZswb948i3ljx47Frl27bjprt27dkJCQgPXr10MIgYSEBPj6+irzjx8/jkuXLuE3v/mNxesaGhosvj5bu3Yt3nvvPRQXF6O2thYNDQ0YMmSIxWsGDBhgcaHDwMBAZGdn33T2jsIiJMmFC0DN4VB4DD3JItSKCxcu4J///Cfmzp3LIkRENumRRx7BPffcYzHNx8cHANC9e3ekp6df97Xr16+/5h4h4PIelCv3ygCX9wj9Ug899JBSsNauXWsxr6amBgCwfft2BAcHW8zT6XQAgI0bN+LJJ5/E6tWrERsbC09PT6xatQoHDx60GO/s7GzxXKPRwGw2/+L81sYiRERE9AsEBgYiMDDwmvNcXFyUr8GuJSIi4rrzunXrhm7duv3ifFcbN24cGhoaoNFoEB8fbzGvf//+0Ol0KC4uxpgxY675+v3792PkyJGYO3euMu3nn3+2es7OwiJERESkIo6OjsjLy1P+fCVPT088+eSTmD9/PsxmM0aPHg2DwYD9+/dDr9dj+vTpCA8Px7/+9S/s2rULYWFh+PDDD/Hjjz8iLCxMxtv5xViEiIiIVKa1A4hffPFFdOvWDcnJyThx4gS8vb0RHR2NZ555BsDlrwIzMzMxadIkaDQaTJ48GXPnzsVXX33VWfGtimeNtaIjzxorKQH6//E4PKOL4ORZz7PGrqOkpASvv/465s2b1+L7aiKiztTaWUrU+XjW2C0uOBjwGZMvO4bNCw4ORnJysuwYRERkp3hlaUmqq4G64i4w1zveeLCKVVdXY9++faiurpYdhYiI7BCLkCQFBUDZJ7EwVbrLjmLTCgoKcNddd6GgoEB2FCIiskMsQkRERKRaLEJERESkWixCREREpFrtLkKpqakYP348goKCoNFo8PnnnyvzTCYTFi5ciKioKLi7uyMoKAh//vOfcebMGYtlVFRUYMqUKdDr9fD29sasWbOUy3o3O3z4MG6//Xa4uLggJCQEK1eubJFl8+bNiIyMhIuLC6KiorBjxw6L+UIILFmyBIGBgXB1dUVcXJzNHGvi7Aw4etRC48irF7TG2dkZwcHBLS7VTkREZA3tLkIXL17E4MGDW9yfBAAuXbqEjIwMLF68GBkZGdiyZQvy8/Nb3INlypQpyMnJQUpKCrZt24bU1FTMmTNHmW80GjF27Fj06NED6enpWLVqFZYtW4a3335bGXPgwAFMnjwZs2bNQmZmJiZMmIAJEybgyJEjypiVK1fi1Vdfxbp163Dw4EG4u7sjPj4edXV17X3bVhcVBXRP3ANtN54N1ZqoqCicPn0aUVFRsqMQEZE9Er8AAPHZZ5+1OuaHH34QAMTJkyeFEELk5uYKAOLHH39Uxnz11VdCo9GIkpISIYQQb7zxhvDx8RH19fXKmIULF4qIiAjl+cSJE0VCQoLFumJiYsQjjzwihBDCbDaLgIAAsWrVKmV+VVWV0Ol04pNPPmnT+zMYDAKAMBgMbRrfXj0WblMeRERk22pra0Vubq6ora2VHYVE67+P9nx+d/gxQgaDARqNBt7e3gCAtLQ0eHt7Y/jw4cqYuLg4ODg4KHeuTUtLwx133AGtVquMiY+PR35+PiorK5UxcXFxFuuKj49HWloaAKCwsBClpaUWY7y8vBATE6OMuVp9fT2MRqPFo6NkZwOn1/4aDed++Z2E7Vl2dja6d++O7Oxs2VGIiOg69u3bB41Gg6qqKgDA+vXrlc99W9ehRaiurg4LFy7E5MmTlUtcl5aWws/Pz2Kck5MTunTpgtLSUmWMv7+/xZjm5zcac+X8K193rTFXS05OhpeXl/IICQlp93tuK5MJaKpxhWjSdNg67IHJZEJJSQlMJpPsKEREt6wZM2ZAo9Hg0UcfbTEvMTERGo0GM2bMsNr6Jk2ahGPHjllteR2pw4qQyWTCxIkTIYTAm2++2VGrsapFixbBYDAoj1OnTsmOREREZBUhISHYuHEjamtrlWl1dXXYsGEDQkNDrbouV1fXFjs9bFWHFKHmEnTy5EmkpKRY3PAsICAA5eXlFuMbGxtRUVGBgIAAZUxZWZnFmObnNxpz5fwrX3etMVfT6XTQ6/UWDyIiInsQHR2NkJAQbNmyRZm2ZcsWhIaGYujQoco0s9mM5ORkhIWFwdXVFYMHD8a///1vi2Xt2LEDffv2haurK+666y4UFRVZzL/6q7Gff/4Z9957L/z9/eHh4YFf/epX+Oabbyxe07NnTyxfvhwPPfQQPD09ERoaanGSVEexehFqLkEFBQX45ptv0LVrV4v5sbGxqKqqQnp6ujJtz549MJvNiImJUcakpqZafB2SkpKCiIgI+Pj4KGN2795tseyUlBTExsYCAMLCwhAQEGAxxmg04uDBg8oYIiIiNXnooYfw/vvvK8/fe+89zJw502JMcnIy/vWvf2HdunXIycnB/PnzMXXqVHz77bcAgFOnTuG+++7D+PHjkZWVhdmzZ+Ppp59udb01NTX43e9+h927dyMzMxPjxo3D+PHjUVxcbDFu9erVGD58ODIzMzF37lw89thjyM/v4BuUt/co7erqapGZmSkyMzMFALFmzRqRmZkpTp48KRoaGsQ999wjunfvLrKyssTZs2eVx5VngI0bN04MHTpUHDx4UHz33XciPDxcTJ48WZlfVVUl/P39xbRp08SRI0fExo0bhZubm3jrrbeUMfv37xdOTk7ipZdeEnl5eWLp0qXC2dlZZGdnK2NWrFghvL29xRdffCEOHz4s7r33XhEWFtbmI/478qwxo1EI/8kHRMgTX/GssVYYjUaxd+9eYTQaZUchIpW73llKZ84IkZ5u+Thxovk1Leelp//3tUePtpx34cLleeXlLecdO3Zz2adPny7uvfdeUV5eLnQ6nSgqKhJFRUXCxcVFnDt3Ttx7771i+vTpoq6uTri5uYkDBw5YvH7WrFnK5/SiRYtE//79LeYvXLhQABCVlZVCCCHef/994eXl1WqmAQMGiNdee0153qNHDzF16lTludlsFn5+fuLNN9+85uutddaYU3uL008//YS77rpLeZ6UlAQAmD59OpYtW4atW7cCAIYMGWLxur179+LOO+8EAHz88ceYN28e7r77bjg4OOD+++/Hq6++qoz18vLC119/jcTERAwbNgy+vr5YsmSJxbWGRo4ciQ0bNuC5557DM888g/DwcHz++ecYOHCgMmbBggW4ePEi5syZg6qqKowePRo7d+6Ei4tLe9+21Xl6Ai6hFbJj2DxPT0/l7w0RkS166y3g+ectp02ZAnz0EXD6NDBsWMvXiP+7lu6MGcD331vO+/BDYOpUYNMmYN48y3ljxwK7dt181m7duiEhIQHr16+HEAIJCQnw9fVV5h8/fhyXLl3Cb37zG4vXNTQ0KF+f5eXlKd/gNLvRNy01NTVYtmwZtm/fjrNnz6KxsRG1tbUt9ggNGjRI+bNGo7nm4TTW1u4idOedd0KI618NubV5zbp06YINGza0OmbQoEH4z3/+0+qYBx54AA888MB152s0Grzwwgt44YUXbpips5WUAJXfRsAzughOnvWy49iskpISvP7665g3bx6Cg4NlxyEiauGRR4CrrhuM/zuKA927A1ccCdLC+vXAxYuW03r2vPxz4kTg6n7haYUrrjz00EOY938N6+qLIzff5WH79u0t/s3V6XQ3vc4nn3wSKSkpeOmll9CnTx+4urrij3/8IxoaGizGXX0XAY1GA7PZfNPrbYt2FyGyjrIywPh9H7hFnGURakVZWRlWrFiBBx54gEWIiGxSYODlx7W4uADR0dd/bUTE9ed163b5YW3jxo1DQ0MDNBoN4uPjLeb1798fOp0OxcXFGDNmzDVf369fP+Xbn2bfX71b6yr79+/HjBkz8Ic//AHA5cJ19QHWsrAIERERqYijoyPy8vKUP1/J09MTTz75JObPnw+z2YzRo0fDYDBg//790Ov1mD59Oh599FGsXr0aTz31FGbPno309HSsX7++1XWGh4djy5YtGD9+PDQaDRYvXtzhe3rainefJyIiUpnWLhHz4osvYvHixUhOTka/fv0wbtw4bN++HWFhYQCA0NBQ/O///i8+//xzDB48GOvWrcPy5ctbXd+aNWvg4+ODkSNHYvz48YiPj0d0a7vKOpFGtOWgHpUyGo3w8vKCwWCw+jWFMjIuH0AXMP0/0AUYUbQiwarLtxcZGRkYNmwY0tPTbeY/GiJSp7q6OhQWFiIsLMwmTrpRu9Z+H+35/OYeIUm6dgU8BhXD0ZW3jmhN165dMWvWrBbXoyIiIrIGHiMkSY8eQNff8kaiN9KjRw+8++67smMQEZGd4h4hSWprgYZzHjCb+CtoTW1tLXJycizujUNERGQt/BSWJC8POPveGJgueMiOYtPy8vIwcOBA5QwHIiIia2IRIiIiItViESIiImoHnmxtG6z1e2ARIiIiaoPm2z9cunRJchICoNye4+qLQrYXzxqTRKMB4Nh0+Sddl0ajgVarhYYbiogkc3R0hLe3t3ITUDc3N/7bJInZbMa5c+fg5uYGJ6dfVmVYhCQZOhTo8eRO2TFs3tChQ1Ffz3uxEZFtCAgIAIAOvyM63ZiDgwNCQ0N/cRllESIiImojjUaDwMBA+Pn5wWTiBXFl0mq1cHD45Uf4sAhJkpcHnF0/Gr6/z4Kzb43sODYrLy8PU6ZMwccff4x+/frJjkNEBODy12S/9NgUsg08WFqS2lqgocwL5kb+ClpTW1uLzMxMXlCRiIg6BD+FiYiISLVYhIiIiEi1WISIiIhItViEJAkLA3zvTYeTNy/M1ZqwsDBs2rQJYWFhsqMQEZEd4lljkvj4AO6RpbJj2DwfHx888MADsmMQEZGd4h4hScrKAOMPYWi6qJUdxaaVlZVhzZo1KCsrkx2FiIjsEIuQJCUlQOXe/misdpEdxaaVlJTgr3/9K0pKSmRHISIiO8QiRERERKrFIkRERESqxSJEREREqsUiJImXF+DapwwOukbZUWyal5cXxo8fDy8vL9lRiIjIDvH0eUl69wb87v9Jdgyb17t3b2zdulV2DCIislPcIySJyQQ0XdJCNGlkR7FpJpMJ586dg8lkkh2FiIjsEIuQJNnZwOnXfoOGc56yo9i07Oxs+Pn5ITs7W3YUIiKyQyxCREREpFosQkRERKRaLEJERESkWixCREREpFo8fV6SwYOBkCd2QePM6wi1ZvDgwTAYDHB3d5cdhYiI7BCLkCSOjuDFFNvA0dERer1edgwiIrJT/GpMkoICoOzTETBVuMmOYtMKCgoQHx+PgoIC2VGIiMgOsQhJUl0N1BV1g7mBO+VaU11dja+//hrV1dWyoxARkR1iESIiIiLVYhEiIiIi1WIRIiIiItViEZIkJATo8psjcNLXyY5i00JCQvD6668jJCREdhQiIrJDPFJXkm7dAM/ok7Jj2Lxu3bohMTFRdgwiIrJT3CMkSUUFUJMTjKZaZ9lRbFpFRQU++ugjVFRUyI5CRER2iEVIkqIi4MK2IWg0uMqOYtOKioowbdo0FBUVyY5CRER2qN1FKDU1FePHj0dQUBA0Gg0+//xzi/lCCCxZsgSBgYFwdXVFXFxci4vhVVRUYMqUKdDr9fD29sasWbNQU1NjMebw4cO4/fbb4eLigpCQEKxcubJFls2bNyMyMhIuLi6IiorCjh072p2FiIiI1KvdRejixYsYPHgw1q5de835K1euxKuvvop169bh4MGDcHd3R3x8POrq/ntQ8JQpU5CTk4OUlBRs27YNqampmDNnjjLfaDRi7Nix6NGjB9LT07Fq1SosW7YMb7/9tjLmwIEDmDx5MmbNmoXMzExMmDABEyZMwJEjR9qVhYiIiFRM/AIAxGeffaY8N5vNIiAgQKxatUqZVlVVJXQ6nfjkk0+EEELk5uYKAOLHH39Uxnz11VdCo9GIkpISIYQQb7zxhvDx8RH19fXKmIULF4qIiAjl+cSJE0VCQoJFnpiYGPHII4+0OcuNGAwGAUAYDIY2jW+P9HQhACECpqeKHgu3WX359iI9PV0AEOnp6bKjEBHRLaI9n99WPUaosLAQpaWliIuLU6Z5eXkhJiYGaWlpAIC0tDR4e3tj+PDhypi4uDg4ODjg4MGDypg77rgDWq1WGRMfH4/8/HxUVlYqY65cT/OY5vW0JcvV6uvrYTQaLR4dxd0d0AZVwsG5qcPWYQ/c3d1x22238e7zRETUIaxahEpLSwEA/v7+FtP9/f2VeaWlpfDz87OY7+TkhC5duliMudYyrlzH9cZcOf9GWa6WnJwMLy8v5dGR166JiAACpx2Ac9eLHbYOexAREYG0tDRERETIjkJERHaIZ41dYdGiRTAYDMrj1KlTsiMRERFRB7JqEQoICAAAlJWVWUwvKytT5gUEBKC8vNxifmNjIyoqKizGXGsZV67jemOunH+jLFfT6XTQ6/UWj46SkQGc/HsC6ks7bh32ICMjAxqNBhkZGbKjEBGRHbJqEQoLC0NAQAB2796tTDMajTh48CBiY2MBALGxsaiqqkJ6eroyZs+ePTCbzYiJiVHGpKamwmQyKWNSUlIQEREBHx8fZcyV62ke07yetmQhIiIidWt3EaqpqUFWVhaysrIAXD4oOSsrC8XFxdBoNHjiiSfwt7/9DVu3bkV2djb+/Oc/IygoCBMmTAAA9OvXD+PGjcPDDz+MH374Afv378e8efPwpz/9CUFBQQCABx98EFqtFrNmzUJOTg4+/fRT/OMf/0BSUpKS4/HHH8fOnTuxevVqHD16FMuWLcNPP/2EefPmAUCbshAREZHKtfeUtL179woALR7Tp08XQlw+bX3x4sXC399f6HQ6cffdd4v8/HyLZVy4cEFMnjxZeHh4CL1eL2bOnCmqq6stxhw6dEiMHj1a6HQ6ERwcLFasWNEiy6ZNm0Tfvn2FVqsVAwYMENu3b7eY35YsreHp8/Lx9HkiImqv9nx+a4QQQl4Ns21GoxFeXl4wGAxWP14oIwMYNgwImP4f6AKMKFqRYNXl24uMjAwMGzYM6enpiI6Olh2HiIhuAe35/Obd5yXp3x8ImrMXTp68ynVr+vfvj4KCAnTv3l12FCIiskMsQpK4uADOPpdkx7B5Li4u6NOnj+wYRERkp3gdIUkKC4HzXw6BqYp3n29NYWEhpk6disLCQtlRiIjIDrEISVJZCVzMDYa5zll2FJtWWVmJjz/+WLm1ChERkTWxCBEREZFqsQgRERGRarEIERERkWqxCEkSGAh4jToGR4962VFsWmBgIJYuXYrAwEDZUYiIyA7x9HlJAgMB79EFsmPYvMDAQCxbtkx2DCIislPcIySJ0QjUnvCFuZ5dtDVGoxG7du2C0WiUHYWIiOwQi5Akx48D5ZtjYKp0kx3Fph0/fhzjxo3D8ePHZUchIiI7xCJEREREqsUiRERERKrFIkRERESqxSIkiU4HOHlfhMbRLDuKTdPpdOjduzd0Op3sKEREZId4ypIkAwYAwY/skx3D5g0YMIAHShMRUYfhHiEiIiJSLRYhSQ4fBk69GoeGck/ZUWza4cOH0a1bNxw+fFh2FCIiskMsQpI0NgLmWh2EWSM7ik1rbGzE+fPn0djYKDsKERHZIRYhIiIiUi0WISIiIlItFiEiIiJSLRYhSfr2BQKm7odzl4uyo9i0vn374sCBA+jbt6/sKEREZId4HSFJPDwAXXCV7Bg2z8PDA7GxsbJjEBGRneIeIUlOnwYqdvdDo9FFdhSbdvr0aSQlJeH06dOyoxARkR1iEZKkvByo/qkXmi5pZUexaeXl5Xj55ZdRXl4uOwoREdkhFiEiIiJSLRYhIiIiUi0WISIiIlItFiFJfH0Bj6FFcHRrkB3Fpvn6+mLu3Lnw9fWVHYWIiOwQT5+XJDQU6Do2R3YMmxcaGoq1a9fKjkFERHaKe4QkuXQJqC/Vw2zir6A1ly5dQkZGBi5duiQ7ChER2SF+Ckty9ChQ+sHtMF3wkB3Fph09ehTDhg3D0aNHZUchIiI7xCJEREREqsUiRERERKrFIkRERESqxSIkiYMDoNGaoNHITmLbHBwc4OnpCQcH/lUlIiLr4+nzkgwZAoTO/1p2DJs3ZMgQGI1G2TGIiMhO8X+ziYiISLVYhCTJzQXOvHsHGs7z9PnW5ObmYsCAAcjNzZUdhYiI7BCLkCR1dYDpgidEI38Framrq0Nubi7q6upkRyEiIjvET2EiIiJSLRYhIiIiUi0WISIiIlItqxehpqYmLF68GGFhYXB1dUXv3r3x4osvQgihjBFCYMmSJQgMDISrqyvi4uJQUFBgsZyKigpMmTIFer0e3t7emDVrFmpqaizGHD58GLfffjtcXFwQEhKClStXtsizefNmREZGwsXFBVFRUdixY4e13/JN6dUL6Hbfj3D25s1EW9OrVy988cUX6NWrl+woRERkh6xehP7+97/jzTffxOuvv468vDz8/e9/x8qVK/Haa68pY1auXIlXX30V69atw8GDB+Hu7o74+HiLA2KnTJmCnJwcpKSkYNu2bUhNTcWcOXOU+UajEWPHjkWPHj2Qnp6OVatWYdmyZXj77beVMQcOHMDkyZMxa9YsZGZmYsKECZgwYQKOHDli7bfdbt7egFt4ORxcGmVHsWne3t6455574O3tLTsKERHZI2FlCQkJ4qGHHrKYdt9994kpU6YIIYQwm80iICBArFq1SplfVVUldDqd+OSTT4QQQuTm5goA4scff1TGfPXVV0Kj0YiSkhIhhBBvvPGG8PHxEfX19cqYhQsXioiICOX5xIkTRUJCgkWWmJgY8cgjj7TpvRgMBgFAGAyGNo1vj7NnhfC+I090T0wRPRZus/ry7cXZs2fF8uXLxdmzZ2VHISKiW0R7Pr+tvkdo5MiR2L17N44dOwYAOHToEL777jv89re/BQAUFhaitLQUcXFxymu8vLwQExODtLQ0AEBaWhq8vb0xfPhwZUxcXBwcHBxw8OBBZcwdd9wBrVarjImPj0d+fj4qKyuVMVeup3lM83pkOnMGqEqNRGONTnYUm3bmzBk888wzOHPmjOwoRERkh6x+i42nn34aRqMRkZGRcHR0RFNTE/7nf/4HU6ZMAQCUlpYCAPz9/S1e5+/vr8wrLS2Fn5+fZVAnJ3Tp0sViTFhYWItlNM/z8fFBaWlpq+u5Wn19Perr65XnvLUDERGRfbP6HqFNmzbh448/xoYNG5CRkYEPPvgAL730Ej744ANrr8rqkpOT4eXlpTxCQkJkRyIiIqIOZPUi9NRTT+Hpp5/Gn/70J0RFRWHatGmYP38+kpOTAQABAQEAgLKyMovXlZWVKfMCAgJQXl5uMb+xsREVFRUWY661jCvXcb0xzfOvtmjRIhgMBuVx6tSpdr9/IiIiunVYvQhdunQJDg6Wi3V0dITZbAYAhIWFISAgALt371bmG41GHDx4ELGxsQCA2NhYVFVVIT09XRmzZ88emM1mxMTEKGNSU1NhMpmUMSkpKYiIiICPj48y5sr1NI9pXs/VdDod9Hq9xaOjeHsDbhFn4eBiuuFYNfP29sYf//hHnjVGREQdw9pHak+fPl0EBweLbdu2icLCQrFlyxbh6+srFixYoIxZsWKF8Pb2Fl988YU4fPiwuPfee0VYWJiora1VxowbN04MHTpUHDx4UHz33XciPDxcTJ48WZlfVVUl/P39xbRp08SRI0fExo0bhZubm3jrrbeUMfv37xdOTk7ipZdeEnl5eWLp0qXC2dlZZGdnt+m9dORZY0II0WPhNuVBRERE1tGez2+rFyGj0Sgef/xxERoaKlxcXESvXr3Es88+a3Gau9lsFosXLxb+/v5Cp9OJu+++W+Tn51ss58KFC2Ly5MnCw8ND6PV6MXPmTFFdXW0x5tChQ2L06NFCp9OJ4OBgsWLFihZ5Nm3aJPr27Su0Wq0YMGCA2L59e5vfS0cWofp6IYIf+0aEPrmdRagV9fX14tSpUxZ/f4iIiFrTns9vjRBXXPKZLBiNRnh5ecFgMFj9a7KMDGDYMCBg+n+gCzCiaEWCVZdvLzIyMjBs2DCkp6cjOjpadhwiIroFtOfzm/caIyIiItViESIiIiLVYhEiIiIi1WIRIiIiItWy+i02qG2GDAFC//oV4GiWHcWmDRkyBHV1dXB2dpYdhYiI7BCLkCQODoDGiSXoRhwcHKDT8ca0RETUMfjVmCTHjgGlG26DqcJddhSbduzYMdx55504duyY7ChERGSHWIQkqakB6k91hbnBUXYUm1ZTU4Nvv/0WNTU1sqMQEZEdYhEiIiIi1WIRIiIiItViESIiIiLVYhGSJDQU6DLuMJz0tbKj2LTQ0FC88847CA0NlR2FiIjsEE+fl8TXF/AcfEp2DJvn6+uL2bNny45BRER2inuEJDl/Hqg+FIKmS7xQYGvOnz+Pd999F+fPn5cdhYiI7BCLkCTFxUDFzkFoNLrKjmLTiouL8fDDD6O4uFh2FCIiskMsQkRERKRaLEJERESkWixCREREpFosQpJ4eAC6kAtw0DbJjmLTPDw8MGbMGHh4eMiOQkREdoinz0vSty8Q8OD3smPYvL59+2Lfvn2yYxARkZ3iHiFJzGZANDpACNlJbJvZbEZ9fT3MZrPsKEREZIdYhCTJygKKV/8WDWV62VFsWlZWFlxcXJCVlSU7ChER2SEWISIiIlItFiEiIiJSLRYhIiIiUi0WISIiIlItnj4vycCBQPBju+HoXi87ik0bOHAgTp06BT8/P9lRiIjIDrEISaLVAk76OtkxbJ5Wq0X37t1lxyAiIjvFr8YkOXECOPd5NExVvPt8a06cOIEHHngAJ06ckB2FiIjsEIuQJFVVwKX8QJjrnGVHsWlVVVX497//jaqqKtlRiIjIDrEIERERkWqxCBEREZFqsQgRERGRarEISRIUBHjfcRROHjx9vjVBQUFYvnw5goKCZEchIiI7xNPnJQkIALxif5Ydw+YFBARg0aJFsmMQEZGd4h4hSaqqgEsFfjDXsYu2pqqqClu3buVZY0RE1CFYhCQ5cQI4t+VXMFW5yY5i006cOIF7772X1xEiIqIOwSJEREREqsUiRERERKrFIkRERESqxSIkiYsL4Ny1Ghons+woNs3FxQX9+/eHi4uL7ChERGSHeMqSJP37A0GzU2XHsHn9+/dHTk6O7BhERGSnuEeIiIiIVItFSJKsLKD45bFoKNPLjmLTsrKyoNfrkZWVJTsKERHZIRYhScxmQDQ4QwjZSWyb2WxGdXU1zGYeS0VERNbXIUWopKQEU6dORdeuXeHq6oqoqCj89NNPynwhBJYsWYLAwEC4uroiLi4OBQUFFsuoqKjAlClToNfr4e3tjVmzZqGmpsZizOHDh3H77bfDxcUFISEhWLlyZYssmzdvRmRkJFxcXBAVFYUdO3Z0xFsmIiKiW5DVi1BlZSVGjRoFZ2dnfPXVV8jNzcXq1avh4+OjjFm5ciVeffVVrFu3DgcPHoS7uzvi4+NRV1enjJkyZQpycnKQkpKCbdu2ITU1FXPmzFHmG41GjB07Fj169EB6ejpWrVqFZcuW4e2331bGHDhwAJMnT8asWbOQmZmJCRMmYMKECThy5Ii13zYRERHdioSVLVy4UIwePfq6881mswgICBCrVq1SplVVVQmdTic++eQTIYQQubm5AoD48ccflTFfffWV0Gg0oqSkRAghxBtvvCF8fHxEfX29xbojIiKU5xMnThQJCQkW64+JiRGPPPJIm96LwWAQAITBYGjT+PZITxcCECJgeqrosXCb1ZdvL9LT0wUAkZ6eLjsKERHdItrz+W31PUJbt27F8OHD8cADD8DPzw9Dhw7FO++8o8wvLCxEaWkp4uLilGleXl6IiYlBWloaACAtLQ3e3t4YPny4MiYuLg4ODg44ePCgMuaOO+6AVqtVxsTHxyM/Px+VlZXKmCvX0zymeT0yRUYCAdP/A+euNTcerGKRkZFIT09HZGSk7ChERGSHrF6ETpw4gTfffBPh4eHYtWsXHnvsMfzlL3/BBx98AAAoLS0FAPj7+1u8zt/fX5lXWloKPz8/i/lOTk7o0qWLxZhrLePKdVxvTPP8q9XX18NoNFo8OoqbG6ALMMLBmQcBt8bNzQ3R0dFwc+PNaYmIyPqsXoTMZjOio6OxfPlyDB06FHPmzMHDDz+MdevWWXtVVpecnAwvLy/lERIS0mHrKi4GLnw9AI1GXjG5NcXFxUhMTERxcbHsKEREZIesXoQCAwPRv39/i2n9+vVTPsgCAgIAAGVlZRZjysrKlHkBAQEoLy+3mN/Y2IiKigqLMddaxpXruN6Y5vlXW7RoEQwGg/I4depU2970TTh/HqjJ7ImmS9obD1ax8+fP44033sD58+dlRyEiIjtk9SI0atQo5OfnW0w7duwYevToAQAICwtDQEAAdu/ercw3Go04ePAgYmNjAQCxsbGoqqpCenq6MmbPnj0wm82IiYlRxqSmpsJkMiljUlJSEBERoZyhFhsba7Ge5jHN67maTqeDXq+3eBAREZH9snoRmj9/Pr7//nssX74cx48fx4YNG/D2228jMTERAKDRaPDEE0/gb3/7G7Zu3Yrs7Gz8+c9/RlBQECZMmADg8h6kcePG4eGHH8YPP/yA/fv3Y968efjTn/6EoKAgAMCDDz4IrVaLWbNmIScnB59++in+8Y9/ICkpScny+OOPY+fOnVi9ejWOHj2KZcuW4aeffsK8efOs/baJiIjoVtQRp619+eWXYuDAgUKn04nIyEjx9ttvW8w3m81i8eLFwt/fX+h0OnH33XeL/Px8izEXLlwQkydPFh4eHkKv14uZM2eK6upqizGHDh0So0ePFjqdTgQHB4sVK1a0yLJp0ybRt29fodVqxYABA8T27dvb/D54+rx8PH2eiIjaqz2f3xoheJOH6zEajfDy8oLBYLD612SnTwP9/3AC+l8Vwklfh6IVCVZdvr04ffo01qxZg6SkJHTv3l12HCIiugW05/PbqZMy0VW6dwe63J0nO4bN6969O9asWSM7BhER2SnedFWSmhqgvsQb5gZH2VFsWk1NDdLS0lrcZ46IiMgaWIQkOXYMKP1oFEwV7rKj2LRjx45h5MiROHbsmOwoRERkh1iEiIiISLVYhIiIiEi1WISIiIhItViEJHFyAhxc66Fx4NULWuPk5ARfX184OfEERyIisj5+ukgyaBAQ8pdvZMeweYMGDcK5c+dkxyAiIjvFPUJERESkWixCkuTkACVv3YmGcx6yo9i0nJwc9OnTBzk5ObKjEBGRHWIRkqS+Hmiscodo4q+gNfX19fj5559RX18vOwoREdkhfgoTERGRarEIERERkWqxCBEREZFqsQhJ0qcP4PfAQTj7XJIdxab16dMHO3fuRJ8+fWRHISIiO8TrCEmi1wOuvc7LjmHz9Ho94uPjZccgIiI7xT1Ckpw9C1R9F47GGp3sKDbt7NmzWLZsGc6ePSs7ChER2SEWIUnOngUM+/uiiUWoVWfPnsXzzz/PIkRERB2CRYiIiIhUi0WIiIiIVItFiIiIiFSLRUgSHx/AvX8JHFxMsqPYNB8fH0yZMgU+Pj6yoxARkR3i6fOShIUBvuOzZMeweWFhYfjoo49kxyAiIjvFPUKS1NUBpko3iEb+ClpTV1eH48ePo66uTnYUIiKyQ/wUliQ3Fzjz9l1oOO8hO4pNy83NRXh4OHJzc2VHISIiO8QiRERERKrFIkRERESqxSJEREREqsUiRERERKrF0+cliY4GeizcLjuGzYuOjoYQQnYMIiKyU9wjRERERKrFIiRJfj5w9sORMF1wlx3FpuXn5yM2Nhb5+fmyoxARkR1iEZLk4kWg4YwPzCZH2VFs2sWLF/H999/j4sWLsqMQEZEdYhEiIiIi1WIRIiIiItViESIiIiLVYhGSpGdPoOvvs+DkVSs7ik3r2bMnPvzwQ/Ts2VN2FCIiskO8jpAkXboAHgNKZMeweV26dMHUqVNlxyAiIjvFPUKSnDsHVGf0QNMlrewoNu3cuXNYu3Ytzp07JzsKERHZIRYhSU6dAipSBqLR6CI7ik07deoU5s2bh1OnTsmOQkREdohFiIiIiFSLRYiIiIhUi0WIiIiIVItFSBJPT8Cl5zk4aBtlR7Fpnp6eGDt2LDw9PWVHISIiO8TT5yUJDwf8J/0gO4bNCw8Px65du2THICIiO9Xhe4RWrFgBjUaDJ554QplWV1eHxMREdO3aFR4eHrj//vtRVlZm8bri4mIkJCTAzc0Nfn5+eOqpp9DYaLn3ZN++fYiOjoZOp0OfPn2wfv36Futfu3YtevbsCRcXF8TExOCHH2yjfDQ1AeZ6Jwiz7CS2rampCUajEU1NTbKjEBGRHerQIvTjjz/irbfewqBBgyymz58/H19++SU2b96Mb7/9FmfOnMF9992nzG9qakJCQgIaGhpw4MABfPDBB1i/fj2WLFmijCksLERCQgLuuusuZGVl4YknnsDs2bMt9h58+umnSEpKwtKlS5GRkYHBgwcjPj4e5eXlHfm22+TQIeDUK/FoKNfLjmLTDh06BC8vLxw6dEh2FCIiskeig1RXV4vw8HCRkpIixowZIx5//HEhhBBVVVXC2dlZbN68WRmbl5cnAIi0tDQhhBA7duwQDg4OorS0VBnz5ptvCr1eL+rr64UQQixYsEAMGDDAYp2TJk0S8fHxyvMRI0aIxMRE5XlTU5MICgoSycnJbXoPBoNBABAGg6F9b74N0tOFAIQImJ4qeizcZvXl24v09HQBQKSnp8uOQkREt4j2fH532B6hxMREJCQkIC4uzmJ6eno6TCaTxfTIyEiEhoYiLS0NAJCWloaoqCj4+/srY+Lj42E0GpGTk6OMuXrZ8fHxyjIaGhqQnp5uMcbBwQFxcXHKmKvV19fDaDRaPIiIiMh+dcjB0hs3bkRGRgZ+/PHHFvNKS0uh1Wrh7e1tMd3f3x+lpaXKmCtLUPP85nmtjTEajaitrUVlZSWampquOebo0aPXzJ2cnIznn3++7W+UiIiIbmlW3yN06tQpPP744/j444/h4nJr3T5i0aJFMBgMyoO3dSAiIrJvVt8jlJ6ejvLyckRHRyvTmpqakJqaitdffx27du1CQ0MDqqqqLPYKlZWVISAgAAAQEBDQ4uyu5rPKrhxz9ZlmZWVl0Ov1cHV1haOjIxwdHa85pnkZV9PpdNDpdDf3xtspKgro/v9S4KAzdcr6blVRUVEoLy9vsQeRiIjIGqy+R+juu+9GdnY2srKylMfw4cMxZcoU5c/Ozs7YvXu38pr8/HwUFxcjNjYWABAbG4vs7GyLs7tSUlKg1+vRv39/ZcyVy2ge07wMrVaLYcOGWYwxm83YvXu3MkYmZ2fA0a0BGkchO4pNc3Z2Rrdu3eDs7Cw7ChER2SGrFyFPT08MHDjQ4uHu7o6uXbti4MCB8PLywqxZs5CUlIS9e/ciPT0dM2fORGxsLG677TYAwNixY9G/f39MmzYNhw4dwq5du/Dcc88hMTFR2WPz6KOP4sSJE1iwYAGOHj2KN954A5s2bcL8+fOVLElJSXjnnXfwwQcfIC8vD4899hguXryImTNnWvttt9vPPwPl/zscpko32VFs2s8//4x77rkHP//8s+woRERkh6TcYuPll1/G73//e9x///244447EBAQgC1btijzHR0dsW3bNjg6OiI2NhZTp07Fn//8Z7zwwgvKmLCwMGzfvh0pKSkYPHgwVq9ejXfffRfx8fHKmEmTJuGll17CkiVLMGTIEGRlZWHnzp0tDqCWwWAAao/7w1zPi3u3xmAw4Msvv4TBYJAdhYiI7JBGCMHvZq7DaDTCy8sLBoMBer11L3yYkQEMGwYETP8PdAFGFK1IsOry7UVGRgaGDRuG9PR0i+POiIiIrqc9n9+86SoRERGpFosQERERqRaLkCTBwYDPXblw8qyTHcWmBQcHY/Xq1QgODpYdhYiI7BCP1JXE3x/QjyiUHcPm+fv7IykpSXYMIiKyU9wjJEllJXDxaACa6thFW1NZWYnNmzejsrJSdhQiIrJDLEKSFBYC578YhsYqXkeoNYWFhZg4cSIKC7n3jIiIrI9FiIiIiFSLRYiIiIhUi0WIiIiIVItFSBJXV0Drb4CDk1l2FJvm6uqKoUOHwtXVVXYUIiKyQzxlSZJ+/YDAGd/JjmHz+vXrh4yMDNkxiIjITnGPEBEREakWi5AkmZnAyZfGoaHMujdztTeZmZnQ6XTIzMyUHYWIiOwQi5AkQgBocrz8k65LCIGGhgYIbigiIuoALEJERESkWixCREREpFosQkRERKRaPH1ekn79gMCHvoWT9yXZUWxav379cOTIEfTq1Ut2FCIiskMsQpK4ugLabjWyY9g8V1dXDBgwQHYMIiKyU/xqTJKTJ4ELX0Wh0cArJrfm5MmTmD17Nk6ePCk7ChER2SEWIUkuXABqDoeiqdZZdhSbduHCBfzzn//EhQsXZEchIiI7xCJEREREqsUiRERERKrFIkRERESqxSIkib8/oL/tOBzd62VHsWn+/v54+umn4e/vLzsKERHZIZ4+L0lwMOAzJl92DJsXHByM5ORk2TGIiMhOcY+QJNXVQF1xF5jrHWVHsWnV1dXYt28fqqurZUchIiI7xCIkSUEBUPZJLEyV7rKj2LSCggLcddddKCgokB2FiIjsEIsQERERqRaLEBEREakWixARERGpFouQJM7OgKNHLTSOQnYUm+bs7Izg4GA4O/NWJEREZH08fV6SqCige+Ie2TFsXlRUFE6fPi07BhER2SnuESIiIiLVYhGSJDsbOL3212g45yk7ik3Lzs5G9+7dkZ2dLTsKERHZIRYhSUwmoKnGFaJJIzuKTTOZTCgpKYHJZJIdhYiI7BCLEBEREakWixARERGpFosQERERqRaLkCTh4YD/5DQ4+1yUHcWmhYeHY+/evQgPD5cdhYiI7BCvIySJpyfgElohO4bN8/T0xJ133ik7BhER2SnuEZKkpASo/DYCjdU62VFsWklJCRYtWoSSkhLZUYiIyA6xCElSVgYYv++DpossQq0pKyvDihUrUFZWJjsKERHZIRYhIiIiUi0WISIiIlItqxeh5ORk/OpXv4Knpyf8/PwwYcIE5OfnW4ypq6tDYmIiunbtCg8PD9x///0tvvooLi5GQkIC3Nzc4Ofnh6eeegqNjY0WY/bt24fo6GjodDr06dMH69evb5Fn7dq16NmzJ1xcXBATE4MffvjB2m+ZiIiIblFWL0LffvstEhMT8f333yMlJQUmkwljx47FxYv/PU18/vz5+PLLL7F582Z8++23OHPmDO677z5lflNTExISEtDQ0IADBw7ggw8+wPr167FkyRJlTGFhIRISEnDXXXchKysLTzzxBGbPno1du3YpYz799FMkJSVh6dKlyMjIwODBgxEfH4/y8nJrv+1269oV8BhUDEdX3jqiNV27dsWsWbPQtWtX2VGIiMgeiQ5WXl4uAIhvv/1WCCFEVVWVcHZ2Fps3b1bG5OXlCQAiLS1NCCHEjh07hIODgygtLVXGvPnmm0Kv14v6+nohhBALFiwQAwYMsFjXpEmTRHx8vPJ8xIgRIjExUXne1NQkgoKCRHJycpuyGwwGAUAYDIZ2vuu26bFwm/IgIiIi62jP53eHHyNkMBgAAF26dAEApKenw2QyIS4uThkTGRmJ0NBQpKWlAQDS0tIQFRUFf39/ZUx8fDyMRiNycnKUMVcuo3lM8zIaGhqQnp5uMcbBwQFxcXHKmKvV19fDaDRaPDpKbS3QcM4DZhMP02pNbW0tcnJyUFtbKzsKERHZoQ79FDabzXjiiScwatQoDBw4EABQWloKrVYLb29vi7H+/v4oLS1VxlxZgprnN89rbYzRaERtbS3Onz+Ppqama45pXsbVkpOT4eXlpTxCQkJu7o23QV4ecPa9MTBd8OiwddiDvLw8DBw4EHl5ebKjEBGRHerQIpSYmIgjR45g48aNHbkaq1m0aBEMBoPyOHXqlOxIRERE1IE67BYb8+bNw7Zt25Camoru3bsr0wMCAtDQ0ICqqiqLvUJlZWUICAhQxlx9dlfzWWVXjrn6TLOysjLo9Xq4urrC0dERjo6O1xzTvIyr6XQ66HS8wCEREZFaWH2PkBAC8+bNw2effYY9e/YgLCzMYv6wYcPg7OyM3bt3K9Py8/NRXFyM2NhYAEBsbCyys7Mtzu5KSUmBXq9H//79lTFXLqN5TPMytFothg0bZjHGbDZj9+7dyhgiIiJSN6vvEUpMTMSGDRvwxRdfwNPTUzkex8vLC66urvDy8sKsWbOQlJSELl26QK/X4//9v/+H2NhY3HbbbQCAsWPHon///pg2bRpWrlyJ0tJSPPfcc0hMTFT22Dz66KN4/fXXsWDBAjz00EPYs2cPNm3ahO3btytZkpKSMH36dAwfPhwjRozAK6+8gosXL2LmzJnWftvtptEAcGy6/JOuS6PRQKvVQsMNRUREHcHap6wBuObj/fffV8bU1taKuXPnCh8fH+Hm5ib+8Ic/iLNnz1osp6ioSPz2t78Vrq6uwtfXV/z1r38VJpPJYszevXvFkCFDhFarFb169bJYR7PXXntNhIaGCq1WK0aMGCG+//77Nr8Xnj5PRER062nP57dGCCHk1TDbZjQa4eXlBYPBAL1eb/Xl93z6v3uvilYkWH35REREatSez29exEaSvDzg7PrRMJ3n6fOtycvLQ3R0NE+fJyKiDsEiJEltLdBQ5gVzI38FramtrUVmZiYvqEhERB2Cn8JERESkWixCREREpFosQkRERKRaLEKShIUBvvemw8n7kuwoNi0sLAybNm1qcWFOIiIia+iwW2xQ63x8APfIa9/8lf7Lx8cHDzzwgOwYRERkp7hHSJKyMsD4QxiaLmplR7FpZWVlWLNmTYt7xhEREVkDi5AkJSVA5d7+aKx2kR3FppWUlOCvf/0rSkpKZEchIiI7xCJEREREqsUiRERERKrFIkRERESqxSIkiZcX4NqnDA66RtlRbJqXlxfGjx8PLy8v2VGIiMgO8fR5SXr3Bvzu/0l2DJvXu3dvbN26VXYMIiKyU9wjJInJBDRd0kI0aWRHsWkmkwnnzp2DyWSSHYWIiOwQi5Ak2dnA6dd+g4ZznrKj2LTs7Gz4+fkhOztbdhQiIrJDLEJERESkWixCREREpFosQkRERKRaLEJERESkWjx9XpLBg4GQJ3ZB48zrCLVm8ODBMBgMcHd3lx2FiIjsEIuQJI6O4MUU28DR0RF6vV52DCIislP8akySggKg7NMRMFW4yY5i0woKChAfH4+CggLZUYiIyA6xCElSXQ3UFXWDuYE75VpTXV2Nr7/+GtXV1bKjEBGRHWIRIiIiItViESIiIiLVYhEiIiIi1WIRkiQkBOjymyNw0tfJjmLTQkJC8PrrryMkJER2FCIiskM8UleSbt0Az+iTsmPYvG7duiExMVF2DCIislPcIyRJRQVQkxOMplpn2VFsWkVFBT766CNUVFTIjkJERHaIRUiSoiLgwrYhaDS4yo5i04qKijBt2jQUFRXJjkJERHaIRYiIiIhUi0WIiIiIVItFiIiIiFSLRUgSd3dAG1QJB+cm2VFsmru7O2677TbefZ6IiDoET5+XJCICCJx2QHYMmxcREYG0tDTZMYiIyE5xjxARERGpFouQJBkZwMm/J6C+VC87ik3LyMiARqNBRkaG7ChERGSHWISIiIhItViEiIiISLVYhIiIiEi1WISIiIhItXj6vCT9+wNBc/bCybNOdhSb1r9/fxQUFKB79+6yoxARkR1iEZLExQVw9rkkO4bNc3FxQZ8+fWTHICIiO8WvxiQpLATOfzkEpirefb41hYWFmDp1KgoLC2VHISIiO6SKPUJr167FqlWrUFpaisGDB+O1117DiBEjpGaqrAQu5gbD81cnANRKzWLLKisr8fHHHyMpKQlhYWGy4xDZtZ5Pb++U9RStSOiU9RC1hd0XoU8//RRJSUlYt24dYmJi8MorryA+Ph75+fnw8/OTHY+I6KZ1VnGxNmvkZpkia7H7IrRmzRo8/PDDmDlzJgBg3bp12L59O9577z08/fTTktMREV12q5YaWW5me7E80bXYdRFqaGhAeno6Fi1apExzcHBAXFzcNW/kWV9fj/r6euW5wWAAABiNRqtnq6m5/NPccBHm+ksdsg57UPN/G6qmpobbiDrFwKW7ZEegDhI6f/Mvev2R5+OtlIQ6WvPnhRDihmPtugidP38eTU1N8Pf3t5ju7++Po0ePthifnJyM559/vsX0kJCQDstY/snln16vdNgq7MKYMWNkRyAileO/07ee6upqeHl5tTrGrotQey1atAhJSUnKc7PZjIqKCnTt2hUajcaq6zIajQgJCcGpU6eg1/PGqx2N27tzcXt3Pm7zzsXt3bnau72FEKiurkZQUNANx9p1EfL19YWjoyPKysosppeVlSEgIKDFeJ1OB51OZzHN29u7IyNCr9fzP6JOxO3dubi9Ox+3eefi9u5c7dneN9oT1MyuryOk1WoxbNgw7N69W5lmNpuxe/duxMbGSkxGREREtsCu9wgBQFJSEqZPn47hw4djxIgReOWVV3Dx4kXlLDIiIiJSL7svQpMmTcK5c+ewZMkSlJaWYsiQIdi5c2eLA6g7m06nw9KlS1t8FUcdg9u7c3F7dz5u887F7d25OnJ7a0Rbzi0jIiIiskN2fYwQERERUWtYhIiIiEi1WISIiIhItViEiIiISLVYhDrQ2rVr0bNnT7i4uCAmJgY//PBDq+M3b96MyMhIuLi4ICoqCjt27OikpPahPdv7nXfewe233w4fHx/4+PggLi7uhr8fstTev9/NNm7cCI1GgwkTJnRsQDvT3u1dVVWFxMREBAYGQqfToW/fvvw3pZ3au81feeUVREREwNXVFSEhIZg/fz7q6uo6Ke2tLTU1FePHj0dQUBA0Gg0+//zzG75m3759iI6Ohk6nQ58+fbB+/fqbW7mgDrFx40ah1WrFe++9J3JycsTDDz8svL29RVlZ2TXH79+/Xzg6OoqVK1eK3Nxc8dxzzwlnZ2eRnZ3dyclvTe3d3g8++KBYu3atyMzMFHl5eWLGjBnCy8tLnD59upOT35rau72bFRYWiuDgYHH77beLe++9t3PC2oH2bu/6+noxfPhw8bvf/U589913orCwUOzbt09kZWV1cvJbV3u3+ccffyx0Op34+OOPRWFhodi1a5cIDAwU8+fP7+Tkt6YdO3aIZ599VmzZskUAEJ999lmr40+cOCHc3NxEUlKSyM3NFa+99ppwdHQUO3fubPe6WYQ6yIgRI0RiYqLyvKmpSQQFBYnk5ORrjp84caJISEiwmBYTEyMeeeSRDs1pL9q7va/W2NgoPD09xQcffNBREe3KzWzvxsZGMXLkSPHuu++K6dOnswi1Q3u395tvvil69eolGhoaOiui3WnvNk9MTBS//vWvLaYlJSWJUaNGdWhOe9SWIrRgwQIxYMAAi2mTJk0S8fHx7V4fvxrrAA0NDUhPT0dcXJwyzcHBAXFxcUhLS7vma9LS0izGA0B8fPx1x9N/3cz2vtqlS5dgMpnQpUuXjoppN252e7/wwgvw8/PDrFmzOiOm3biZ7b1161bExsYiMTER/v7+GDhwIJYvX46mpqbOin1Lu5ltPnLkSKSnpytfn504cQI7duzA7373u07JrDbW/My0+ytLy3D+/Hk0NTW1uHq1v78/jh49es3XlJaWXnN8aWlph+W0Fzezva+2cOFCBAUFtfgPi1q6me393Xff4Z///CeysrI6IaF9uZntfeLECezZswdTpkzBjh07cPz4ccydOxcmkwlLly7tjNi3tJvZ5g8++CDOnz+P0aNHQwiBxsZGPProo3jmmWc6I7LqXO8z02g0ora2Fq6urm1eFvcIkeqtWLECGzduxGeffQYXFxfZcexOdXU1pk2bhnfeeQe+vr6y46iC2WyGn58f3n77bQwbNgyTJk3Cs88+i3Xr1smOZrf27duH5cuX44033kBGRga2bNmC7du348UXX5QdjW6Ae4Q6gK+vLxwdHVFWVmYxvaysDAEBAdd8TUBAQLvG03/dzPZu9tJLL2HFihX45ptvMGjQoI6MaTfau71//vlnFBUVYfz48co0s9kMAHByckJ+fj569+7dsaFvYTfz9zswMBDOzs5wdHRUpvXr1w+lpaVoaGiAVqvt0My3upvZ5osXL8a0adMwe/ZsAEBUVBQuXryIOXPm4Nlnn4WDA/c7WNP1PjP1en279gYB3CPUIbRaLYYNG4bdu3cr08xmM3bv3o3Y2NhrviY2NtZiPACkpKRcdzz9181sbwBYuXIlXnzxRezcuRPDhw/vjKh2ob3bOzIyEtnZ2cjKylIe99xzD+666y5kZWUhJCSkM+Pfcm7m7/eoUaNw/PhxpXACwLFjxxAYGMgS1AY3s80vXbrUouw0F1HBW3panVU/M9t9eDW1ycaNG4VOpxPr168Xubm5Ys6cOcLb21uUlpYKIYSYNm2aePrpp5Xx+/fvF05OTuKll14SeXl5YunSpTx9vh3au71XrFghtFqt+Pe//y3Onj2rPKqrq2W9hVtKe7f31XjWWPu0d3sXFxcLT09PMW/ePJGfny+2bdsm/Pz8xN/+9jdZb+GW095tvnTpUuHp6Sk++eQTceLECfH111+L3r17i4kTJ8p6C7eU6upqkZmZKTIzMwUAsWbNGpGZmSlOnjwphBDi6aefFtOmTVPGN58+/9RTT4m8vDyxdu1anj5vi1577TURGhoqtFqtGDFihPj++++VeWPGjBHTp0+3GL9p0ybRt29fodVqxYABA8T27ds7OfGtrT3bu0ePHgJAi8fSpUs7P/gtqr1/v6/EItR+7d3eBw4cEDExMUKn04levXqJ//mf/xGNjY2dnPrW1p5tbjKZxLJly0Tv3r2Fi4uLCAkJEXPnzhWVlZWdH/wWtHfv3mv+m9y8jadPny7GjBnT4jVDhgwRWq1W9OrVS7z//vs3tW6NENxnR0REROrEY4SIiIhItViEiIiISLVYhIiIiEi1WISIiIhItViEiIiISLVYhIiIiEi1WISIiIhItViEiIiISLVYhIiIiEi1WISIiIhItViEiIiISLVYhIiIiEi1/j/oNZDxIazcowAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        0    1         2         3    4         5    6    7    8    9   ...  \\\n",
              "0      0.0  0.0  0.672990  0.757724  0.0  0.733961  0.0  0.0  0.0  0.0  ...   \n",
              "1      0.0  0.0  0.585479  0.531645  0.0  0.619044  0.0  0.0  0.0  0.0  ...   \n",
              "2      0.0  0.0  0.623519  0.608375  0.0  0.584744  0.0  0.0  0.0  0.0  ...   \n",
              "3      0.0  0.0  0.477308  0.452107  0.0  0.550653  0.0  0.0  0.0  0.0  ...   \n",
              "4      0.0  0.0  0.635936  0.718918  0.0  0.673960  0.0  0.0  0.0  0.0  ...   \n",
              "...    ...  ...       ...       ...  ...       ...  ...  ...  ...  ...  ...   \n",
              "52424  0.0  0.0  0.572783  0.522857  0.0  0.609314  0.0  0.0  0.0  0.0  ...   \n",
              "52425  0.0  0.0  0.692145  0.669218  0.0  0.670203  0.0  0.0  0.0  0.0  ...   \n",
              "52426  0.0  0.0  0.710077  0.598233  0.0  0.704481  0.0  0.0  0.0  0.0  ...   \n",
              "52427  0.0  0.0  0.747086  0.707230  0.0  0.738122  0.0  0.0  0.0  0.0  ...   \n",
              "52428  0.0  0.0  0.412218  0.345434  0.0  0.287335  0.0  0.0  0.0  0.0  ...   \n",
              "\n",
              "        22   23   24   25   26   27   28   29   30        31  \n",
              "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.757082  \n",
              "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.518198  \n",
              "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.580232  \n",
              "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.441705  \n",
              "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.694717  \n",
              "...    ...  ...  ...  ...  ...  ...  ...  ...  ...       ...  \n",
              "52424  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.504837  \n",
              "52425  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.668074  \n",
              "52426  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.628872  \n",
              "52427  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.758396  \n",
              "52428  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.376129  \n",
              "\n",
              "[52429 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-baaae76c-5c75-4210-afa8-477b17ea2a71\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.672990</td>\n",
              "      <td>0.757724</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.733961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.757082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.585479</td>\n",
              "      <td>0.531645</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.619044</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.518198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.623519</td>\n",
              "      <td>0.608375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.584744</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.580232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.477308</td>\n",
              "      <td>0.452107</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.550653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.441705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.635936</td>\n",
              "      <td>0.718918</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.673960</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.694717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52424</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.572783</td>\n",
              "      <td>0.522857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.609314</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.504837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52425</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.692145</td>\n",
              "      <td>0.669218</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.670203</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.668074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52426</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.710077</td>\n",
              "      <td>0.598233</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.704481</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.628872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52427</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.747086</td>\n",
              "      <td>0.707230</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.738122</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.758396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52428</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.412218</td>\n",
              "      <td>0.345434</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.287335</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.376129</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52429 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-baaae76c-5c75-4210-afa8-477b17ea2a71')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-baaae76c-5c75-4210-afa8-477b17ea2a71 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-baaae76c-5c75-4210-afa8-477b17ea2a71');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1ca9413a-7736-4eca-b614-f1f75ee656ae\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1ca9413a-7736-4eca-b614-f1f75ee656ae')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1ca9413a-7736-4eca-b614-f1f75ee656ae button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is my yTrainData the same is my xTrainData? False\n",
            "Is my yDataTrain the same is my xDataTrain? False\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split as trsplit\n",
        "usePreviousTrainTest = False\n",
        "total_beams = _rawData.shape[1]\n",
        "K1= _rawData.shape[1]\n",
        "\n",
        "randDrawPerSample = 1\n",
        "totalSamples = int(_rawData.shape[0]*randDrawPerSample)\n",
        "trainTestRatio = 0.9\n",
        "trainDataLen = int(_rawData.shape[0] * trainTestRatio)\n",
        "trainValSplitRatio = 0.10\n",
        "xDataTrain, xDataVal, yDataTrain, yDataVal = trsplit(x_data[:trainDataLen], y_data[:trainDataLen], test_size=trainValSplitRatio,shuffle=True)\n",
        "\n",
        "xScaledData, scaler_xDataTrain = _getScaledData(xDataTrain)\n",
        "# print(scaler_xDataTrain)\n",
        "xTrainData = xScaledData.reshape(len(xScaledData), K1, 1)\n",
        "xValData = scaler_xDataTrain.transform(xDataVal).reshape(len(xDataVal), K1, 1)\n",
        "# x_test, scaler_xData = _getScaledData(x_data[trainDataLen:, :])\n",
        "xTestData = scaler_xDataTrain.transform(x_data[trainDataLen:, :]).reshape(totalSamples - trainDataLen, K1, 1)\n",
        "\n",
        "print(\" ==> after reshape: \\n     -> xDataTrain,xScaledData ==> xTrainData=%s, xDataVal==> xValData=%s, xTestData=%s\\n\\n\" %\n",
        " (xTrainData.shape, xValData.shape, xTestData.shape))\n",
        "\n",
        "\n",
        "\n",
        "yScaledData, scaler_yDataTrain = _getScaledData(yDataTrain)\n",
        "# print(scaler_yDataTrain)\n",
        "yTrainData = yScaledData.reshape(len(yScaledData), K1, 1)\n",
        "yValData = scaler_yDataTrain.transform(yDataVal).reshape(len(yDataVal), K1, 1)\n",
        "# y_test, scaler_yData = _getScaledData(y_data[trainDataLen:, :])\n",
        "yTestData = scaler_yDataTrain.transform(y_data[trainDataLen:, :]).reshape(totalSamples - trainDataLen, K1, 1)\n",
        "\n",
        "print(\" ==> after reshape: \\n     -> yDataTrain,yScaledData ==> yTrainData=%s, yDataVal==> yValData=%s, yTestData=%s\\n\\n\" %\n",
        " (yTrainData.shape, yValData.shape, yTestData.shape))\n",
        "\n",
        "# yTestData = y_data[trainDataLen:, :]\n",
        "dataLen = len(_rawData)\n",
        "trainLen = len(xTrainData)\n",
        "\n",
        "xTrain, yTrain, xVal, yVal, xTest, yTest, trainedScaler, yTestRSRPs = xTrainData, yTrainData, xValData, yValData, xTestData, yTestData, scaler_yDataTrain, _rawData[trainDataLen:,:]\n",
        "\n",
        "print('Indices: [2, 3, 5, 10, 11, 17, 19, 31]')\n",
        "\n",
        "print('yTrainData:')\n",
        "display(pd.DataFrame(yTrainData.reshape(len(yTrainData), 32)))\n",
        "print('xTrainData:')\n",
        "display(pd.DataFrame(xTrainData.reshape(len(xTrainData), 32)))\n",
        "\n",
        "print('xTest:')\n",
        "plot_data_statistics(xTest,'xTest')\n",
        "display(pd.DataFrame(xTest.reshape(len(xTest), 32)))\n",
        "\n",
        "print(\"Is my yTrainData the same is my xTrainData?\",np.all(yTrainData == xTrainData))\n",
        "print(\"Is my yDataTrain the same is my xDataTrain?\",np.all(xDataTrain == yDataTrain))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xts_4F8SSKib",
        "outputId": "eca5ab59-5ee1-4c67-e055-8afbe45ca111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> inside getModel, parser.modelNameString=fcJalal\n"
          ]
        }
      ],
      "source": [
        "# Add the Directory to sys.path:\n",
        "# This step is necessary to allow Python to find your models.py file.\n",
        "sys.path.append('/content/drive/MyDrive/regression_problem')\n",
        "\n",
        "# Import the getModel Class:\n",
        "# Now, you can import the getModel class from your models.py file.\n",
        "from models import getModel\n",
        "\n",
        "\n",
        "totalBeams = _rawData.shape[1]\n",
        "ipCh = 1\n",
        "ipFilter = 32\n",
        "kerWid = 3\n",
        "kerLen = 3\n",
        "fcLayer1s = 1024\n",
        "fcLayer2s = 512\n",
        "ipLength = 32\n",
        "ipWidth = 1\n",
        "modelNameString ='fcJalal'\n",
        "modelClassIns = getModel(_opDim=_rawData.shape[1])\n",
        "\n",
        "def _getModel():\n",
        "    print(' --> inside getModel, parser.modelNameString=%s' % modelNameString)\n",
        "    if modelNameString=='cnn':\n",
        "        model = modelClassIns.getCnnModel(ipCh=ipCh, ipLen=ipLength, ipWid=ipWidth,\n",
        "                                                _inputFilter=ipFilter, _kerLen=kerLen,\n",
        "                                                _kerWid=kerWid)\n",
        "\n",
        "    elif modelNameString=='fc':\n",
        "        model = modelClassIns.getFCModel(ipCh=ipCh, ipLen=ipLength, ipWid=ipWidth,\n",
        "                                         fcLayer1s=fcLayer1s,fcLayer2s=fcLayer2s)\n",
        "\n",
        "    # What is the ceiling of the performance for the FC model architecture\n",
        "    elif modelNameString=='fc_ceiling':\n",
        "        model = modelClassIns.getFCModel_ceiling(ipCh=ipCh, ipLen=ipLength, ipWid=ipWidth,\n",
        "                                                 fcLayer1s=fcLayer1s,fcLayer2s=fcLayer2s)\n",
        "\n",
        "    elif modelNameString=='fcJalal':\n",
        "        model = modelClassIns.getFCModelJalal1(ipCh=ipCh, ipLen=ipLength, ipWid=ipWidth,\n",
        "                                               fcLayer1s=fcLayer1s,fcLayer2s=fcLayer2s)\n",
        "\n",
        "    # What is the ceiling of the performance for the simplest model architecture\n",
        "    elif modelNameString=='fcJalal1_ceiling':\n",
        "        model = modelClassIns.getFCModelJalal1_ceiling(ipCh=ipCh, ipLen=ipLength, ipWid=ipWidth,\n",
        "                                                       fcLayer1s=fcLayer1s,fcLayer2s=fcLayer2s)\n",
        "\n",
        "    #add Batch Normalization (BN)\n",
        "    elif modelNameString=='fcJalal2':\n",
        "        model = modelClassIns.getFCModelJalal2(ipCh=ipCh, ipLen=ipLength, ipWid=ipWidth,\n",
        "                                               fcLayer1s=fcLayer1s,fcLayer2s=fcLayer2s)\n",
        "\n",
        "    # increasing # of layers\n",
        "    elif modelNameString=='fcJalal3':\n",
        "        model = modelClassIns.getFCModelJalal3(ipCh=ipCh, ipLen=ipLength, ipWid=ipWidth,\n",
        "                                               fcLayer1s=fcLayer1s,fcLayer2s=fcLayer2s)\n",
        "\n",
        "    # What is the ceiling of the performance for the best model architecture\n",
        "    elif modelNameString=='fcJalal3_ceiling':\n",
        "        model = modelClassIns.getFCModelJalal3_ceiling(ipCh=ipCh, ipLen=ipLength, ipWid=ipWidth,\n",
        "                                                       fcLayer1s=fcLayer1s,fcLayer2s=fcLayer2s)\n",
        "\n",
        "    #increasing # of neurons\n",
        "    elif modelNameString=='fcJalal4':\n",
        "        model = modelClassIns.getFCModelJalal4(ipCh=ipCh, ipLen=ipLength, ipWid=ipWidth,\n",
        "                                               fcLayer1s=fcLayer1s, fcLayer2s=fcLayer2s)\n",
        "\n",
        "    # with leakyRelu\n",
        "    elif modelNameString=='fcJalal5':\n",
        "        model = modelClassIns.getFCModelJalal5(ipCh=ipCh, ipLen=ipLength, ipWid=ipWidth,\n",
        "                                               fcLayer1s=fcLayer1s,fcLayer2s=fcLayer2s)\n",
        "\n",
        "    # print(\"Model Summary\", model.summary())\n",
        "\n",
        "    return model\n",
        "\n",
        "model = _getModel()\n",
        "custom_KPI = CustomKPI()\n",
        "\n",
        "# yhat = model.predict(xTrain)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# o\tPrediction input: raw, scaled, direct input to the model\n",
        "print('xTrain')\n",
        "display(pd.DataFrame(xTrain.reshape(len(xTrain), 32)))\n",
        "print('yTrain')\n",
        "display(pd.DataFrame(yTrain.reshape(len(yTrain), 32)))\n",
        "print('xVal')\n",
        "display(pd.DataFrame(xVal.reshape(len(yVal), 32)))\n",
        "print('yVal')\n",
        "display(pd.DataFrame(yVal.reshape(len(yVal), 32)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9k-enMeZYHJP",
        "outputId": "1cb53677-c0db-45db-d879-9ffb601f3325"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xTrain\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         0    1         2         3    4         5    6    7    8    9   ...  \\\n",
              "0       0.0  0.0  0.390028  0.457215  0.0  0.386982  0.0  0.0  0.0  0.0  ...   \n",
              "1       0.0  0.0  0.645584  0.685337  0.0  0.687604  0.0  0.0  0.0  0.0  ...   \n",
              "2       0.0  0.0  0.752972  0.684181  0.0  0.661112  0.0  0.0  0.0  0.0  ...   \n",
              "3       0.0  0.0  0.652438  0.689942  0.0  0.684422  0.0  0.0  0.0  0.0  ...   \n",
              "4       0.0  0.0  0.636943  0.707400  0.0  0.736688  0.0  0.0  0.0  0.0  ...   \n",
              "...     ...  ...       ...       ...  ...       ...  ...  ...  ...  ...  ...   \n",
              "424668  0.0  0.0  0.600883  0.683433  0.0  0.759644  0.0  0.0  0.0  0.0  ...   \n",
              "424669  0.0  0.0  0.667273  0.624564  0.0  0.664439  0.0  0.0  0.0  0.0  ...   \n",
              "424670  0.0  0.0  0.711464  0.671408  0.0  0.680436  0.0  0.0  0.0  0.0  ...   \n",
              "424671  0.0  0.0  0.696021  0.768375  0.0  0.622659  0.0  0.0  0.0  0.0  ...   \n",
              "424672  0.0  0.0  0.710649  0.704004  0.0  0.566945  0.0  0.0  0.0  0.0  ...   \n",
              "\n",
              "         22   23   24   25   26   27   28   29   30        31  \n",
              "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.484721  \n",
              "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.651237  \n",
              "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.683787  \n",
              "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.662135  \n",
              "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.805198  \n",
              "...     ...  ...  ...  ...  ...  ...  ...  ...  ...       ...  \n",
              "424668  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.602619  \n",
              "424669  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.590760  \n",
              "424670  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.642989  \n",
              "424671  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.745216  \n",
              "424672  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.683709  \n",
              "\n",
              "[424673 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa974798-4510-47b1-b064-493551687bd3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.390028</td>\n",
              "      <td>0.457215</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.386982</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.484721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.645584</td>\n",
              "      <td>0.685337</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.687604</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.651237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.752972</td>\n",
              "      <td>0.684181</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.661112</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.683787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.652438</td>\n",
              "      <td>0.689942</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.684422</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.662135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.636943</td>\n",
              "      <td>0.707400</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.736688</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.805198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424668</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.600883</td>\n",
              "      <td>0.683433</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.759644</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.602619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424669</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.667273</td>\n",
              "      <td>0.624564</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.664439</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.590760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424670</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.711464</td>\n",
              "      <td>0.671408</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.680436</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.642989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424671</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.696021</td>\n",
              "      <td>0.768375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.622659</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.745216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424672</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.710649</td>\n",
              "      <td>0.704004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.566945</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.683709</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>424673 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa974798-4510-47b1-b064-493551687bd3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aa974798-4510-47b1-b064-493551687bd3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aa974798-4510-47b1-b064-493551687bd3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bdc5cb0f-e7d0-4c63-bf12-937d615b0f26\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bdc5cb0f-e7d0-4c63-bf12-937d615b0f26')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bdc5cb0f-e7d0-4c63-bf12-937d615b0f26 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yTrain\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6   \\\n",
              "0       0.406902  0.392012  0.390028  0.457215  0.401168  0.386982  0.450856   \n",
              "1       0.777527  0.711429  0.645584  0.685337  0.789529  0.687604  0.648294   \n",
              "2       0.711286  0.681307  0.752972  0.684181  0.666503  0.661112  0.704636   \n",
              "3       0.599443  0.656779  0.652438  0.689942  0.735209  0.684422  0.667135   \n",
              "4       0.638695  0.594968  0.636943  0.707400  0.634925  0.736688  0.769632   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "424668  0.772937  0.701956  0.600883  0.683433  0.824450  0.759644  0.693519   \n",
              "424669  0.682404  0.692280  0.667273  0.624564  0.654651  0.664439  0.613516   \n",
              "424670  0.632162  0.737093  0.711464  0.671408  0.631695  0.680436  0.646090   \n",
              "424671  0.672567  0.690898  0.696021  0.768375  0.643788  0.622659  0.654690   \n",
              "424672  0.770945  0.715777  0.710649  0.704004  0.808796  0.566945  0.645552   \n",
              "\n",
              "              7         8         9   ...        22        23        24  \\\n",
              "0       0.430801  0.436325  0.387880  ...  0.411647  0.437074  0.399029   \n",
              "1       0.708204  0.771874  0.689922  ...  0.644622  0.732900  0.815446   \n",
              "2       0.668516  0.733276  0.701697  ...  0.721700  0.712694  0.687341   \n",
              "3       0.786497  0.648669  0.604740  ...  0.631862  0.720130  0.701764   \n",
              "4       0.842367  0.614178  0.549434  ...  0.706250  0.755031  0.570652   \n",
              "...          ...       ...       ...  ...       ...       ...       ...   \n",
              "424668  0.686911  0.726741  0.671068  ...  0.566615  0.650046  0.802510   \n",
              "424669  0.631959  0.695639  0.672653  ...  0.619481  0.648907  0.617844   \n",
              "424670  0.609568  0.650943  0.677125  ...  0.685677  0.681701  0.617377   \n",
              "424671  0.674555  0.640582  0.610347  ...  0.670599  0.721592  0.642292   \n",
              "424672  0.728701  0.821590  0.668461  ...  0.641548  0.761739  0.796355   \n",
              "\n",
              "              25        26        27        28        29        30        31  \n",
              "0       0.378055  0.483082  0.452112  0.405860  0.403140  0.440933  0.484721  \n",
              "1       0.739966  0.691955  0.715568  0.736122  0.669998  0.651836  0.651237  \n",
              "2       0.669075  0.743057  0.667719  0.710833  0.691024  0.790427  0.683787  \n",
              "3       0.654651  0.662740  0.749254  0.604362  0.624862  0.657597  0.662135  \n",
              "4       0.664444  0.702813  0.766688  0.727392  0.662770  0.757457  0.805198  \n",
              "...          ...       ...       ...       ...       ...       ...       ...  \n",
              "424668  0.738192  0.675667  0.657159  0.684970  0.630829  0.571534  0.602619  \n",
              "424669  0.643519  0.630881  0.620515  0.669455  0.683276  0.686585  0.590760  \n",
              "424670  0.665863  0.646974  0.606469  0.634668  0.720018  0.694595  0.642989  \n",
              "424671  0.628853  0.674823  0.638445  0.644152  0.671320  0.706751  0.745216  \n",
              "424672  0.565831  0.636776  0.710073  0.745206  0.693924  0.709269  0.683709  \n",
              "\n",
              "[424673 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7dd5ed5a-c027-43a1-a2e6-954715c018b4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.406902</td>\n",
              "      <td>0.392012</td>\n",
              "      <td>0.390028</td>\n",
              "      <td>0.457215</td>\n",
              "      <td>0.401168</td>\n",
              "      <td>0.386982</td>\n",
              "      <td>0.450856</td>\n",
              "      <td>0.430801</td>\n",
              "      <td>0.436325</td>\n",
              "      <td>0.387880</td>\n",
              "      <td>...</td>\n",
              "      <td>0.411647</td>\n",
              "      <td>0.437074</td>\n",
              "      <td>0.399029</td>\n",
              "      <td>0.378055</td>\n",
              "      <td>0.483082</td>\n",
              "      <td>0.452112</td>\n",
              "      <td>0.405860</td>\n",
              "      <td>0.403140</td>\n",
              "      <td>0.440933</td>\n",
              "      <td>0.484721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.777527</td>\n",
              "      <td>0.711429</td>\n",
              "      <td>0.645584</td>\n",
              "      <td>0.685337</td>\n",
              "      <td>0.789529</td>\n",
              "      <td>0.687604</td>\n",
              "      <td>0.648294</td>\n",
              "      <td>0.708204</td>\n",
              "      <td>0.771874</td>\n",
              "      <td>0.689922</td>\n",
              "      <td>...</td>\n",
              "      <td>0.644622</td>\n",
              "      <td>0.732900</td>\n",
              "      <td>0.815446</td>\n",
              "      <td>0.739966</td>\n",
              "      <td>0.691955</td>\n",
              "      <td>0.715568</td>\n",
              "      <td>0.736122</td>\n",
              "      <td>0.669998</td>\n",
              "      <td>0.651836</td>\n",
              "      <td>0.651237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.711286</td>\n",
              "      <td>0.681307</td>\n",
              "      <td>0.752972</td>\n",
              "      <td>0.684181</td>\n",
              "      <td>0.666503</td>\n",
              "      <td>0.661112</td>\n",
              "      <td>0.704636</td>\n",
              "      <td>0.668516</td>\n",
              "      <td>0.733276</td>\n",
              "      <td>0.701697</td>\n",
              "      <td>...</td>\n",
              "      <td>0.721700</td>\n",
              "      <td>0.712694</td>\n",
              "      <td>0.687341</td>\n",
              "      <td>0.669075</td>\n",
              "      <td>0.743057</td>\n",
              "      <td>0.667719</td>\n",
              "      <td>0.710833</td>\n",
              "      <td>0.691024</td>\n",
              "      <td>0.790427</td>\n",
              "      <td>0.683787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.599443</td>\n",
              "      <td>0.656779</td>\n",
              "      <td>0.652438</td>\n",
              "      <td>0.689942</td>\n",
              "      <td>0.735209</td>\n",
              "      <td>0.684422</td>\n",
              "      <td>0.667135</td>\n",
              "      <td>0.786497</td>\n",
              "      <td>0.648669</td>\n",
              "      <td>0.604740</td>\n",
              "      <td>...</td>\n",
              "      <td>0.631862</td>\n",
              "      <td>0.720130</td>\n",
              "      <td>0.701764</td>\n",
              "      <td>0.654651</td>\n",
              "      <td>0.662740</td>\n",
              "      <td>0.749254</td>\n",
              "      <td>0.604362</td>\n",
              "      <td>0.624862</td>\n",
              "      <td>0.657597</td>\n",
              "      <td>0.662135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.638695</td>\n",
              "      <td>0.594968</td>\n",
              "      <td>0.636943</td>\n",
              "      <td>0.707400</td>\n",
              "      <td>0.634925</td>\n",
              "      <td>0.736688</td>\n",
              "      <td>0.769632</td>\n",
              "      <td>0.842367</td>\n",
              "      <td>0.614178</td>\n",
              "      <td>0.549434</td>\n",
              "      <td>...</td>\n",
              "      <td>0.706250</td>\n",
              "      <td>0.755031</td>\n",
              "      <td>0.570652</td>\n",
              "      <td>0.664444</td>\n",
              "      <td>0.702813</td>\n",
              "      <td>0.766688</td>\n",
              "      <td>0.727392</td>\n",
              "      <td>0.662770</td>\n",
              "      <td>0.757457</td>\n",
              "      <td>0.805198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424668</th>\n",
              "      <td>0.772937</td>\n",
              "      <td>0.701956</td>\n",
              "      <td>0.600883</td>\n",
              "      <td>0.683433</td>\n",
              "      <td>0.824450</td>\n",
              "      <td>0.759644</td>\n",
              "      <td>0.693519</td>\n",
              "      <td>0.686911</td>\n",
              "      <td>0.726741</td>\n",
              "      <td>0.671068</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566615</td>\n",
              "      <td>0.650046</td>\n",
              "      <td>0.802510</td>\n",
              "      <td>0.738192</td>\n",
              "      <td>0.675667</td>\n",
              "      <td>0.657159</td>\n",
              "      <td>0.684970</td>\n",
              "      <td>0.630829</td>\n",
              "      <td>0.571534</td>\n",
              "      <td>0.602619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424669</th>\n",
              "      <td>0.682404</td>\n",
              "      <td>0.692280</td>\n",
              "      <td>0.667273</td>\n",
              "      <td>0.624564</td>\n",
              "      <td>0.654651</td>\n",
              "      <td>0.664439</td>\n",
              "      <td>0.613516</td>\n",
              "      <td>0.631959</td>\n",
              "      <td>0.695639</td>\n",
              "      <td>0.672653</td>\n",
              "      <td>...</td>\n",
              "      <td>0.619481</td>\n",
              "      <td>0.648907</td>\n",
              "      <td>0.617844</td>\n",
              "      <td>0.643519</td>\n",
              "      <td>0.630881</td>\n",
              "      <td>0.620515</td>\n",
              "      <td>0.669455</td>\n",
              "      <td>0.683276</td>\n",
              "      <td>0.686585</td>\n",
              "      <td>0.590760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424670</th>\n",
              "      <td>0.632162</td>\n",
              "      <td>0.737093</td>\n",
              "      <td>0.711464</td>\n",
              "      <td>0.671408</td>\n",
              "      <td>0.631695</td>\n",
              "      <td>0.680436</td>\n",
              "      <td>0.646090</td>\n",
              "      <td>0.609568</td>\n",
              "      <td>0.650943</td>\n",
              "      <td>0.677125</td>\n",
              "      <td>...</td>\n",
              "      <td>0.685677</td>\n",
              "      <td>0.681701</td>\n",
              "      <td>0.617377</td>\n",
              "      <td>0.665863</td>\n",
              "      <td>0.646974</td>\n",
              "      <td>0.606469</td>\n",
              "      <td>0.634668</td>\n",
              "      <td>0.720018</td>\n",
              "      <td>0.694595</td>\n",
              "      <td>0.642989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424671</th>\n",
              "      <td>0.672567</td>\n",
              "      <td>0.690898</td>\n",
              "      <td>0.696021</td>\n",
              "      <td>0.768375</td>\n",
              "      <td>0.643788</td>\n",
              "      <td>0.622659</td>\n",
              "      <td>0.654690</td>\n",
              "      <td>0.674555</td>\n",
              "      <td>0.640582</td>\n",
              "      <td>0.610347</td>\n",
              "      <td>...</td>\n",
              "      <td>0.670599</td>\n",
              "      <td>0.721592</td>\n",
              "      <td>0.642292</td>\n",
              "      <td>0.628853</td>\n",
              "      <td>0.674823</td>\n",
              "      <td>0.638445</td>\n",
              "      <td>0.644152</td>\n",
              "      <td>0.671320</td>\n",
              "      <td>0.706751</td>\n",
              "      <td>0.745216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424672</th>\n",
              "      <td>0.770945</td>\n",
              "      <td>0.715777</td>\n",
              "      <td>0.710649</td>\n",
              "      <td>0.704004</td>\n",
              "      <td>0.808796</td>\n",
              "      <td>0.566945</td>\n",
              "      <td>0.645552</td>\n",
              "      <td>0.728701</td>\n",
              "      <td>0.821590</td>\n",
              "      <td>0.668461</td>\n",
              "      <td>...</td>\n",
              "      <td>0.641548</td>\n",
              "      <td>0.761739</td>\n",
              "      <td>0.796355</td>\n",
              "      <td>0.565831</td>\n",
              "      <td>0.636776</td>\n",
              "      <td>0.710073</td>\n",
              "      <td>0.745206</td>\n",
              "      <td>0.693924</td>\n",
              "      <td>0.709269</td>\n",
              "      <td>0.683709</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>424673 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7dd5ed5a-c027-43a1-a2e6-954715c018b4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7dd5ed5a-c027-43a1-a2e6-954715c018b4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7dd5ed5a-c027-43a1-a2e6-954715c018b4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-222df5df-8a0b-46f1-a44d-bf16eb430de5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-222df5df-8a0b-46f1-a44d-bf16eb430de5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-222df5df-8a0b-46f1-a44d-bf16eb430de5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xVal\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        0    1         2         3    4         5    6    7    8    9   ...  \\\n",
              "0      0.0  0.0  0.689451  0.754834  0.0  0.603231  0.0  0.0  0.0  0.0  ...   \n",
              "1      0.0  0.0  0.612186  0.621414  0.0  0.619122  0.0  0.0  0.0  0.0  ...   \n",
              "2      0.0  0.0  0.754830  0.674236  0.0  0.741145  0.0  0.0  0.0  0.0  ...   \n",
              "3      0.0  0.0  0.701725  0.617859  0.0  0.580227  0.0  0.0  0.0  0.0  ...   \n",
              "4      0.0  0.0  0.744177  0.758867  0.0  0.614804  0.0  0.0  0.0  0.0  ...   \n",
              "...    ...  ...       ...       ...  ...       ...  ...  ...  ...  ...  ...   \n",
              "47181  0.0  0.0  0.531791  0.534323  0.0  0.487540  0.0  0.0  0.0  0.0  ...   \n",
              "47182  0.0  0.0  0.695377  0.703931  0.0  0.698951  0.0  0.0  0.0  0.0  ...   \n",
              "47183  0.0  0.0  0.776749  0.848687  0.0  0.694070  0.0  0.0  0.0  0.0  ...   \n",
              "47184  0.0  0.0  0.599946  0.649415  0.0  0.788975  0.0  0.0  0.0  0.0  ...   \n",
              "47185  0.0  0.0  0.589787  0.595175  0.0  0.707383  0.0  0.0  0.0  0.0  ...   \n",
              "\n",
              "        22   23   24   25   26   27   28   29   30        31  \n",
              "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.764392  \n",
              "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.628266  \n",
              "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.622590  \n",
              "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.610846  \n",
              "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.741831  \n",
              "...    ...  ...  ...  ...  ...  ...  ...  ...  ...       ...  \n",
              "47181  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.526410  \n",
              "47182  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.640210  \n",
              "47183  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.762720  \n",
              "47184  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.602839  \n",
              "47185  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.562583  \n",
              "\n",
              "[47186 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b42fde1f-bceb-4586-9a49-2f68338bba96\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.689451</td>\n",
              "      <td>0.754834</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.603231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.764392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.612186</td>\n",
              "      <td>0.621414</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.619122</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.628266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.754830</td>\n",
              "      <td>0.674236</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.741145</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.622590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.701725</td>\n",
              "      <td>0.617859</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.580227</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.610846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.744177</td>\n",
              "      <td>0.758867</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.614804</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.741831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47181</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.531791</td>\n",
              "      <td>0.534323</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.487540</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.526410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47182</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.695377</td>\n",
              "      <td>0.703931</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.698951</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.640210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47183</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.776749</td>\n",
              "      <td>0.848687</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.694070</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.762720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47184</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.599946</td>\n",
              "      <td>0.649415</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.788975</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.602839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47185</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.589787</td>\n",
              "      <td>0.595175</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.707383</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.562583</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>47186 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b42fde1f-bceb-4586-9a49-2f68338bba96')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b42fde1f-bceb-4586-9a49-2f68338bba96 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b42fde1f-bceb-4586-9a49-2f68338bba96');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-49038399-1509-4615-8b5e-e3c1f737cd7f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-49038399-1509-4615-8b5e-e3c1f737cd7f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-49038399-1509-4615-8b5e-e3c1f737cd7f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yVal\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6   \\\n",
              "0      0.683421  0.663505  0.689451  0.754834  0.649772  0.603231  0.616968   \n",
              "1      0.681910  0.677110  0.612186  0.621414  0.670194  0.619122  0.562335   \n",
              "2      0.691839  0.709186  0.754830  0.674236  0.727468  0.741145  0.782562   \n",
              "3      0.644631  0.644718  0.701725  0.617859  0.575283  0.580227  0.617248   \n",
              "4      0.672891  0.695126  0.744177  0.758867  0.654293  0.614804  0.683107   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "47181  0.492625  0.487544  0.531791  0.534323  0.522712  0.487540  0.580178   \n",
              "47182  0.727507  0.792243  0.695377  0.703931  0.694295  0.698951  0.593872   \n",
              "47183  0.774935  0.785789  0.776749  0.848687  0.731481  0.694070  0.645255   \n",
              "47184  0.613846  0.724927  0.599946  0.649415  0.742537  0.788975  0.640426   \n",
              "47185  0.657644  0.703286  0.589787  0.595175  0.626650  0.707383  0.588427   \n",
              "\n",
              "             7         8         9   ...        22        23        24  \\\n",
              "0      0.665190  0.728015  0.642946  ...  0.635176  0.713678  0.662707   \n",
              "1      0.591382  0.716213  0.695159  ...  0.550426  0.642725  0.692542   \n",
              "2      0.743234  0.723423  0.682205  ...  0.662950  0.673820  0.716799   \n",
              "3      0.575170  0.609573  0.591356  ...  0.654734  0.638846  0.554665   \n",
              "4      0.646131  0.674243  0.617127  ...  0.725220  0.729704  0.638460   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "47181  0.551118  0.498992  0.465855  ...  0.533855  0.518959  0.505254   \n",
              "47182  0.626429  0.716263  0.736978  ...  0.664085  0.737904  0.698466   \n",
              "47183  0.666922  0.790775  0.731123  ...  0.846252  0.866291  0.775697   \n",
              "47184  0.612112  0.624821  0.726597  ...  0.578987  0.646655  0.732746   \n",
              "47185  0.604921  0.638051  0.645617  ...  0.586398  0.641148  0.600941   \n",
              "\n",
              "             25        26        27        28        29        30        31  \n",
              "0      0.609690  0.628564  0.682903  0.682409  0.652133  0.716488  0.764392  \n",
              "1      0.642891  0.574664  0.603442  0.636962  0.689559  0.600884  0.628266  \n",
              "2      0.733944  0.775438  0.727316  0.646189  0.667120  0.724240  0.622590  \n",
              "3      0.576133  0.616568  0.577170  0.617925  0.642227  0.688729  0.610846  \n",
              "4      0.620610  0.671673  0.630659  0.654017  0.674432  0.753149  0.741831  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "47181  0.482280  0.566937  0.522734  0.479487  0.459935  0.518799  0.526410  \n",
              "47182  0.755249  0.627011  0.633216  0.638969  0.668260  0.685044  0.640210  \n",
              "47183  0.757032  0.788441  0.761341  0.705581  0.719483  0.717602  0.762720  \n",
              "47184  0.782378  0.636849  0.593905  0.579813  0.677074  0.574520  0.602839  \n",
              "47185  0.661129  0.588972  0.568712  0.622767  0.687307  0.614770  0.562583  \n",
              "\n",
              "[47186 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d3d5cdd0-a1fe-48b3-b98a-f747ccee59f4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.683421</td>\n",
              "      <td>0.663505</td>\n",
              "      <td>0.689451</td>\n",
              "      <td>0.754834</td>\n",
              "      <td>0.649772</td>\n",
              "      <td>0.603231</td>\n",
              "      <td>0.616968</td>\n",
              "      <td>0.665190</td>\n",
              "      <td>0.728015</td>\n",
              "      <td>0.642946</td>\n",
              "      <td>...</td>\n",
              "      <td>0.635176</td>\n",
              "      <td>0.713678</td>\n",
              "      <td>0.662707</td>\n",
              "      <td>0.609690</td>\n",
              "      <td>0.628564</td>\n",
              "      <td>0.682903</td>\n",
              "      <td>0.682409</td>\n",
              "      <td>0.652133</td>\n",
              "      <td>0.716488</td>\n",
              "      <td>0.764392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.681910</td>\n",
              "      <td>0.677110</td>\n",
              "      <td>0.612186</td>\n",
              "      <td>0.621414</td>\n",
              "      <td>0.670194</td>\n",
              "      <td>0.619122</td>\n",
              "      <td>0.562335</td>\n",
              "      <td>0.591382</td>\n",
              "      <td>0.716213</td>\n",
              "      <td>0.695159</td>\n",
              "      <td>...</td>\n",
              "      <td>0.550426</td>\n",
              "      <td>0.642725</td>\n",
              "      <td>0.692542</td>\n",
              "      <td>0.642891</td>\n",
              "      <td>0.574664</td>\n",
              "      <td>0.603442</td>\n",
              "      <td>0.636962</td>\n",
              "      <td>0.689559</td>\n",
              "      <td>0.600884</td>\n",
              "      <td>0.628266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.691839</td>\n",
              "      <td>0.709186</td>\n",
              "      <td>0.754830</td>\n",
              "      <td>0.674236</td>\n",
              "      <td>0.727468</td>\n",
              "      <td>0.741145</td>\n",
              "      <td>0.782562</td>\n",
              "      <td>0.743234</td>\n",
              "      <td>0.723423</td>\n",
              "      <td>0.682205</td>\n",
              "      <td>...</td>\n",
              "      <td>0.662950</td>\n",
              "      <td>0.673820</td>\n",
              "      <td>0.716799</td>\n",
              "      <td>0.733944</td>\n",
              "      <td>0.775438</td>\n",
              "      <td>0.727316</td>\n",
              "      <td>0.646189</td>\n",
              "      <td>0.667120</td>\n",
              "      <td>0.724240</td>\n",
              "      <td>0.622590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.644631</td>\n",
              "      <td>0.644718</td>\n",
              "      <td>0.701725</td>\n",
              "      <td>0.617859</td>\n",
              "      <td>0.575283</td>\n",
              "      <td>0.580227</td>\n",
              "      <td>0.617248</td>\n",
              "      <td>0.575170</td>\n",
              "      <td>0.609573</td>\n",
              "      <td>0.591356</td>\n",
              "      <td>...</td>\n",
              "      <td>0.654734</td>\n",
              "      <td>0.638846</td>\n",
              "      <td>0.554665</td>\n",
              "      <td>0.576133</td>\n",
              "      <td>0.616568</td>\n",
              "      <td>0.577170</td>\n",
              "      <td>0.617925</td>\n",
              "      <td>0.642227</td>\n",
              "      <td>0.688729</td>\n",
              "      <td>0.610846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.672891</td>\n",
              "      <td>0.695126</td>\n",
              "      <td>0.744177</td>\n",
              "      <td>0.758867</td>\n",
              "      <td>0.654293</td>\n",
              "      <td>0.614804</td>\n",
              "      <td>0.683107</td>\n",
              "      <td>0.646131</td>\n",
              "      <td>0.674243</td>\n",
              "      <td>0.617127</td>\n",
              "      <td>...</td>\n",
              "      <td>0.725220</td>\n",
              "      <td>0.729704</td>\n",
              "      <td>0.638460</td>\n",
              "      <td>0.620610</td>\n",
              "      <td>0.671673</td>\n",
              "      <td>0.630659</td>\n",
              "      <td>0.654017</td>\n",
              "      <td>0.674432</td>\n",
              "      <td>0.753149</td>\n",
              "      <td>0.741831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47181</th>\n",
              "      <td>0.492625</td>\n",
              "      <td>0.487544</td>\n",
              "      <td>0.531791</td>\n",
              "      <td>0.534323</td>\n",
              "      <td>0.522712</td>\n",
              "      <td>0.487540</td>\n",
              "      <td>0.580178</td>\n",
              "      <td>0.551118</td>\n",
              "      <td>0.498992</td>\n",
              "      <td>0.465855</td>\n",
              "      <td>...</td>\n",
              "      <td>0.533855</td>\n",
              "      <td>0.518959</td>\n",
              "      <td>0.505254</td>\n",
              "      <td>0.482280</td>\n",
              "      <td>0.566937</td>\n",
              "      <td>0.522734</td>\n",
              "      <td>0.479487</td>\n",
              "      <td>0.459935</td>\n",
              "      <td>0.518799</td>\n",
              "      <td>0.526410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47182</th>\n",
              "      <td>0.727507</td>\n",
              "      <td>0.792243</td>\n",
              "      <td>0.695377</td>\n",
              "      <td>0.703931</td>\n",
              "      <td>0.694295</td>\n",
              "      <td>0.698951</td>\n",
              "      <td>0.593872</td>\n",
              "      <td>0.626429</td>\n",
              "      <td>0.716263</td>\n",
              "      <td>0.736978</td>\n",
              "      <td>...</td>\n",
              "      <td>0.664085</td>\n",
              "      <td>0.737904</td>\n",
              "      <td>0.698466</td>\n",
              "      <td>0.755249</td>\n",
              "      <td>0.627011</td>\n",
              "      <td>0.633216</td>\n",
              "      <td>0.638969</td>\n",
              "      <td>0.668260</td>\n",
              "      <td>0.685044</td>\n",
              "      <td>0.640210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47183</th>\n",
              "      <td>0.774935</td>\n",
              "      <td>0.785789</td>\n",
              "      <td>0.776749</td>\n",
              "      <td>0.848687</td>\n",
              "      <td>0.731481</td>\n",
              "      <td>0.694070</td>\n",
              "      <td>0.645255</td>\n",
              "      <td>0.666922</td>\n",
              "      <td>0.790775</td>\n",
              "      <td>0.731123</td>\n",
              "      <td>...</td>\n",
              "      <td>0.846252</td>\n",
              "      <td>0.866291</td>\n",
              "      <td>0.775697</td>\n",
              "      <td>0.757032</td>\n",
              "      <td>0.788441</td>\n",
              "      <td>0.761341</td>\n",
              "      <td>0.705581</td>\n",
              "      <td>0.719483</td>\n",
              "      <td>0.717602</td>\n",
              "      <td>0.762720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47184</th>\n",
              "      <td>0.613846</td>\n",
              "      <td>0.724927</td>\n",
              "      <td>0.599946</td>\n",
              "      <td>0.649415</td>\n",
              "      <td>0.742537</td>\n",
              "      <td>0.788975</td>\n",
              "      <td>0.640426</td>\n",
              "      <td>0.612112</td>\n",
              "      <td>0.624821</td>\n",
              "      <td>0.726597</td>\n",
              "      <td>...</td>\n",
              "      <td>0.578987</td>\n",
              "      <td>0.646655</td>\n",
              "      <td>0.732746</td>\n",
              "      <td>0.782378</td>\n",
              "      <td>0.636849</td>\n",
              "      <td>0.593905</td>\n",
              "      <td>0.579813</td>\n",
              "      <td>0.677074</td>\n",
              "      <td>0.574520</td>\n",
              "      <td>0.602839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47185</th>\n",
              "      <td>0.657644</td>\n",
              "      <td>0.703286</td>\n",
              "      <td>0.589787</td>\n",
              "      <td>0.595175</td>\n",
              "      <td>0.626650</td>\n",
              "      <td>0.707383</td>\n",
              "      <td>0.588427</td>\n",
              "      <td>0.604921</td>\n",
              "      <td>0.638051</td>\n",
              "      <td>0.645617</td>\n",
              "      <td>...</td>\n",
              "      <td>0.586398</td>\n",
              "      <td>0.641148</td>\n",
              "      <td>0.600941</td>\n",
              "      <td>0.661129</td>\n",
              "      <td>0.588972</td>\n",
              "      <td>0.568712</td>\n",
              "      <td>0.622767</td>\n",
              "      <td>0.687307</td>\n",
              "      <td>0.614770</td>\n",
              "      <td>0.562583</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>47186 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3d5cdd0-a1fe-48b3-b98a-f747ccee59f4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d3d5cdd0-a1fe-48b3-b98a-f747ccee59f4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d3d5cdd0-a1fe-48b3-b98a-f747ccee59f4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fdb63c15-7804-4d64-a25f-75fa10cbff3a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fdb63c15-7804-4d64-a25f-75fa10cbff3a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fdb63c15-7804-4d64-a25f-75fa10cbff3a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G40kfcrl7Wty",
        "outputId": "bbb98dc1-003b-4914-82b6-58390adbb370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --> inside getModel, parser.modelNameString=fcJalal\n",
            "\n",
            "\n",
            " -> before calling history, batch_size=512\n",
            "\n",
            "\n",
            "Epoch 1/10\n",
            "830/830 [==============================] - 6s 4ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - mean_absolute_error: 0.0463 - val_loss: 0.0010 - val_mean_squared_error: 0.0010 - val_mean_absolute_error: 0.0227\n",
            "Epoch 2/10\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - mean_absolute_error: 0.0302 - val_loss: 9.0605e-04 - val_mean_squared_error: 9.0605e-04 - val_mean_absolute_error: 0.0217\n",
            "Epoch 3/10\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0264 - val_loss: 7.3279e-04 - val_mean_squared_error: 7.3279e-04 - val_mean_absolute_error: 0.0187\n",
            "Epoch 4/10\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - mean_absolute_error: 0.0239 - val_loss: 6.4617e-04 - val_mean_squared_error: 6.4617e-04 - val_mean_absolute_error: 0.0171\n",
            "Epoch 5/10\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 9.0351e-04 - mean_squared_error: 9.0351e-04 - mean_absolute_error: 0.0223 - val_loss: 6.9117e-04 - val_mean_squared_error: 6.9117e-04 - val_mean_absolute_error: 0.0190\n",
            "Epoch 6/10\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 8.3059e-04 - mean_squared_error: 8.3059e-04 - mean_absolute_error: 0.0212 - val_loss: 6.2699e-04 - val_mean_squared_error: 6.2699e-04 - val_mean_absolute_error: 0.0176\n",
            "Epoch 7/10\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 7.7553e-04 - mean_squared_error: 7.7553e-04 - mean_absolute_error: 0.0204 - val_loss: 5.6435e-04 - val_mean_squared_error: 5.6435e-04 - val_mean_absolute_error: 0.0159\n",
            "Epoch 8/10\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 7.3198e-04 - mean_squared_error: 7.3198e-04 - mean_absolute_error: 0.0197 - val_loss: 5.5578e-04 - val_mean_squared_error: 5.5578e-04 - val_mean_absolute_error: 0.0160\n",
            "Epoch 9/10\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 7.0018e-04 - mean_squared_error: 7.0018e-04 - mean_absolute_error: 0.0191 - val_loss: 5.5250e-04 - val_mean_squared_error: 5.5250e-04 - val_mean_absolute_error: 0.0159\n",
            "Epoch 10/10\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 6.7237e-04 - mean_squared_error: 6.7237e-04 - mean_absolute_error: 0.0186 - val_loss: 5.9861e-04 - val_mean_squared_error: 5.9861e-04 - val_mean_absolute_error: 0.0177\n"
          ]
        }
      ],
      "source": [
        "# sample test\n",
        "\n",
        "epoch = 10\n",
        "batchSize = 512\n",
        "chkpointFname = '/content/drive/MyDrive/regression_problem/chkpoint'\n",
        "\n",
        "trainDatasetTFint = tf.data.Dataset.from_tensor_slices((xTrain, yTrain))\n",
        "valDatasetTFint = tf.data.Dataset.from_tensor_slices((xVal, yVal))\n",
        "\n",
        "trainDatasetTF = trainDatasetTFint.batch(batchSize)\n",
        "valDatasetTF = valDatasetTFint.batch(batchSize)\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=chkpointFname,\n",
        "    save_weights_only=False, verbose=2,  # save_weights_only=True\n",
        "    monitor='val_accuracy', mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "model = _getModel()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                    loss=tf.keras.losses.MeanSquaredError(),\n",
        "                    metrics=[tf.keras.metrics.MeanSquaredError(),\n",
        "                             tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "\n",
        "print('\\n\\n -> before calling history, batch_size=%d\\n\\n' % batchSize)\n",
        "\n",
        "history = model.fit(trainDatasetTF, epochs=epoch, validation_data=valDatasetTF,\n",
        "                          batch_size=batchSize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "iB-D9cyScHn6",
        "outputId": "e967d3bc-fdf9-4ec5-a1f2-c27111fd0853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1639/1639 [==============================] - 2s 1ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f06929ac-73a8-4d6d-b84f-065f9888c2b1\", \"xTest_model_input_first_AND_last_10_samples.xlsx\", 8575)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ea1e7f75-ce21-4b18-bbd7-32c0209ea758\", \"yTest_expected_output_first_AND_last_10_samples.xlsx\", 13280)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2cd93342-cafe-418e-85c9-71c900de2163\", \"yTest_expected_descaled_output_first_AND_last_10_samples.xlsx\", 13651)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7168a326-93ea-48e3-a00a-e97353271b8c\", \"yhat_prediction_output_first_AND_last_10_samples.xlsx\", 13202)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8e965207-c7c5-4680-9452-976e2461fdf5\", \"yhat_prediction_descaled_output_first_AND_last_10_samples.xlsx\", 12812)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "               0           1           2           3           4           5   \\\n",
              "0      -73.759132  -70.356781  -71.348427  -57.827824  -52.884880  -60.673538   \n",
              "1      -87.240303  -88.269371  -90.448570 -100.477242  -85.945274  -81.327667   \n",
              "2      -78.161964  -82.139313  -80.860580  -85.763374  -78.548416  -87.767166   \n",
              "3     -112.722305 -103.117699 -110.969032 -116.696175 -108.435555  -94.227776   \n",
              "4      -77.687965  -86.679840  -77.035896  -63.908966  -76.518509  -73.519577   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "52424 -100.181030  -94.967751  -92.441559 -103.905540  -96.787209  -84.623634   \n",
              "52425  -62.005577  -69.308548  -67.532707  -73.834259  -49.747471  -72.944809   \n",
              "52426  -77.808235  -83.271957  -62.782448  -90.099083  -73.783562  -65.643471   \n",
              "52427  -76.945435  -73.163429  -57.108143  -67.197266  -53.742378  -58.976597   \n",
              "52428 -142.714371 -142.751129 -124.894943 -136.783157 -142.221771 -143.273865   \n",
              "\n",
              "               6           7           8           9   ...          22  \\\n",
              "0      -64.963310  -53.201313  -80.622658  -84.596825  ...  -63.204018   \n",
              "1      -89.613808  -88.326118  -93.258743  -91.021446  ...  -98.545876   \n",
              "2      -85.649773  -84.181229  -84.540520  -90.089478  ...  -88.611588   \n",
              "3     -105.639740 -109.465240 -115.384224 -106.986931  ... -112.759827   \n",
              "4      -70.862404  -62.142666  -74.402962  -78.489166  ...  -77.946350   \n",
              "...           ...         ...         ...         ...  ...         ...   \n",
              "52424  -87.062279  -95.676407 -101.963753  -95.796043  ...  -95.909012   \n",
              "52425  -64.530884  -57.348843  -67.135078  -77.420677  ...  -77.007187   \n",
              "52426  -39.352760  -54.861713  -79.899506  -82.265198  ...  -51.410633   \n",
              "52427  -40.658016  -55.933849  -75.859291  -79.223839  ...  -45.504452   \n",
              "52428 -125.779831 -139.342453 -136.883118 -139.079269  ... -126.491928   \n",
              "\n",
              "               23          24          25          26          27          28  \\\n",
              "0      -57.142239  -67.710869  -71.614723  -76.373024  -66.549049  -75.274628   \n",
              "1      -98.447884  -90.269936  -84.337486  -93.393135  -92.402290  -92.555397   \n",
              "2      -86.913986  -83.287193  -89.154831  -89.125572  -88.291031  -84.831543   \n",
              "3     -116.884727 -110.928474  -96.106712 -107.266174 -111.795860 -115.523293   \n",
              "4      -76.839233  -77.542068  -72.688660  -71.544533  -63.584763  -84.621758   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "52424 -104.229362 -100.092018  -86.723083  -89.325684  -98.874657 -104.092003   \n",
              "52425  -73.838257  -64.204613  -80.847214  -76.469154  -71.935883  -62.660923   \n",
              "52426  -79.195869  -88.180817  -79.729927  -55.579609  -71.534668  -73.361931   \n",
              "52427  -58.273144  -72.473053  -73.120529  -57.341412  -72.774063  -69.129295   \n",
              "52428 -141.900467 -142.409790 -141.341675 -124.957031 -138.160873 -138.413208   \n",
              "\n",
              "               29          30          31  \n",
              "0      -65.226112  -70.779839  -57.394985  \n",
              "1      -88.168800  -95.743973 -103.297607  \n",
              "2      -83.170570  -85.863647  -90.370888  \n",
              "3     -102.114395 -112.583992 -118.619003  \n",
              "4      -87.564919  -80.625343  -69.467247  \n",
              "...           ...         ...         ...  \n",
              "52424  -94.020576  -94.580528 -105.903656  \n",
              "52425  -64.436829  -66.008080  -73.033150  \n",
              "52426  -72.504707  -55.494526  -81.097664  \n",
              "52427  -58.515965  -47.225040  -55.462997  \n",
              "52428 -136.238602 -119.627846 -131.176590  \n",
              "\n",
              "[52429 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c63b7d95-1e71-447f-88f7-61cadbd172aa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-73.759132</td>\n",
              "      <td>-70.356781</td>\n",
              "      <td>-71.348427</td>\n",
              "      <td>-57.827824</td>\n",
              "      <td>-52.884880</td>\n",
              "      <td>-60.673538</td>\n",
              "      <td>-64.963310</td>\n",
              "      <td>-53.201313</td>\n",
              "      <td>-80.622658</td>\n",
              "      <td>-84.596825</td>\n",
              "      <td>...</td>\n",
              "      <td>-63.204018</td>\n",
              "      <td>-57.142239</td>\n",
              "      <td>-67.710869</td>\n",
              "      <td>-71.614723</td>\n",
              "      <td>-76.373024</td>\n",
              "      <td>-66.549049</td>\n",
              "      <td>-75.274628</td>\n",
              "      <td>-65.226112</td>\n",
              "      <td>-70.779839</td>\n",
              "      <td>-57.394985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-87.240303</td>\n",
              "      <td>-88.269371</td>\n",
              "      <td>-90.448570</td>\n",
              "      <td>-100.477242</td>\n",
              "      <td>-85.945274</td>\n",
              "      <td>-81.327667</td>\n",
              "      <td>-89.613808</td>\n",
              "      <td>-88.326118</td>\n",
              "      <td>-93.258743</td>\n",
              "      <td>-91.021446</td>\n",
              "      <td>...</td>\n",
              "      <td>-98.545876</td>\n",
              "      <td>-98.447884</td>\n",
              "      <td>-90.269936</td>\n",
              "      <td>-84.337486</td>\n",
              "      <td>-93.393135</td>\n",
              "      <td>-92.402290</td>\n",
              "      <td>-92.555397</td>\n",
              "      <td>-88.168800</td>\n",
              "      <td>-95.743973</td>\n",
              "      <td>-103.297607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-78.161964</td>\n",
              "      <td>-82.139313</td>\n",
              "      <td>-80.860580</td>\n",
              "      <td>-85.763374</td>\n",
              "      <td>-78.548416</td>\n",
              "      <td>-87.767166</td>\n",
              "      <td>-85.649773</td>\n",
              "      <td>-84.181229</td>\n",
              "      <td>-84.540520</td>\n",
              "      <td>-90.089478</td>\n",
              "      <td>...</td>\n",
              "      <td>-88.611588</td>\n",
              "      <td>-86.913986</td>\n",
              "      <td>-83.287193</td>\n",
              "      <td>-89.154831</td>\n",
              "      <td>-89.125572</td>\n",
              "      <td>-88.291031</td>\n",
              "      <td>-84.831543</td>\n",
              "      <td>-83.170570</td>\n",
              "      <td>-85.863647</td>\n",
              "      <td>-90.370888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-112.722305</td>\n",
              "      <td>-103.117699</td>\n",
              "      <td>-110.969032</td>\n",
              "      <td>-116.696175</td>\n",
              "      <td>-108.435555</td>\n",
              "      <td>-94.227776</td>\n",
              "      <td>-105.639740</td>\n",
              "      <td>-109.465240</td>\n",
              "      <td>-115.384224</td>\n",
              "      <td>-106.986931</td>\n",
              "      <td>...</td>\n",
              "      <td>-112.759827</td>\n",
              "      <td>-116.884727</td>\n",
              "      <td>-110.928474</td>\n",
              "      <td>-96.106712</td>\n",
              "      <td>-107.266174</td>\n",
              "      <td>-111.795860</td>\n",
              "      <td>-115.523293</td>\n",
              "      <td>-102.114395</td>\n",
              "      <td>-112.583992</td>\n",
              "      <td>-118.619003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-77.687965</td>\n",
              "      <td>-86.679840</td>\n",
              "      <td>-77.035896</td>\n",
              "      <td>-63.908966</td>\n",
              "      <td>-76.518509</td>\n",
              "      <td>-73.519577</td>\n",
              "      <td>-70.862404</td>\n",
              "      <td>-62.142666</td>\n",
              "      <td>-74.402962</td>\n",
              "      <td>-78.489166</td>\n",
              "      <td>...</td>\n",
              "      <td>-77.946350</td>\n",
              "      <td>-76.839233</td>\n",
              "      <td>-77.542068</td>\n",
              "      <td>-72.688660</td>\n",
              "      <td>-71.544533</td>\n",
              "      <td>-63.584763</td>\n",
              "      <td>-84.621758</td>\n",
              "      <td>-87.564919</td>\n",
              "      <td>-80.625343</td>\n",
              "      <td>-69.467247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52424</th>\n",
              "      <td>-100.181030</td>\n",
              "      <td>-94.967751</td>\n",
              "      <td>-92.441559</td>\n",
              "      <td>-103.905540</td>\n",
              "      <td>-96.787209</td>\n",
              "      <td>-84.623634</td>\n",
              "      <td>-87.062279</td>\n",
              "      <td>-95.676407</td>\n",
              "      <td>-101.963753</td>\n",
              "      <td>-95.796043</td>\n",
              "      <td>...</td>\n",
              "      <td>-95.909012</td>\n",
              "      <td>-104.229362</td>\n",
              "      <td>-100.092018</td>\n",
              "      <td>-86.723083</td>\n",
              "      <td>-89.325684</td>\n",
              "      <td>-98.874657</td>\n",
              "      <td>-104.092003</td>\n",
              "      <td>-94.020576</td>\n",
              "      <td>-94.580528</td>\n",
              "      <td>-105.903656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52425</th>\n",
              "      <td>-62.005577</td>\n",
              "      <td>-69.308548</td>\n",
              "      <td>-67.532707</td>\n",
              "      <td>-73.834259</td>\n",
              "      <td>-49.747471</td>\n",
              "      <td>-72.944809</td>\n",
              "      <td>-64.530884</td>\n",
              "      <td>-57.348843</td>\n",
              "      <td>-67.135078</td>\n",
              "      <td>-77.420677</td>\n",
              "      <td>...</td>\n",
              "      <td>-77.007187</td>\n",
              "      <td>-73.838257</td>\n",
              "      <td>-64.204613</td>\n",
              "      <td>-80.847214</td>\n",
              "      <td>-76.469154</td>\n",
              "      <td>-71.935883</td>\n",
              "      <td>-62.660923</td>\n",
              "      <td>-64.436829</td>\n",
              "      <td>-66.008080</td>\n",
              "      <td>-73.033150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52426</th>\n",
              "      <td>-77.808235</td>\n",
              "      <td>-83.271957</td>\n",
              "      <td>-62.782448</td>\n",
              "      <td>-90.099083</td>\n",
              "      <td>-73.783562</td>\n",
              "      <td>-65.643471</td>\n",
              "      <td>-39.352760</td>\n",
              "      <td>-54.861713</td>\n",
              "      <td>-79.899506</td>\n",
              "      <td>-82.265198</td>\n",
              "      <td>...</td>\n",
              "      <td>-51.410633</td>\n",
              "      <td>-79.195869</td>\n",
              "      <td>-88.180817</td>\n",
              "      <td>-79.729927</td>\n",
              "      <td>-55.579609</td>\n",
              "      <td>-71.534668</td>\n",
              "      <td>-73.361931</td>\n",
              "      <td>-72.504707</td>\n",
              "      <td>-55.494526</td>\n",
              "      <td>-81.097664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52427</th>\n",
              "      <td>-76.945435</td>\n",
              "      <td>-73.163429</td>\n",
              "      <td>-57.108143</td>\n",
              "      <td>-67.197266</td>\n",
              "      <td>-53.742378</td>\n",
              "      <td>-58.976597</td>\n",
              "      <td>-40.658016</td>\n",
              "      <td>-55.933849</td>\n",
              "      <td>-75.859291</td>\n",
              "      <td>-79.223839</td>\n",
              "      <td>...</td>\n",
              "      <td>-45.504452</td>\n",
              "      <td>-58.273144</td>\n",
              "      <td>-72.473053</td>\n",
              "      <td>-73.120529</td>\n",
              "      <td>-57.341412</td>\n",
              "      <td>-72.774063</td>\n",
              "      <td>-69.129295</td>\n",
              "      <td>-58.515965</td>\n",
              "      <td>-47.225040</td>\n",
              "      <td>-55.462997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52428</th>\n",
              "      <td>-142.714371</td>\n",
              "      <td>-142.751129</td>\n",
              "      <td>-124.894943</td>\n",
              "      <td>-136.783157</td>\n",
              "      <td>-142.221771</td>\n",
              "      <td>-143.273865</td>\n",
              "      <td>-125.779831</td>\n",
              "      <td>-139.342453</td>\n",
              "      <td>-136.883118</td>\n",
              "      <td>-139.079269</td>\n",
              "      <td>...</td>\n",
              "      <td>-126.491928</td>\n",
              "      <td>-141.900467</td>\n",
              "      <td>-142.409790</td>\n",
              "      <td>-141.341675</td>\n",
              "      <td>-124.957031</td>\n",
              "      <td>-138.160873</td>\n",
              "      <td>-138.413208</td>\n",
              "      <td>-136.238602</td>\n",
              "      <td>-119.627846</td>\n",
              "      <td>-131.176590</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52429 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c63b7d95-1e71-447f-88f7-61cadbd172aa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c63b7d95-1e71-447f-88f7-61cadbd172aa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c63b7d95-1e71-447f-88f7-61cadbd172aa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3c5500cd-306a-49c1-9720-94173e5ceed2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c5500cd-306a-49c1-9720-94173e5ceed2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3c5500cd-306a-49c1-9720-94173e5ceed2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def add_best_beam_column(array):\n",
        "    \"\"\"\n",
        "    Computes the argmax (index of the maximum value) for each row of the array,\n",
        "    and adds it as a new column named 'best_beam' to the DataFrame.\n",
        "\n",
        "    Args:\n",
        "    array (numpy.ndarray): The array to which the 'best_beam' column will be added.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: DataFrame with the added 'best_beam' column.\n",
        "    \"\"\"\n",
        "    best_beam = np.argmax(array, axis=1)\n",
        "    df = pd.DataFrame(array)\n",
        "    df['best_beam'] = best_beam\n",
        "    return df\n",
        "\n",
        "### prediction ###\n",
        "yhat = model.predict(xTest)\n",
        "yhat_desclaed  = scaler_raw_org.inverse_transform(yhat)\n",
        "\n",
        "yhat_with_best_beam_ID = add_best_beam_column(yhat)\n",
        "yhat_desclaed_with_best_beam_ID = add_best_beam_column(yhat_desclaed)\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "### expected ###\n",
        "yTest = yTest.reshape(len(yTest), 32)\n",
        "yTest_desclaed = scaler_y_org.inverse_transform(yTest)\n",
        "\n",
        "yTest_with_best_beam_ID = add_best_beam_column(yTest)\n",
        "yTest_desclaed_with_best_beam_ID = add_best_beam_column(yTest_desclaed)\n",
        "################################################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "data_sets = [xTest.reshape(len(yTest), 32),\n",
        "             yTest_with_best_beam_ID,\n",
        "             yTest_desclaed_with_best_beam_ID,\n",
        "             yhat_with_best_beam_ID,\n",
        "             yhat_desclaed_with_best_beam_ID]\n",
        "file_names = ['xTest_model_input',\n",
        "              'yTest_expected_output',\n",
        "              'yTest_expected_descaled_output',\n",
        "              'yhat_prediction_output',\n",
        "              'yhat_prediction_descaled_output']\n",
        "num_entries = 10\n",
        "save_and_download_dataframes(data_sets, file_names, num_entries)\n",
        "\n",
        "custom_KPI.top_k_accuracies(yhat, yTest, top_k=[1,2,4,8])\n",
        "\n",
        "yhat_desclaed\n",
        "display(pd.DataFrame(yhat_desclaed))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5f4FujwQRLBR",
        "outputId": "eed2e610-a7e8-45b9-c22d-c898ed678878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yTest\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6   \\\n",
              "0      0.686174  0.659283  0.672990  0.757724  0.781947  0.733961  0.711599   \n",
              "1      0.571073  0.606441  0.585479  0.531645  0.553308  0.619044  0.571520   \n",
              "2      0.686819  0.603198  0.623519  0.608375  0.666045  0.584744  0.571788   \n",
              "3      0.506711  0.559569  0.477308  0.452107  0.478545  0.550653  0.450409   \n",
              "4      0.650697  0.581288  0.635936  0.718918  0.684242  0.673960  0.686614   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "52424  0.569612  0.531163  0.572783  0.522857  0.579275  0.609314  0.521238   \n",
              "52425  0.768320  0.693999  0.692145  0.669218  0.828176  0.670203  0.678047   \n",
              "52426  0.660911  0.617742  0.710077  0.598233  0.669362  0.704481  0.808421   \n",
              "52427  0.674250  0.701779  0.747086  0.707230  0.779679  0.738122  0.748297   \n",
              "52428  0.339756  0.357206  0.412218  0.345434  0.304354  0.287335  0.327711   \n",
              "\n",
              "             7         8         9   ...        22        23        24  \\\n",
              "0      0.834988  0.702218  0.621929  ...  0.702033  0.821130  0.717988   \n",
              "1      0.600097  0.597011  0.572288  ...  0.544823  0.579339  0.587987   \n",
              "2      0.604648  0.689137  0.570342  ...  0.571367  0.644250  0.682603   \n",
              "3      0.470583  0.491071  0.507257  ...  0.441677  0.477570  0.466048   \n",
              "4      0.766468  0.702049  0.624656  ...  0.607326  0.681503  0.680123   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "52424  0.536128  0.567228  0.556823  ...  0.520851  0.521320  0.562041   \n",
              "52425  0.750663  0.809707  0.674733  ...  0.627805  0.696802  0.781420   \n",
              "52426  0.771272  0.674006  0.613141  ...  0.758739  0.660842  0.613995   \n",
              "52427  0.832653  0.724575  0.667304  ...  0.716760  0.818107  0.717187   \n",
              "52428  0.273502  0.371033  0.337499  ...  0.341512  0.321741  0.314362   \n",
              "\n",
              "             25        26        27        28        29        30        31  \n",
              "0      0.671221  0.668371  0.769059  0.651031  0.689358  0.678273  0.757082  \n",
              "1      0.622852  0.585500  0.600365  0.584685  0.600568  0.570407  0.518198  \n",
              "2      0.572565  0.589103  0.604318  0.610367  0.573647  0.584006  0.580232  \n",
              "3      0.538756  0.455190  0.448945  0.484731  0.529778  0.488029  0.441705  \n",
              "4      0.671636  0.689275  0.760339  0.624677  0.551458  0.634883  0.694717  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "52424  0.590817  0.549696  0.513546  0.530858  0.526044  0.590431  0.504837  \n",
              "52425  0.644406  0.636930  0.697035  0.759061  0.689050  0.716906  0.668074  \n",
              "52426  0.650751  0.759604  0.712760  0.685564  0.642218  0.766214  0.628872  \n",
              "52427  0.684747  0.738149  0.759024  0.675842  0.699550  0.761359  0.758396  \n",
              "52428  0.292439  0.380668  0.297960  0.339517  0.369747  0.461184  0.376129  \n",
              "\n",
              "[52429 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02c9867d-0008-4c3d-af05-586eb3ae47bc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.686174</td>\n",
              "      <td>0.659283</td>\n",
              "      <td>0.672990</td>\n",
              "      <td>0.757724</td>\n",
              "      <td>0.781947</td>\n",
              "      <td>0.733961</td>\n",
              "      <td>0.711599</td>\n",
              "      <td>0.834988</td>\n",
              "      <td>0.702218</td>\n",
              "      <td>0.621929</td>\n",
              "      <td>...</td>\n",
              "      <td>0.702033</td>\n",
              "      <td>0.821130</td>\n",
              "      <td>0.717988</td>\n",
              "      <td>0.671221</td>\n",
              "      <td>0.668371</td>\n",
              "      <td>0.769059</td>\n",
              "      <td>0.651031</td>\n",
              "      <td>0.689358</td>\n",
              "      <td>0.678273</td>\n",
              "      <td>0.757082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.571073</td>\n",
              "      <td>0.606441</td>\n",
              "      <td>0.585479</td>\n",
              "      <td>0.531645</td>\n",
              "      <td>0.553308</td>\n",
              "      <td>0.619044</td>\n",
              "      <td>0.571520</td>\n",
              "      <td>0.600097</td>\n",
              "      <td>0.597011</td>\n",
              "      <td>0.572288</td>\n",
              "      <td>...</td>\n",
              "      <td>0.544823</td>\n",
              "      <td>0.579339</td>\n",
              "      <td>0.587987</td>\n",
              "      <td>0.622852</td>\n",
              "      <td>0.585500</td>\n",
              "      <td>0.600365</td>\n",
              "      <td>0.584685</td>\n",
              "      <td>0.600568</td>\n",
              "      <td>0.570407</td>\n",
              "      <td>0.518198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.686819</td>\n",
              "      <td>0.603198</td>\n",
              "      <td>0.623519</td>\n",
              "      <td>0.608375</td>\n",
              "      <td>0.666045</td>\n",
              "      <td>0.584744</td>\n",
              "      <td>0.571788</td>\n",
              "      <td>0.604648</td>\n",
              "      <td>0.689137</td>\n",
              "      <td>0.570342</td>\n",
              "      <td>...</td>\n",
              "      <td>0.571367</td>\n",
              "      <td>0.644250</td>\n",
              "      <td>0.682603</td>\n",
              "      <td>0.572565</td>\n",
              "      <td>0.589103</td>\n",
              "      <td>0.604318</td>\n",
              "      <td>0.610367</td>\n",
              "      <td>0.573647</td>\n",
              "      <td>0.584006</td>\n",
              "      <td>0.580232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.506711</td>\n",
              "      <td>0.559569</td>\n",
              "      <td>0.477308</td>\n",
              "      <td>0.452107</td>\n",
              "      <td>0.478545</td>\n",
              "      <td>0.550653</td>\n",
              "      <td>0.450409</td>\n",
              "      <td>0.470583</td>\n",
              "      <td>0.491071</td>\n",
              "      <td>0.507257</td>\n",
              "      <td>...</td>\n",
              "      <td>0.441677</td>\n",
              "      <td>0.477570</td>\n",
              "      <td>0.466048</td>\n",
              "      <td>0.538756</td>\n",
              "      <td>0.455190</td>\n",
              "      <td>0.448945</td>\n",
              "      <td>0.484731</td>\n",
              "      <td>0.529778</td>\n",
              "      <td>0.488029</td>\n",
              "      <td>0.441705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.650697</td>\n",
              "      <td>0.581288</td>\n",
              "      <td>0.635936</td>\n",
              "      <td>0.718918</td>\n",
              "      <td>0.684242</td>\n",
              "      <td>0.673960</td>\n",
              "      <td>0.686614</td>\n",
              "      <td>0.766468</td>\n",
              "      <td>0.702049</td>\n",
              "      <td>0.624656</td>\n",
              "      <td>...</td>\n",
              "      <td>0.607326</td>\n",
              "      <td>0.681503</td>\n",
              "      <td>0.680123</td>\n",
              "      <td>0.671636</td>\n",
              "      <td>0.689275</td>\n",
              "      <td>0.760339</td>\n",
              "      <td>0.624677</td>\n",
              "      <td>0.551458</td>\n",
              "      <td>0.634883</td>\n",
              "      <td>0.694717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52424</th>\n",
              "      <td>0.569612</td>\n",
              "      <td>0.531163</td>\n",
              "      <td>0.572783</td>\n",
              "      <td>0.522857</td>\n",
              "      <td>0.579275</td>\n",
              "      <td>0.609314</td>\n",
              "      <td>0.521238</td>\n",
              "      <td>0.536128</td>\n",
              "      <td>0.567228</td>\n",
              "      <td>0.556823</td>\n",
              "      <td>...</td>\n",
              "      <td>0.520851</td>\n",
              "      <td>0.521320</td>\n",
              "      <td>0.562041</td>\n",
              "      <td>0.590817</td>\n",
              "      <td>0.549696</td>\n",
              "      <td>0.513546</td>\n",
              "      <td>0.530858</td>\n",
              "      <td>0.526044</td>\n",
              "      <td>0.590431</td>\n",
              "      <td>0.504837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52425</th>\n",
              "      <td>0.768320</td>\n",
              "      <td>0.693999</td>\n",
              "      <td>0.692145</td>\n",
              "      <td>0.669218</td>\n",
              "      <td>0.828176</td>\n",
              "      <td>0.670203</td>\n",
              "      <td>0.678047</td>\n",
              "      <td>0.750663</td>\n",
              "      <td>0.809707</td>\n",
              "      <td>0.674733</td>\n",
              "      <td>...</td>\n",
              "      <td>0.627805</td>\n",
              "      <td>0.696802</td>\n",
              "      <td>0.781420</td>\n",
              "      <td>0.644406</td>\n",
              "      <td>0.636930</td>\n",
              "      <td>0.697035</td>\n",
              "      <td>0.759061</td>\n",
              "      <td>0.689050</td>\n",
              "      <td>0.716906</td>\n",
              "      <td>0.668074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52426</th>\n",
              "      <td>0.660911</td>\n",
              "      <td>0.617742</td>\n",
              "      <td>0.710077</td>\n",
              "      <td>0.598233</td>\n",
              "      <td>0.669362</td>\n",
              "      <td>0.704481</td>\n",
              "      <td>0.808421</td>\n",
              "      <td>0.771272</td>\n",
              "      <td>0.674006</td>\n",
              "      <td>0.613141</td>\n",
              "      <td>...</td>\n",
              "      <td>0.758739</td>\n",
              "      <td>0.660842</td>\n",
              "      <td>0.613995</td>\n",
              "      <td>0.650751</td>\n",
              "      <td>0.759604</td>\n",
              "      <td>0.712760</td>\n",
              "      <td>0.685564</td>\n",
              "      <td>0.642218</td>\n",
              "      <td>0.766214</td>\n",
              "      <td>0.628872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52427</th>\n",
              "      <td>0.674250</td>\n",
              "      <td>0.701779</td>\n",
              "      <td>0.747086</td>\n",
              "      <td>0.707230</td>\n",
              "      <td>0.779679</td>\n",
              "      <td>0.738122</td>\n",
              "      <td>0.748297</td>\n",
              "      <td>0.832653</td>\n",
              "      <td>0.724575</td>\n",
              "      <td>0.667304</td>\n",
              "      <td>...</td>\n",
              "      <td>0.716760</td>\n",
              "      <td>0.818107</td>\n",
              "      <td>0.717187</td>\n",
              "      <td>0.684747</td>\n",
              "      <td>0.738149</td>\n",
              "      <td>0.759024</td>\n",
              "      <td>0.675842</td>\n",
              "      <td>0.699550</td>\n",
              "      <td>0.761359</td>\n",
              "      <td>0.758396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52428</th>\n",
              "      <td>0.339756</td>\n",
              "      <td>0.357206</td>\n",
              "      <td>0.412218</td>\n",
              "      <td>0.345434</td>\n",
              "      <td>0.304354</td>\n",
              "      <td>0.287335</td>\n",
              "      <td>0.327711</td>\n",
              "      <td>0.273502</td>\n",
              "      <td>0.371033</td>\n",
              "      <td>0.337499</td>\n",
              "      <td>...</td>\n",
              "      <td>0.341512</td>\n",
              "      <td>0.321741</td>\n",
              "      <td>0.314362</td>\n",
              "      <td>0.292439</td>\n",
              "      <td>0.380668</td>\n",
              "      <td>0.297960</td>\n",
              "      <td>0.339517</td>\n",
              "      <td>0.369747</td>\n",
              "      <td>0.461184</td>\n",
              "      <td>0.376129</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52429 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02c9867d-0008-4c3d-af05-586eb3ae47bc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-02c9867d-0008-4c3d-af05-586eb3ae47bc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-02c9867d-0008-4c3d-af05-586eb3ae47bc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4d77caf4-d49f-4c09-93a0-62e3a3d7e434\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4d77caf4-d49f-4c09-93a0-62e3a3d7e434')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4d77caf4-d49f-4c09-93a0-62e3a3d7e434 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yTest_descaled\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "               0           1           2           3           4           5   \\\n",
              "0      -70.078119  -75.935155  -69.698873  -54.367540  -52.417797  -58.520976   \n",
              "1      -94.528134  -87.224453  -87.550821 -100.054168  -98.280999  -81.107658   \n",
              "2      -69.941148  -87.917272  -79.790765  -84.548404  -75.666735  -87.849301   \n",
              "3     -108.200065  -97.238039 -109.617204 -116.127450 -113.277686  -94.549859   \n",
              "4      -77.614163  -92.598169  -77.257799  -62.209566  -72.016704  -70.314167   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "52424  -94.838484 -103.306818  -90.140719 -101.830118  -93.072212  -83.020175   \n",
              "52425  -52.628598  -68.518402  -65.791425  -72.253242  -43.144683  -71.052591   \n",
              "52426  -75.444533  -84.810088  -62.133281  -86.597873  -75.001403  -64.315165   \n",
              "52427  -72.611100  -66.856433  -54.583709  -64.571581  -52.872744  -57.703096   \n",
              "52428 -143.665043 -140.470917 -122.895308 -137.684194 -148.219090 -146.304373   \n",
              "\n",
              "               6           7           8           9   ...          22  \\\n",
              "0      -58.541122  -39.489093  -71.820250  -79.736607  ...  -59.872680   \n",
              "1      -86.225724  -85.697698  -93.302664  -90.032255  ...  -91.415101   \n",
              "2      -86.172690  -84.802426  -74.491461  -90.435906  ...  -86.089380   \n",
              "3     -110.161527 -111.176341 -114.934822 -103.519804  ... -112.110210   \n",
              "4      -63.479070  -52.968567  -71.854833  -79.170938  ...  -78.874532   \n",
              "...           ...         ...         ...         ...  ...         ...   \n",
              "52424  -96.163157  -98.282060  -99.384114  -93.239713  ...  -96.224927   \n",
              "52425  -65.172268  -56.077779  -49.872111  -68.784902  ...  -74.765648   \n",
              "52426  -39.405734  -52.023641  -77.581069  -81.559306  ...  -48.495157   \n",
              "52427  -51.288291  -39.948491  -67.255217  -70.325537  ...  -56.917793   \n",
              "52428 -134.410912 -149.946863 -139.445370 -138.728225  ... -132.207337   \n",
              "\n",
              "               23          24          25          26          27          28  \\\n",
              "0      -49.056662  -66.712194  -69.897150  -68.800250  -53.441182  -78.170053   \n",
              "1      -95.699556  -92.250549  -79.368513  -85.496298  -85.578337  -92.545703   \n",
              "2      -83.177785  -73.663516  -89.215600  -84.770287  -84.825196  -86.980948   \n",
              "3     -115.331342 -116.205232  -95.836118 -111.749844 -114.424644 -114.203711   \n",
              "4      -75.991419  -74.150629  -69.815735  -64.588574  -55.102492  -83.880394   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "52424 -106.891672  -97.347730  -85.641521  -92.709687 -102.117710 -104.208950   \n",
              "52425  -73.040134  -54.251176  -75.147958  -75.134509  -67.162258  -54.762101   \n",
              "52426  -79.977009  -87.141424  -73.905544  -50.419425  -64.166387  -70.687349   \n",
              "52427  -49.639721  -66.869561  -67.248489  -54.742004  -55.352928  -72.793990   \n",
              "52428 -145.391675 -146.003615 -144.069307 -126.763856 -143.188023 -145.668644   \n",
              "\n",
              "               29          30          31  \n",
              "0      -64.400762  -72.399849  -54.767686  \n",
              "1      -83.355971  -94.104562 -103.392887  \n",
              "2      -89.103042  -91.368173  -90.765721  \n",
              "3      -98.468337 -110.680475 -118.963308  \n",
              "4      -93.840158  -81.130807  -67.462126  \n",
              "...           ...         ...         ...  \n",
              "52424  -99.265638  -90.075374 -106.112644  \n",
              "52425  -64.466520  -64.626148  -72.885516  \n",
              "52426  -74.464245  -54.704609  -80.865053  \n",
              "52427  -62.224932  -55.681433  -54.500185  \n",
              "52428 -132.632519 -116.082192 -132.311341  \n",
              "\n",
              "[52429 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-619c5d5e-f173-4de7-932c-e5a275c33bb2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-70.078119</td>\n",
              "      <td>-75.935155</td>\n",
              "      <td>-69.698873</td>\n",
              "      <td>-54.367540</td>\n",
              "      <td>-52.417797</td>\n",
              "      <td>-58.520976</td>\n",
              "      <td>-58.541122</td>\n",
              "      <td>-39.489093</td>\n",
              "      <td>-71.820250</td>\n",
              "      <td>-79.736607</td>\n",
              "      <td>...</td>\n",
              "      <td>-59.872680</td>\n",
              "      <td>-49.056662</td>\n",
              "      <td>-66.712194</td>\n",
              "      <td>-69.897150</td>\n",
              "      <td>-68.800250</td>\n",
              "      <td>-53.441182</td>\n",
              "      <td>-78.170053</td>\n",
              "      <td>-64.400762</td>\n",
              "      <td>-72.399849</td>\n",
              "      <td>-54.767686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-94.528134</td>\n",
              "      <td>-87.224453</td>\n",
              "      <td>-87.550821</td>\n",
              "      <td>-100.054168</td>\n",
              "      <td>-98.280999</td>\n",
              "      <td>-81.107658</td>\n",
              "      <td>-86.225724</td>\n",
              "      <td>-85.697698</td>\n",
              "      <td>-93.302664</td>\n",
              "      <td>-90.032255</td>\n",
              "      <td>...</td>\n",
              "      <td>-91.415101</td>\n",
              "      <td>-95.699556</td>\n",
              "      <td>-92.250549</td>\n",
              "      <td>-79.368513</td>\n",
              "      <td>-85.496298</td>\n",
              "      <td>-85.578337</td>\n",
              "      <td>-92.545703</td>\n",
              "      <td>-83.355971</td>\n",
              "      <td>-94.104562</td>\n",
              "      <td>-103.392887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-69.941148</td>\n",
              "      <td>-87.917272</td>\n",
              "      <td>-79.790765</td>\n",
              "      <td>-84.548404</td>\n",
              "      <td>-75.666735</td>\n",
              "      <td>-87.849301</td>\n",
              "      <td>-86.172690</td>\n",
              "      <td>-84.802426</td>\n",
              "      <td>-74.491461</td>\n",
              "      <td>-90.435906</td>\n",
              "      <td>...</td>\n",
              "      <td>-86.089380</td>\n",
              "      <td>-83.177785</td>\n",
              "      <td>-73.663516</td>\n",
              "      <td>-89.215600</td>\n",
              "      <td>-84.770287</td>\n",
              "      <td>-84.825196</td>\n",
              "      <td>-86.980948</td>\n",
              "      <td>-89.103042</td>\n",
              "      <td>-91.368173</td>\n",
              "      <td>-90.765721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-108.200065</td>\n",
              "      <td>-97.238039</td>\n",
              "      <td>-109.617204</td>\n",
              "      <td>-116.127450</td>\n",
              "      <td>-113.277686</td>\n",
              "      <td>-94.549859</td>\n",
              "      <td>-110.161527</td>\n",
              "      <td>-111.176341</td>\n",
              "      <td>-114.934822</td>\n",
              "      <td>-103.519804</td>\n",
              "      <td>...</td>\n",
              "      <td>-112.110210</td>\n",
              "      <td>-115.331342</td>\n",
              "      <td>-116.205232</td>\n",
              "      <td>-95.836118</td>\n",
              "      <td>-111.749844</td>\n",
              "      <td>-114.424644</td>\n",
              "      <td>-114.203711</td>\n",
              "      <td>-98.468337</td>\n",
              "      <td>-110.680475</td>\n",
              "      <td>-118.963308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-77.614163</td>\n",
              "      <td>-92.598169</td>\n",
              "      <td>-77.257799</td>\n",
              "      <td>-62.209566</td>\n",
              "      <td>-72.016704</td>\n",
              "      <td>-70.314167</td>\n",
              "      <td>-63.479070</td>\n",
              "      <td>-52.968567</td>\n",
              "      <td>-71.854833</td>\n",
              "      <td>-79.170938</td>\n",
              "      <td>...</td>\n",
              "      <td>-78.874532</td>\n",
              "      <td>-75.991419</td>\n",
              "      <td>-74.150629</td>\n",
              "      <td>-69.815735</td>\n",
              "      <td>-64.588574</td>\n",
              "      <td>-55.102492</td>\n",
              "      <td>-83.880394</td>\n",
              "      <td>-93.840158</td>\n",
              "      <td>-81.130807</td>\n",
              "      <td>-67.462126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52424</th>\n",
              "      <td>-94.838484</td>\n",
              "      <td>-103.306818</td>\n",
              "      <td>-90.140719</td>\n",
              "      <td>-101.830118</td>\n",
              "      <td>-93.072212</td>\n",
              "      <td>-83.020175</td>\n",
              "      <td>-96.163157</td>\n",
              "      <td>-98.282060</td>\n",
              "      <td>-99.384114</td>\n",
              "      <td>-93.239713</td>\n",
              "      <td>...</td>\n",
              "      <td>-96.224927</td>\n",
              "      <td>-106.891672</td>\n",
              "      <td>-97.347730</td>\n",
              "      <td>-85.641521</td>\n",
              "      <td>-92.709687</td>\n",
              "      <td>-102.117710</td>\n",
              "      <td>-104.208950</td>\n",
              "      <td>-99.265638</td>\n",
              "      <td>-90.075374</td>\n",
              "      <td>-106.112644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52425</th>\n",
              "      <td>-52.628598</td>\n",
              "      <td>-68.518402</td>\n",
              "      <td>-65.791425</td>\n",
              "      <td>-72.253242</td>\n",
              "      <td>-43.144683</td>\n",
              "      <td>-71.052591</td>\n",
              "      <td>-65.172268</td>\n",
              "      <td>-56.077779</td>\n",
              "      <td>-49.872111</td>\n",
              "      <td>-68.784902</td>\n",
              "      <td>...</td>\n",
              "      <td>-74.765648</td>\n",
              "      <td>-73.040134</td>\n",
              "      <td>-54.251176</td>\n",
              "      <td>-75.147958</td>\n",
              "      <td>-75.134509</td>\n",
              "      <td>-67.162258</td>\n",
              "      <td>-54.762101</td>\n",
              "      <td>-64.466520</td>\n",
              "      <td>-64.626148</td>\n",
              "      <td>-72.885516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52426</th>\n",
              "      <td>-75.444533</td>\n",
              "      <td>-84.810088</td>\n",
              "      <td>-62.133281</td>\n",
              "      <td>-86.597873</td>\n",
              "      <td>-75.001403</td>\n",
              "      <td>-64.315165</td>\n",
              "      <td>-39.405734</td>\n",
              "      <td>-52.023641</td>\n",
              "      <td>-77.581069</td>\n",
              "      <td>-81.559306</td>\n",
              "      <td>...</td>\n",
              "      <td>-48.495157</td>\n",
              "      <td>-79.977009</td>\n",
              "      <td>-87.141424</td>\n",
              "      <td>-73.905544</td>\n",
              "      <td>-50.419425</td>\n",
              "      <td>-64.166387</td>\n",
              "      <td>-70.687349</td>\n",
              "      <td>-74.464245</td>\n",
              "      <td>-54.704609</td>\n",
              "      <td>-80.865053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52427</th>\n",
              "      <td>-72.611100</td>\n",
              "      <td>-66.856433</td>\n",
              "      <td>-54.583709</td>\n",
              "      <td>-64.571581</td>\n",
              "      <td>-52.872744</td>\n",
              "      <td>-57.703096</td>\n",
              "      <td>-51.288291</td>\n",
              "      <td>-39.948491</td>\n",
              "      <td>-67.255217</td>\n",
              "      <td>-70.325537</td>\n",
              "      <td>...</td>\n",
              "      <td>-56.917793</td>\n",
              "      <td>-49.639721</td>\n",
              "      <td>-66.869561</td>\n",
              "      <td>-67.248489</td>\n",
              "      <td>-54.742004</td>\n",
              "      <td>-55.352928</td>\n",
              "      <td>-72.793990</td>\n",
              "      <td>-62.224932</td>\n",
              "      <td>-55.681433</td>\n",
              "      <td>-54.500185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52428</th>\n",
              "      <td>-143.665043</td>\n",
              "      <td>-140.470917</td>\n",
              "      <td>-122.895308</td>\n",
              "      <td>-137.684194</td>\n",
              "      <td>-148.219090</td>\n",
              "      <td>-146.304373</td>\n",
              "      <td>-134.410912</td>\n",
              "      <td>-149.946863</td>\n",
              "      <td>-139.445370</td>\n",
              "      <td>-138.728225</td>\n",
              "      <td>...</td>\n",
              "      <td>-132.207337</td>\n",
              "      <td>-145.391675</td>\n",
              "      <td>-146.003615</td>\n",
              "      <td>-144.069307</td>\n",
              "      <td>-126.763856</td>\n",
              "      <td>-143.188023</td>\n",
              "      <td>-145.668644</td>\n",
              "      <td>-132.632519</td>\n",
              "      <td>-116.082192</td>\n",
              "      <td>-132.311341</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52429 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-619c5d5e-f173-4de7-932c-e5a275c33bb2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-619c5d5e-f173-4de7-932c-e5a275c33bb2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-619c5d5e-f173-4de7-932c-e5a275c33bb2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5d323ea2-bc1f-4619-8501-17a26f7c3223\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5d323ea2-bc1f-4619-8501-17a26f7c3223')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5d323ea2-bc1f-4619-8501-17a26f7c3223 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1639/1639 [==============================] - 2s 1ms/step\n",
            "yhat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6   \\\n",
              "0      0.668845  0.685394  0.664904  0.740601  0.779619  0.723009  0.679104   \n",
              "1      0.605381  0.601550  0.571274  0.529552  0.614804  0.617925  0.554377   \n",
              "2      0.648119  0.630243  0.618275  0.602363  0.651679  0.585162  0.574434   \n",
              "3      0.485422  0.532048  0.470682  0.449293  0.502685  0.552291  0.473288   \n",
              "4      0.650350  0.608990  0.637024  0.710509  0.661799  0.657651  0.649256   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "52424  0.544462  0.570196  0.561504  0.512587  0.560754  0.601155  0.567287   \n",
              "52425  0.724176  0.690301  0.683609  0.661394  0.795259  0.660575  0.681292   \n",
              "52426  0.649784  0.624941  0.706895  0.580908  0.675433  0.697723  0.808689   \n",
              "52427  0.653846  0.672257  0.734711  0.694237  0.775344  0.731643  0.802085   \n",
              "52428  0.344232  0.346533  0.402416  0.349893  0.334252  0.302753  0.371383   \n",
              "\n",
              "             7         8         9   ...        22        23        24  \\\n",
              "0      0.765285  0.659110  0.598495  ...  0.685429  0.779215  0.712905   \n",
              "1      0.586736  0.597226  0.567518  ...  0.509283  0.565092  0.598069   \n",
              "2      0.607806  0.639923  0.572012  ...  0.558796  0.624882  0.633615   \n",
              "3      0.479281  0.488870  0.490540  ...  0.438440  0.469517  0.492909   \n",
              "4      0.719834  0.689570  0.627943  ...  0.611952  0.677108  0.662860   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "52424  0.549373  0.554595  0.544498  ...  0.522425  0.535121  0.548071   \n",
              "52425  0.744202  0.725163  0.633095  ...  0.616633  0.692665  0.730753   \n",
              "52426  0.756845  0.662651  0.609737  ...  0.744208  0.664892  0.608704   \n",
              "52427  0.751395  0.682438  0.624401  ...  0.773645  0.773353  0.688663   \n",
              "52428  0.327407  0.383582  0.335806  ...  0.369998  0.339839  0.332656   \n",
              "\n",
              "             25        26        27        28        29        30        31  \n",
              "0      0.662449  0.630783  0.700253  0.664393  0.685492  0.686324  0.744175  \n",
              "1      0.597477  0.546304  0.564545  0.584641  0.578023  0.562259  0.518666  \n",
              "2      0.572876  0.567486  0.586125  0.620287  0.601436  0.611362  0.582172  \n",
              "3      0.537374  0.477445  0.462744  0.478641  0.512699  0.478569  0.443396  \n",
              "4      0.656965  0.654749  0.715814  0.621255  0.580852  0.637395  0.684867  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "52424  0.585294  0.566492  0.530570  0.531398  0.550613  0.568041  0.505863  \n",
              "52425  0.615301  0.630306  0.671977  0.722607  0.689189  0.710039  0.667348  \n",
              "52426  0.621007  0.733991  0.674083  0.673221  0.651397  0.762288  0.627729  \n",
              "52427  0.654760  0.725247  0.667577  0.692755  0.716923  0.803385  0.753666  \n",
              "52428  0.306369  0.389636  0.324349  0.373002  0.352855  0.443563  0.381704  \n",
              "\n",
              "[52429 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3da12291-1122-40c5-9431-7fa815155b1a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.668845</td>\n",
              "      <td>0.685394</td>\n",
              "      <td>0.664904</td>\n",
              "      <td>0.740601</td>\n",
              "      <td>0.779619</td>\n",
              "      <td>0.723009</td>\n",
              "      <td>0.679104</td>\n",
              "      <td>0.765285</td>\n",
              "      <td>0.659110</td>\n",
              "      <td>0.598495</td>\n",
              "      <td>...</td>\n",
              "      <td>0.685429</td>\n",
              "      <td>0.779215</td>\n",
              "      <td>0.712905</td>\n",
              "      <td>0.662449</td>\n",
              "      <td>0.630783</td>\n",
              "      <td>0.700253</td>\n",
              "      <td>0.664393</td>\n",
              "      <td>0.685492</td>\n",
              "      <td>0.686324</td>\n",
              "      <td>0.744175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.605381</td>\n",
              "      <td>0.601550</td>\n",
              "      <td>0.571274</td>\n",
              "      <td>0.529552</td>\n",
              "      <td>0.614804</td>\n",
              "      <td>0.617925</td>\n",
              "      <td>0.554377</td>\n",
              "      <td>0.586736</td>\n",
              "      <td>0.597226</td>\n",
              "      <td>0.567518</td>\n",
              "      <td>...</td>\n",
              "      <td>0.509283</td>\n",
              "      <td>0.565092</td>\n",
              "      <td>0.598069</td>\n",
              "      <td>0.597477</td>\n",
              "      <td>0.546304</td>\n",
              "      <td>0.564545</td>\n",
              "      <td>0.584641</td>\n",
              "      <td>0.578023</td>\n",
              "      <td>0.562259</td>\n",
              "      <td>0.518666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.648119</td>\n",
              "      <td>0.630243</td>\n",
              "      <td>0.618275</td>\n",
              "      <td>0.602363</td>\n",
              "      <td>0.651679</td>\n",
              "      <td>0.585162</td>\n",
              "      <td>0.574434</td>\n",
              "      <td>0.607806</td>\n",
              "      <td>0.639923</td>\n",
              "      <td>0.572012</td>\n",
              "      <td>...</td>\n",
              "      <td>0.558796</td>\n",
              "      <td>0.624882</td>\n",
              "      <td>0.633615</td>\n",
              "      <td>0.572876</td>\n",
              "      <td>0.567486</td>\n",
              "      <td>0.586125</td>\n",
              "      <td>0.620287</td>\n",
              "      <td>0.601436</td>\n",
              "      <td>0.611362</td>\n",
              "      <td>0.582172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.485422</td>\n",
              "      <td>0.532048</td>\n",
              "      <td>0.470682</td>\n",
              "      <td>0.449293</td>\n",
              "      <td>0.502685</td>\n",
              "      <td>0.552291</td>\n",
              "      <td>0.473288</td>\n",
              "      <td>0.479281</td>\n",
              "      <td>0.488870</td>\n",
              "      <td>0.490540</td>\n",
              "      <td>...</td>\n",
              "      <td>0.438440</td>\n",
              "      <td>0.469517</td>\n",
              "      <td>0.492909</td>\n",
              "      <td>0.537374</td>\n",
              "      <td>0.477445</td>\n",
              "      <td>0.462744</td>\n",
              "      <td>0.478641</td>\n",
              "      <td>0.512699</td>\n",
              "      <td>0.478569</td>\n",
              "      <td>0.443396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.650350</td>\n",
              "      <td>0.608990</td>\n",
              "      <td>0.637024</td>\n",
              "      <td>0.710509</td>\n",
              "      <td>0.661799</td>\n",
              "      <td>0.657651</td>\n",
              "      <td>0.649256</td>\n",
              "      <td>0.719834</td>\n",
              "      <td>0.689570</td>\n",
              "      <td>0.627943</td>\n",
              "      <td>...</td>\n",
              "      <td>0.611952</td>\n",
              "      <td>0.677108</td>\n",
              "      <td>0.662860</td>\n",
              "      <td>0.656965</td>\n",
              "      <td>0.654749</td>\n",
              "      <td>0.715814</td>\n",
              "      <td>0.621255</td>\n",
              "      <td>0.580852</td>\n",
              "      <td>0.637395</td>\n",
              "      <td>0.684867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52424</th>\n",
              "      <td>0.544462</td>\n",
              "      <td>0.570196</td>\n",
              "      <td>0.561504</td>\n",
              "      <td>0.512587</td>\n",
              "      <td>0.560754</td>\n",
              "      <td>0.601155</td>\n",
              "      <td>0.567287</td>\n",
              "      <td>0.549373</td>\n",
              "      <td>0.554595</td>\n",
              "      <td>0.544498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.522425</td>\n",
              "      <td>0.535121</td>\n",
              "      <td>0.548071</td>\n",
              "      <td>0.585294</td>\n",
              "      <td>0.566492</td>\n",
              "      <td>0.530570</td>\n",
              "      <td>0.531398</td>\n",
              "      <td>0.550613</td>\n",
              "      <td>0.568041</td>\n",
              "      <td>0.505863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52425</th>\n",
              "      <td>0.724176</td>\n",
              "      <td>0.690301</td>\n",
              "      <td>0.683609</td>\n",
              "      <td>0.661394</td>\n",
              "      <td>0.795259</td>\n",
              "      <td>0.660575</td>\n",
              "      <td>0.681292</td>\n",
              "      <td>0.744202</td>\n",
              "      <td>0.725163</td>\n",
              "      <td>0.633095</td>\n",
              "      <td>...</td>\n",
              "      <td>0.616633</td>\n",
              "      <td>0.692665</td>\n",
              "      <td>0.730753</td>\n",
              "      <td>0.615301</td>\n",
              "      <td>0.630306</td>\n",
              "      <td>0.671977</td>\n",
              "      <td>0.722607</td>\n",
              "      <td>0.689189</td>\n",
              "      <td>0.710039</td>\n",
              "      <td>0.667348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52426</th>\n",
              "      <td>0.649784</td>\n",
              "      <td>0.624941</td>\n",
              "      <td>0.706895</td>\n",
              "      <td>0.580908</td>\n",
              "      <td>0.675433</td>\n",
              "      <td>0.697723</td>\n",
              "      <td>0.808689</td>\n",
              "      <td>0.756845</td>\n",
              "      <td>0.662651</td>\n",
              "      <td>0.609737</td>\n",
              "      <td>...</td>\n",
              "      <td>0.744208</td>\n",
              "      <td>0.664892</td>\n",
              "      <td>0.608704</td>\n",
              "      <td>0.621007</td>\n",
              "      <td>0.733991</td>\n",
              "      <td>0.674083</td>\n",
              "      <td>0.673221</td>\n",
              "      <td>0.651397</td>\n",
              "      <td>0.762288</td>\n",
              "      <td>0.627729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52427</th>\n",
              "      <td>0.653846</td>\n",
              "      <td>0.672257</td>\n",
              "      <td>0.734711</td>\n",
              "      <td>0.694237</td>\n",
              "      <td>0.775344</td>\n",
              "      <td>0.731643</td>\n",
              "      <td>0.802085</td>\n",
              "      <td>0.751395</td>\n",
              "      <td>0.682438</td>\n",
              "      <td>0.624401</td>\n",
              "      <td>...</td>\n",
              "      <td>0.773645</td>\n",
              "      <td>0.773353</td>\n",
              "      <td>0.688663</td>\n",
              "      <td>0.654760</td>\n",
              "      <td>0.725247</td>\n",
              "      <td>0.667577</td>\n",
              "      <td>0.692755</td>\n",
              "      <td>0.716923</td>\n",
              "      <td>0.803385</td>\n",
              "      <td>0.753666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52428</th>\n",
              "      <td>0.344232</td>\n",
              "      <td>0.346533</td>\n",
              "      <td>0.402416</td>\n",
              "      <td>0.349893</td>\n",
              "      <td>0.334252</td>\n",
              "      <td>0.302753</td>\n",
              "      <td>0.371383</td>\n",
              "      <td>0.327407</td>\n",
              "      <td>0.383582</td>\n",
              "      <td>0.335806</td>\n",
              "      <td>...</td>\n",
              "      <td>0.369998</td>\n",
              "      <td>0.339839</td>\n",
              "      <td>0.332656</td>\n",
              "      <td>0.306369</td>\n",
              "      <td>0.389636</td>\n",
              "      <td>0.324349</td>\n",
              "      <td>0.373002</td>\n",
              "      <td>0.352855</td>\n",
              "      <td>0.443563</td>\n",
              "      <td>0.381704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52429 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3da12291-1122-40c5-9431-7fa815155b1a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3da12291-1122-40c5-9431-7fa815155b1a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3da12291-1122-40c5-9431-7fa815155b1a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fc2fe6c4-3f79-4bfc-9e17-a44ad02b8d69\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc2fe6c4-3f79-4bfc-9e17-a44ad02b8d69')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fc2fe6c4-3f79-4bfc-9e17-a44ad02b8d69 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yTest_rescaled\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "               0           1           2           3           4           5   \\\n",
              "0      -70.078119  -75.935155  -69.698873  -54.367540  -52.417797  -58.520976   \n",
              "1      -94.528134  -87.224453  -87.550821 -100.054168  -98.280999  -81.107658   \n",
              "2      -69.941148  -87.917272  -79.790765  -84.548404  -75.666735  -87.849301   \n",
              "3     -108.200065  -97.238039 -109.617204 -116.127450 -113.277686  -94.549859   \n",
              "4      -77.614163  -92.598169  -77.257799  -62.209566  -72.016704  -70.314167   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "52424  -94.838484 -103.306818  -90.140719 -101.830118  -93.072212  -83.020175   \n",
              "52425  -52.628598  -68.518402  -65.791425  -72.253242  -43.144683  -71.052591   \n",
              "52426  -75.444533  -84.810088  -62.133281  -86.597873  -75.001403  -64.315165   \n",
              "52427  -72.611100  -66.856433  -54.583709  -64.571581  -52.872744  -57.703096   \n",
              "52428 -143.665043 -140.470917 -122.895308 -137.684194 -148.219090 -146.304373   \n",
              "\n",
              "               6           7           8           9   ...          22  \\\n",
              "0      -58.541122  -39.489093  -71.820250  -79.736607  ...  -59.872680   \n",
              "1      -86.225724  -85.697698  -93.302664  -90.032255  ...  -91.415101   \n",
              "2      -86.172690  -84.802426  -74.491461  -90.435906  ...  -86.089380   \n",
              "3     -110.161527 -111.176341 -114.934822 -103.519804  ... -112.110210   \n",
              "4      -63.479070  -52.968567  -71.854833  -79.170938  ...  -78.874532   \n",
              "...           ...         ...         ...         ...  ...         ...   \n",
              "52424  -96.163157  -98.282060  -99.384114  -93.239713  ...  -96.224927   \n",
              "52425  -65.172268  -56.077779  -49.872111  -68.784902  ...  -74.765648   \n",
              "52426  -39.405734  -52.023641  -77.581069  -81.559306  ...  -48.495157   \n",
              "52427  -51.288291  -39.948491  -67.255217  -70.325537  ...  -56.917793   \n",
              "52428 -134.410912 -149.946863 -139.445370 -138.728225  ... -132.207337   \n",
              "\n",
              "               23          24          25          26          27          28  \\\n",
              "0      -49.056662  -66.712194  -69.897150  -68.800250  -53.441182  -78.170053   \n",
              "1      -95.699556  -92.250549  -79.368513  -85.496298  -85.578337  -92.545703   \n",
              "2      -83.177785  -73.663516  -89.215600  -84.770287  -84.825196  -86.980948   \n",
              "3     -115.331342 -116.205232  -95.836118 -111.749844 -114.424644 -114.203711   \n",
              "4      -75.991419  -74.150629  -69.815735  -64.588574  -55.102492  -83.880394   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "52424 -106.891672  -97.347730  -85.641521  -92.709687 -102.117710 -104.208950   \n",
              "52425  -73.040134  -54.251176  -75.147958  -75.134509  -67.162258  -54.762101   \n",
              "52426  -79.977009  -87.141424  -73.905544  -50.419425  -64.166387  -70.687349   \n",
              "52427  -49.639721  -66.869561  -67.248489  -54.742004  -55.352928  -72.793990   \n",
              "52428 -145.391675 -146.003615 -144.069307 -126.763856 -143.188023 -145.668644   \n",
              "\n",
              "               29          30          31  \n",
              "0      -64.400762  -72.399849  -54.767686  \n",
              "1      -83.355971  -94.104562 -103.392887  \n",
              "2      -89.103042  -91.368173  -90.765721  \n",
              "3      -98.468337 -110.680475 -118.963308  \n",
              "4      -93.840158  -81.130807  -67.462126  \n",
              "...           ...         ...         ...  \n",
              "52424  -99.265638  -90.075374 -106.112644  \n",
              "52425  -64.466520  -64.626148  -72.885516  \n",
              "52426  -74.464245  -54.704609  -80.865053  \n",
              "52427  -62.224932  -55.681433  -54.500185  \n",
              "52428 -132.632519 -116.082192 -132.311341  \n",
              "\n",
              "[52429 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25b46315-7117-4f75-a8ac-c463e218000e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-70.078119</td>\n",
              "      <td>-75.935155</td>\n",
              "      <td>-69.698873</td>\n",
              "      <td>-54.367540</td>\n",
              "      <td>-52.417797</td>\n",
              "      <td>-58.520976</td>\n",
              "      <td>-58.541122</td>\n",
              "      <td>-39.489093</td>\n",
              "      <td>-71.820250</td>\n",
              "      <td>-79.736607</td>\n",
              "      <td>...</td>\n",
              "      <td>-59.872680</td>\n",
              "      <td>-49.056662</td>\n",
              "      <td>-66.712194</td>\n",
              "      <td>-69.897150</td>\n",
              "      <td>-68.800250</td>\n",
              "      <td>-53.441182</td>\n",
              "      <td>-78.170053</td>\n",
              "      <td>-64.400762</td>\n",
              "      <td>-72.399849</td>\n",
              "      <td>-54.767686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-94.528134</td>\n",
              "      <td>-87.224453</td>\n",
              "      <td>-87.550821</td>\n",
              "      <td>-100.054168</td>\n",
              "      <td>-98.280999</td>\n",
              "      <td>-81.107658</td>\n",
              "      <td>-86.225724</td>\n",
              "      <td>-85.697698</td>\n",
              "      <td>-93.302664</td>\n",
              "      <td>-90.032255</td>\n",
              "      <td>...</td>\n",
              "      <td>-91.415101</td>\n",
              "      <td>-95.699556</td>\n",
              "      <td>-92.250549</td>\n",
              "      <td>-79.368513</td>\n",
              "      <td>-85.496298</td>\n",
              "      <td>-85.578337</td>\n",
              "      <td>-92.545703</td>\n",
              "      <td>-83.355971</td>\n",
              "      <td>-94.104562</td>\n",
              "      <td>-103.392887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-69.941148</td>\n",
              "      <td>-87.917272</td>\n",
              "      <td>-79.790765</td>\n",
              "      <td>-84.548404</td>\n",
              "      <td>-75.666735</td>\n",
              "      <td>-87.849301</td>\n",
              "      <td>-86.172690</td>\n",
              "      <td>-84.802426</td>\n",
              "      <td>-74.491461</td>\n",
              "      <td>-90.435906</td>\n",
              "      <td>...</td>\n",
              "      <td>-86.089380</td>\n",
              "      <td>-83.177785</td>\n",
              "      <td>-73.663516</td>\n",
              "      <td>-89.215600</td>\n",
              "      <td>-84.770287</td>\n",
              "      <td>-84.825196</td>\n",
              "      <td>-86.980948</td>\n",
              "      <td>-89.103042</td>\n",
              "      <td>-91.368173</td>\n",
              "      <td>-90.765721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-108.200065</td>\n",
              "      <td>-97.238039</td>\n",
              "      <td>-109.617204</td>\n",
              "      <td>-116.127450</td>\n",
              "      <td>-113.277686</td>\n",
              "      <td>-94.549859</td>\n",
              "      <td>-110.161527</td>\n",
              "      <td>-111.176341</td>\n",
              "      <td>-114.934822</td>\n",
              "      <td>-103.519804</td>\n",
              "      <td>...</td>\n",
              "      <td>-112.110210</td>\n",
              "      <td>-115.331342</td>\n",
              "      <td>-116.205232</td>\n",
              "      <td>-95.836118</td>\n",
              "      <td>-111.749844</td>\n",
              "      <td>-114.424644</td>\n",
              "      <td>-114.203711</td>\n",
              "      <td>-98.468337</td>\n",
              "      <td>-110.680475</td>\n",
              "      <td>-118.963308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-77.614163</td>\n",
              "      <td>-92.598169</td>\n",
              "      <td>-77.257799</td>\n",
              "      <td>-62.209566</td>\n",
              "      <td>-72.016704</td>\n",
              "      <td>-70.314167</td>\n",
              "      <td>-63.479070</td>\n",
              "      <td>-52.968567</td>\n",
              "      <td>-71.854833</td>\n",
              "      <td>-79.170938</td>\n",
              "      <td>...</td>\n",
              "      <td>-78.874532</td>\n",
              "      <td>-75.991419</td>\n",
              "      <td>-74.150629</td>\n",
              "      <td>-69.815735</td>\n",
              "      <td>-64.588574</td>\n",
              "      <td>-55.102492</td>\n",
              "      <td>-83.880394</td>\n",
              "      <td>-93.840158</td>\n",
              "      <td>-81.130807</td>\n",
              "      <td>-67.462126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52424</th>\n",
              "      <td>-94.838484</td>\n",
              "      <td>-103.306818</td>\n",
              "      <td>-90.140719</td>\n",
              "      <td>-101.830118</td>\n",
              "      <td>-93.072212</td>\n",
              "      <td>-83.020175</td>\n",
              "      <td>-96.163157</td>\n",
              "      <td>-98.282060</td>\n",
              "      <td>-99.384114</td>\n",
              "      <td>-93.239713</td>\n",
              "      <td>...</td>\n",
              "      <td>-96.224927</td>\n",
              "      <td>-106.891672</td>\n",
              "      <td>-97.347730</td>\n",
              "      <td>-85.641521</td>\n",
              "      <td>-92.709687</td>\n",
              "      <td>-102.117710</td>\n",
              "      <td>-104.208950</td>\n",
              "      <td>-99.265638</td>\n",
              "      <td>-90.075374</td>\n",
              "      <td>-106.112644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52425</th>\n",
              "      <td>-52.628598</td>\n",
              "      <td>-68.518402</td>\n",
              "      <td>-65.791425</td>\n",
              "      <td>-72.253242</td>\n",
              "      <td>-43.144683</td>\n",
              "      <td>-71.052591</td>\n",
              "      <td>-65.172268</td>\n",
              "      <td>-56.077779</td>\n",
              "      <td>-49.872111</td>\n",
              "      <td>-68.784902</td>\n",
              "      <td>...</td>\n",
              "      <td>-74.765648</td>\n",
              "      <td>-73.040134</td>\n",
              "      <td>-54.251176</td>\n",
              "      <td>-75.147958</td>\n",
              "      <td>-75.134509</td>\n",
              "      <td>-67.162258</td>\n",
              "      <td>-54.762101</td>\n",
              "      <td>-64.466520</td>\n",
              "      <td>-64.626148</td>\n",
              "      <td>-72.885516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52426</th>\n",
              "      <td>-75.444533</td>\n",
              "      <td>-84.810088</td>\n",
              "      <td>-62.133281</td>\n",
              "      <td>-86.597873</td>\n",
              "      <td>-75.001403</td>\n",
              "      <td>-64.315165</td>\n",
              "      <td>-39.405734</td>\n",
              "      <td>-52.023641</td>\n",
              "      <td>-77.581069</td>\n",
              "      <td>-81.559306</td>\n",
              "      <td>...</td>\n",
              "      <td>-48.495157</td>\n",
              "      <td>-79.977009</td>\n",
              "      <td>-87.141424</td>\n",
              "      <td>-73.905544</td>\n",
              "      <td>-50.419425</td>\n",
              "      <td>-64.166387</td>\n",
              "      <td>-70.687349</td>\n",
              "      <td>-74.464245</td>\n",
              "      <td>-54.704609</td>\n",
              "      <td>-80.865053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52427</th>\n",
              "      <td>-72.611100</td>\n",
              "      <td>-66.856433</td>\n",
              "      <td>-54.583709</td>\n",
              "      <td>-64.571581</td>\n",
              "      <td>-52.872744</td>\n",
              "      <td>-57.703096</td>\n",
              "      <td>-51.288291</td>\n",
              "      <td>-39.948491</td>\n",
              "      <td>-67.255217</td>\n",
              "      <td>-70.325537</td>\n",
              "      <td>...</td>\n",
              "      <td>-56.917793</td>\n",
              "      <td>-49.639721</td>\n",
              "      <td>-66.869561</td>\n",
              "      <td>-67.248489</td>\n",
              "      <td>-54.742004</td>\n",
              "      <td>-55.352928</td>\n",
              "      <td>-72.793990</td>\n",
              "      <td>-62.224932</td>\n",
              "      <td>-55.681433</td>\n",
              "      <td>-54.500185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52428</th>\n",
              "      <td>-143.665043</td>\n",
              "      <td>-140.470917</td>\n",
              "      <td>-122.895308</td>\n",
              "      <td>-137.684194</td>\n",
              "      <td>-148.219090</td>\n",
              "      <td>-146.304373</td>\n",
              "      <td>-134.410912</td>\n",
              "      <td>-149.946863</td>\n",
              "      <td>-139.445370</td>\n",
              "      <td>-138.728225</td>\n",
              "      <td>...</td>\n",
              "      <td>-132.207337</td>\n",
              "      <td>-145.391675</td>\n",
              "      <td>-146.003615</td>\n",
              "      <td>-144.069307</td>\n",
              "      <td>-126.763856</td>\n",
              "      <td>-143.188023</td>\n",
              "      <td>-145.668644</td>\n",
              "      <td>-132.632519</td>\n",
              "      <td>-116.082192</td>\n",
              "      <td>-132.311341</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52429 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25b46315-7117-4f75-a8ac-c463e218000e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-25b46315-7117-4f75-a8ac-c463e218000e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-25b46315-7117-4f75-a8ac-c463e218000e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f49d1d1c-3c94-4156-91be-17b25cb89d06\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f49d1d1c-3c94-4156-91be-17b25cb89d06')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f49d1d1c-3c94-4156-91be-17b25cb89d06 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# o\tPrediction output: direct output from the model, descaled (if needed for KPI calculation, e.g., for L1-RSRP difference)\n",
        "print('yTest')\n",
        "display(pd.DataFrame(yTest))\n",
        "print('yTest_descaled')\n",
        "display(pd.DataFrame(yTest_desclaed))\n",
        "\n",
        "yhat = model.predict(xTest)\n",
        "print('yhat')\n",
        "display(pd.DataFrame(yhat))\n",
        "\n",
        "print('yTest_rescaled')\n",
        "yTest_rescaled = scaler_raw_org.inverse_transform(yTest)\n",
        "display(pd.DataFrame(yTest_rescaled))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jECVGPafG4ur",
        "outputId": "4841938b-d39f-4c4d-9f98-27fe236ed040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model fcJalal\n",
            " --> inside getModel, parser.modelNameString=fcJalal\n",
            " --> inside getModel, parser.modelNameString=fcJalal\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_13 (Flatten)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 1024)              33792     \n",
            "                                                                 \n",
            " dropout_44 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout_45 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 32)                16416     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 575008 (2.19 MB)\n",
            "Trainable params: 575008 (2.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model Summary None\n",
            "Epoch 1/100\n",
            "830/830 [==============================] - 4s 3ms/step - loss: 0.0052 - mean_squared_error: 0.0052 - mean_absolute_error: 0.0475 - val_loss: 9.6874e-04 - val_mean_squared_error: 9.6874e-04 - val_mean_absolute_error: 0.0217\n",
            "Epoch 2/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - mean_absolute_error: 0.0305 - val_loss: 8.7816e-04 - val_mean_squared_error: 8.7816e-04 - val_mean_absolute_error: 0.0207\n",
            "Epoch 3/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0266 - val_loss: 7.1151e-04 - val_mean_squared_error: 7.1151e-04 - val_mean_absolute_error: 0.0182\n",
            "Epoch 4/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - mean_absolute_error: 0.0242 - val_loss: 6.6002e-04 - val_mean_squared_error: 6.6002e-04 - val_mean_absolute_error: 0.0174\n",
            "Epoch 5/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 9.3006e-04 - mean_squared_error: 9.3006e-04 - mean_absolute_error: 0.0227 - val_loss: 6.2579e-04 - val_mean_squared_error: 6.2579e-04 - val_mean_absolute_error: 0.0170\n",
            "Epoch 6/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 8.4825e-04 - mean_squared_error: 8.4825e-04 - mean_absolute_error: 0.0215 - val_loss: 6.2953e-04 - val_mean_squared_error: 6.2953e-04 - val_mean_absolute_error: 0.0174\n",
            "Epoch 7/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 7.9264e-04 - mean_squared_error: 7.9264e-04 - mean_absolute_error: 0.0206 - val_loss: 6.5832e-04 - val_mean_squared_error: 6.5832e-04 - val_mean_absolute_error: 0.0186\n",
            "Epoch 8/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 7.4729e-04 - mean_squared_error: 7.4729e-04 - mean_absolute_error: 0.0199 - val_loss: 5.6078e-04 - val_mean_squared_error: 5.6078e-04 - val_mean_absolute_error: 0.0160\n",
            "Epoch 9/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 7.1165e-04 - mean_squared_error: 7.1165e-04 - mean_absolute_error: 0.0193 - val_loss: 5.5398e-04 - val_mean_squared_error: 5.5398e-04 - val_mean_absolute_error: 0.0158\n",
            "Epoch 10/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 6.7951e-04 - mean_squared_error: 6.7951e-04 - mean_absolute_error: 0.0187 - val_loss: 5.5153e-04 - val_mean_squared_error: 5.5153e-04 - val_mean_absolute_error: 0.0159\n",
            "Epoch 11/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 6.5642e-04 - mean_squared_error: 6.5642e-04 - mean_absolute_error: 0.0183 - val_loss: 5.4255e-04 - val_mean_squared_error: 5.4255e-04 - val_mean_absolute_error: 0.0161\n",
            "Epoch 12/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 6.3979e-04 - mean_squared_error: 6.3979e-04 - mean_absolute_error: 0.0180 - val_loss: 5.3438e-04 - val_mean_squared_error: 5.3438e-04 - val_mean_absolute_error: 0.0156\n",
            "Epoch 13/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 6.2237e-04 - mean_squared_error: 6.2237e-04 - mean_absolute_error: 0.0177 - val_loss: 5.2166e-04 - val_mean_squared_error: 5.2166e-04 - val_mean_absolute_error: 0.0153\n",
            "Epoch 14/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 6.0965e-04 - mean_squared_error: 6.0965e-04 - mean_absolute_error: 0.0174 - val_loss: 5.0994e-04 - val_mean_squared_error: 5.0994e-04 - val_mean_absolute_error: 0.0150\n",
            "Epoch 15/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 6.0179e-04 - mean_squared_error: 6.0179e-04 - mean_absolute_error: 0.0173 - val_loss: 5.1576e-04 - val_mean_squared_error: 5.1576e-04 - val_mean_absolute_error: 0.0152\n",
            "Epoch 16/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.9234e-04 - mean_squared_error: 5.9234e-04 - mean_absolute_error: 0.0171 - val_loss: 4.9490e-04 - val_mean_squared_error: 4.9490e-04 - val_mean_absolute_error: 0.0148\n",
            "Epoch 17/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.8707e-04 - mean_squared_error: 5.8707e-04 - mean_absolute_error: 0.0170 - val_loss: 4.9556e-04 - val_mean_squared_error: 4.9556e-04 - val_mean_absolute_error: 0.0148\n",
            "Epoch 18/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.8047e-04 - mean_squared_error: 5.8047e-04 - mean_absolute_error: 0.0169 - val_loss: 5.0733e-04 - val_mean_squared_error: 5.0733e-04 - val_mean_absolute_error: 0.0151\n",
            "Epoch 19/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.7636e-04 - mean_squared_error: 5.7636e-04 - mean_absolute_error: 0.0168 - val_loss: 4.9808e-04 - val_mean_squared_error: 4.9808e-04 - val_mean_absolute_error: 0.0151\n",
            "Epoch 20/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.7126e-04 - mean_squared_error: 5.7126e-04 - mean_absolute_error: 0.0167 - val_loss: 5.0502e-04 - val_mean_squared_error: 5.0502e-04 - val_mean_absolute_error: 0.0155\n",
            "Epoch 21/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.6981e-04 - mean_squared_error: 5.6981e-04 - mean_absolute_error: 0.0167 - val_loss: 4.8505e-04 - val_mean_squared_error: 4.8505e-04 - val_mean_absolute_error: 0.0147\n",
            "Epoch 22/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.6595e-04 - mean_squared_error: 5.6595e-04 - mean_absolute_error: 0.0166 - val_loss: 4.9771e-04 - val_mean_squared_error: 4.9771e-04 - val_mean_absolute_error: 0.0150\n",
            "Epoch 23/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.6294e-04 - mean_squared_error: 5.6294e-04 - mean_absolute_error: 0.0166 - val_loss: 4.8581e-04 - val_mean_squared_error: 4.8581e-04 - val_mean_absolute_error: 0.0146\n",
            "Epoch 24/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.5935e-04 - mean_squared_error: 5.5935e-04 - mean_absolute_error: 0.0165 - val_loss: 4.7816e-04 - val_mean_squared_error: 4.7816e-04 - val_mean_absolute_error: 0.0146\n",
            "Epoch 25/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.5527e-04 - mean_squared_error: 5.5527e-04 - mean_absolute_error: 0.0165 - val_loss: 4.7233e-04 - val_mean_squared_error: 4.7233e-04 - val_mean_absolute_error: 0.0146\n",
            "Epoch 26/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.5362e-04 - mean_squared_error: 5.5362e-04 - mean_absolute_error: 0.0164 - val_loss: 4.7619e-04 - val_mean_squared_error: 4.7619e-04 - val_mean_absolute_error: 0.0145\n",
            "Epoch 27/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.5096e-04 - mean_squared_error: 5.5096e-04 - mean_absolute_error: 0.0164 - val_loss: 4.6647e-04 - val_mean_squared_error: 4.6647e-04 - val_mean_absolute_error: 0.0143\n",
            "Epoch 28/100\n",
            "830/830 [==============================] - 2s 3ms/step - loss: 5.4843e-04 - mean_squared_error: 5.4843e-04 - mean_absolute_error: 0.0163 - val_loss: 4.6073e-04 - val_mean_squared_error: 4.6073e-04 - val_mean_absolute_error: 0.0141\n",
            "Epoch 29/100\n",
            "830/830 [==============================] - 2s 3ms/step - loss: 5.4565e-04 - mean_squared_error: 5.4565e-04 - mean_absolute_error: 0.0163 - val_loss: 4.6962e-04 - val_mean_squared_error: 4.6962e-04 - val_mean_absolute_error: 0.0145\n",
            "Epoch 30/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.4473e-04 - mean_squared_error: 5.4473e-04 - mean_absolute_error: 0.0163 - val_loss: 4.6100e-04 - val_mean_squared_error: 4.6100e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 31/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.4230e-04 - mean_squared_error: 5.4230e-04 - mean_absolute_error: 0.0162 - val_loss: 4.6124e-04 - val_mean_squared_error: 4.6124e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 32/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.3914e-04 - mean_squared_error: 5.3914e-04 - mean_absolute_error: 0.0162 - val_loss: 4.6677e-04 - val_mean_squared_error: 4.6677e-04 - val_mean_absolute_error: 0.0143\n",
            "Epoch 33/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.3764e-04 - mean_squared_error: 5.3764e-04 - mean_absolute_error: 0.0162 - val_loss: 4.5985e-04 - val_mean_squared_error: 4.5985e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 34/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.3673e-04 - mean_squared_error: 5.3673e-04 - mean_absolute_error: 0.0162 - val_loss: 4.6383e-04 - val_mean_squared_error: 4.6383e-04 - val_mean_absolute_error: 0.0143\n",
            "Epoch 35/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.3547e-04 - mean_squared_error: 5.3547e-04 - mean_absolute_error: 0.0161 - val_loss: 4.5408e-04 - val_mean_squared_error: 4.5408e-04 - val_mean_absolute_error: 0.0140\n",
            "Epoch 36/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.3406e-04 - mean_squared_error: 5.3406e-04 - mean_absolute_error: 0.0161 - val_loss: 4.6560e-04 - val_mean_squared_error: 4.6560e-04 - val_mean_absolute_error: 0.0144\n",
            "Epoch 37/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.3205e-04 - mean_squared_error: 5.3205e-04 - mean_absolute_error: 0.0161 - val_loss: 4.5551e-04 - val_mean_squared_error: 4.5551e-04 - val_mean_absolute_error: 0.0141\n",
            "Epoch 38/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.3064e-04 - mean_squared_error: 5.3064e-04 - mean_absolute_error: 0.0160 - val_loss: 4.6182e-04 - val_mean_squared_error: 4.6182e-04 - val_mean_absolute_error: 0.0143\n",
            "Epoch 39/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.2919e-04 - mean_squared_error: 5.2919e-04 - mean_absolute_error: 0.0160 - val_loss: 4.5854e-04 - val_mean_squared_error: 4.5854e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 40/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.2766e-04 - mean_squared_error: 5.2766e-04 - mean_absolute_error: 0.0160 - val_loss: 4.4632e-04 - val_mean_squared_error: 4.4632e-04 - val_mean_absolute_error: 0.0139\n",
            "Epoch 41/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.2660e-04 - mean_squared_error: 5.2660e-04 - mean_absolute_error: 0.0160 - val_loss: 4.4861e-04 - val_mean_squared_error: 4.4861e-04 - val_mean_absolute_error: 0.0141\n",
            "Epoch 42/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.2403e-04 - mean_squared_error: 5.2403e-04 - mean_absolute_error: 0.0159 - val_loss: 4.4772e-04 - val_mean_squared_error: 4.4772e-04 - val_mean_absolute_error: 0.0140\n",
            "Epoch 43/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.2564e-04 - mean_squared_error: 5.2564e-04 - mean_absolute_error: 0.0160 - val_loss: 4.6150e-04 - val_mean_squared_error: 4.6150e-04 - val_mean_absolute_error: 0.0144\n",
            "Epoch 44/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.2243e-04 - mean_squared_error: 5.2243e-04 - mean_absolute_error: 0.0159 - val_loss: 4.5027e-04 - val_mean_squared_error: 4.5027e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 45/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.2226e-04 - mean_squared_error: 5.2226e-04 - mean_absolute_error: 0.0159 - val_loss: 4.4518e-04 - val_mean_squared_error: 4.4518e-04 - val_mean_absolute_error: 0.0139\n",
            "Epoch 46/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.2057e-04 - mean_squared_error: 5.2057e-04 - mean_absolute_error: 0.0159 - val_loss: 4.4018e-04 - val_mean_squared_error: 4.4018e-04 - val_mean_absolute_error: 0.0137\n",
            "Epoch 47/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.1977e-04 - mean_squared_error: 5.1977e-04 - mean_absolute_error: 0.0159 - val_loss: 4.4770e-04 - val_mean_squared_error: 4.4770e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 48/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.1911e-04 - mean_squared_error: 5.1911e-04 - mean_absolute_error: 0.0159 - val_loss: 4.4444e-04 - val_mean_squared_error: 4.4444e-04 - val_mean_absolute_error: 0.0139\n",
            "Epoch 49/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.1772e-04 - mean_squared_error: 5.1772e-04 - mean_absolute_error: 0.0158 - val_loss: 4.4191e-04 - val_mean_squared_error: 4.4191e-04 - val_mean_absolute_error: 0.0139\n",
            "Epoch 50/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.1683e-04 - mean_squared_error: 5.1683e-04 - mean_absolute_error: 0.0158 - val_loss: 4.4078e-04 - val_mean_squared_error: 4.4078e-04 - val_mean_absolute_error: 0.0138\n",
            "Epoch 51/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.1626e-04 - mean_squared_error: 5.1626e-04 - mean_absolute_error: 0.0158 - val_loss: 4.5269e-04 - val_mean_squared_error: 4.5269e-04 - val_mean_absolute_error: 0.0144\n",
            "Epoch 52/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.1442e-04 - mean_squared_error: 5.1442e-04 - mean_absolute_error: 0.0158 - val_loss: 4.4534e-04 - val_mean_squared_error: 4.4534e-04 - val_mean_absolute_error: 0.0141\n",
            "Epoch 53/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.1461e-04 - mean_squared_error: 5.1461e-04 - mean_absolute_error: 0.0158 - val_loss: 4.4249e-04 - val_mean_squared_error: 4.4249e-04 - val_mean_absolute_error: 0.0141\n",
            "Epoch 54/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.1318e-04 - mean_squared_error: 5.1318e-04 - mean_absolute_error: 0.0158 - val_loss: 4.3887e-04 - val_mean_squared_error: 4.3887e-04 - val_mean_absolute_error: 0.0138\n",
            "Epoch 55/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.1243e-04 - mean_squared_error: 5.1243e-04 - mean_absolute_error: 0.0158 - val_loss: 4.3000e-04 - val_mean_squared_error: 4.3000e-04 - val_mean_absolute_error: 0.0137\n",
            "Epoch 56/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.1310e-04 - mean_squared_error: 5.1310e-04 - mean_absolute_error: 0.0158 - val_loss: 4.4219e-04 - val_mean_squared_error: 4.4219e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 57/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.1163e-04 - mean_squared_error: 5.1163e-04 - mean_absolute_error: 0.0157 - val_loss: 4.3503e-04 - val_mean_squared_error: 4.3503e-04 - val_mean_absolute_error: 0.0138\n",
            "Epoch 58/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.1076e-04 - mean_squared_error: 5.1076e-04 - mean_absolute_error: 0.0157 - val_loss: 4.2921e-04 - val_mean_squared_error: 4.2921e-04 - val_mean_absolute_error: 0.0136\n",
            "Epoch 59/100\n",
            "830/830 [==============================] - 2s 3ms/step - loss: 5.1018e-04 - mean_squared_error: 5.1018e-04 - mean_absolute_error: 0.0157 - val_loss: 4.2888e-04 - val_mean_squared_error: 4.2888e-04 - val_mean_absolute_error: 0.0136\n",
            "Epoch 60/100\n",
            "830/830 [==============================] - 2s 3ms/step - loss: 5.0892e-04 - mean_squared_error: 5.0892e-04 - mean_absolute_error: 0.0157 - val_loss: 4.4074e-04 - val_mean_squared_error: 4.4074e-04 - val_mean_absolute_error: 0.0140\n",
            "Epoch 61/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0897e-04 - mean_squared_error: 5.0897e-04 - mean_absolute_error: 0.0157 - val_loss: 4.3548e-04 - val_mean_squared_error: 4.3548e-04 - val_mean_absolute_error: 0.0139\n",
            "Epoch 62/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0825e-04 - mean_squared_error: 5.0825e-04 - mean_absolute_error: 0.0157 - val_loss: 4.3359e-04 - val_mean_squared_error: 4.3359e-04 - val_mean_absolute_error: 0.0138\n",
            "Epoch 63/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0717e-04 - mean_squared_error: 5.0717e-04 - mean_absolute_error: 0.0157 - val_loss: 4.4106e-04 - val_mean_squared_error: 4.4106e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 64/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0680e-04 - mean_squared_error: 5.0680e-04 - mean_absolute_error: 0.0157 - val_loss: 4.3219e-04 - val_mean_squared_error: 4.3219e-04 - val_mean_absolute_error: 0.0138\n",
            "Epoch 65/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0606e-04 - mean_squared_error: 5.0606e-04 - mean_absolute_error: 0.0157 - val_loss: 4.3771e-04 - val_mean_squared_error: 4.3771e-04 - val_mean_absolute_error: 0.0141\n",
            "Epoch 66/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0514e-04 - mean_squared_error: 5.0514e-04 - mean_absolute_error: 0.0157 - val_loss: 4.2673e-04 - val_mean_squared_error: 4.2673e-04 - val_mean_absolute_error: 0.0136\n",
            "Epoch 67/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0577e-04 - mean_squared_error: 5.0577e-04 - mean_absolute_error: 0.0157 - val_loss: 4.2790e-04 - val_mean_squared_error: 4.2790e-04 - val_mean_absolute_error: 0.0137\n",
            "Epoch 68/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0359e-04 - mean_squared_error: 5.0359e-04 - mean_absolute_error: 0.0156 - val_loss: 4.3885e-04 - val_mean_squared_error: 4.3885e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 69/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0367e-04 - mean_squared_error: 5.0367e-04 - mean_absolute_error: 0.0156 - val_loss: 4.3837e-04 - val_mean_squared_error: 4.3837e-04 - val_mean_absolute_error: 0.0141\n",
            "Epoch 70/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0308e-04 - mean_squared_error: 5.0308e-04 - mean_absolute_error: 0.0156 - val_loss: 4.3816e-04 - val_mean_squared_error: 4.3816e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 71/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0225e-04 - mean_squared_error: 5.0225e-04 - mean_absolute_error: 0.0156 - val_loss: 4.4056e-04 - val_mean_squared_error: 4.4056e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 72/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0224e-04 - mean_squared_error: 5.0224e-04 - mean_absolute_error: 0.0156 - val_loss: 4.3162e-04 - val_mean_squared_error: 4.3162e-04 - val_mean_absolute_error: 0.0140\n",
            "Epoch 73/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0160e-04 - mean_squared_error: 5.0160e-04 - mean_absolute_error: 0.0156 - val_loss: 4.3415e-04 - val_mean_squared_error: 4.3415e-04 - val_mean_absolute_error: 0.0140\n",
            "Epoch 74/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 5.0127e-04 - mean_squared_error: 5.0127e-04 - mean_absolute_error: 0.0156 - val_loss: 4.3242e-04 - val_mean_squared_error: 4.3242e-04 - val_mean_absolute_error: 0.0139\n",
            "Epoch 75/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9992e-04 - mean_squared_error: 4.9992e-04 - mean_absolute_error: 0.0156 - val_loss: 4.5030e-04 - val_mean_squared_error: 4.5030e-04 - val_mean_absolute_error: 0.0147\n",
            "Epoch 76/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9954e-04 - mean_squared_error: 4.9954e-04 - mean_absolute_error: 0.0156 - val_loss: 4.1968e-04 - val_mean_squared_error: 4.1968e-04 - val_mean_absolute_error: 0.0135\n",
            "Epoch 77/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9968e-04 - mean_squared_error: 4.9968e-04 - mean_absolute_error: 0.0156 - val_loss: 4.2584e-04 - val_mean_squared_error: 4.2584e-04 - val_mean_absolute_error: 0.0137\n",
            "Epoch 78/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9821e-04 - mean_squared_error: 4.9821e-04 - mean_absolute_error: 0.0155 - val_loss: 4.2103e-04 - val_mean_squared_error: 4.2103e-04 - val_mean_absolute_error: 0.0135\n",
            "Epoch 79/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9813e-04 - mean_squared_error: 4.9813e-04 - mean_absolute_error: 0.0155 - val_loss: 4.1664e-04 - val_mean_squared_error: 4.1664e-04 - val_mean_absolute_error: 0.0134\n",
            "Epoch 80/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9820e-04 - mean_squared_error: 4.9820e-04 - mean_absolute_error: 0.0155 - val_loss: 4.2417e-04 - val_mean_squared_error: 4.2417e-04 - val_mean_absolute_error: 0.0136\n",
            "Epoch 81/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9753e-04 - mean_squared_error: 4.9753e-04 - mean_absolute_error: 0.0155 - val_loss: 4.2403e-04 - val_mean_squared_error: 4.2403e-04 - val_mean_absolute_error: 0.0137\n",
            "Epoch 82/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9780e-04 - mean_squared_error: 4.9780e-04 - mean_absolute_error: 0.0155 - val_loss: 4.3448e-04 - val_mean_squared_error: 4.3448e-04 - val_mean_absolute_error: 0.0142\n",
            "Epoch 83/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9634e-04 - mean_squared_error: 4.9634e-04 - mean_absolute_error: 0.0155 - val_loss: 4.2240e-04 - val_mean_squared_error: 4.2240e-04 - val_mean_absolute_error: 0.0138\n",
            "Epoch 84/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9590e-04 - mean_squared_error: 4.9590e-04 - mean_absolute_error: 0.0155 - val_loss: 4.2722e-04 - val_mean_squared_error: 4.2722e-04 - val_mean_absolute_error: 0.0137\n",
            "Epoch 85/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9515e-04 - mean_squared_error: 4.9515e-04 - mean_absolute_error: 0.0155 - val_loss: 4.2003e-04 - val_mean_squared_error: 4.2003e-04 - val_mean_absolute_error: 0.0136\n",
            "Epoch 86/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9465e-04 - mean_squared_error: 4.9465e-04 - mean_absolute_error: 0.0155 - val_loss: 4.4640e-04 - val_mean_squared_error: 4.4640e-04 - val_mean_absolute_error: 0.0145\n",
            "Epoch 87/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9514e-04 - mean_squared_error: 4.9514e-04 - mean_absolute_error: 0.0155 - val_loss: 4.3563e-04 - val_mean_squared_error: 4.3563e-04 - val_mean_absolute_error: 0.0143\n",
            "Epoch 88/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9437e-04 - mean_squared_error: 4.9437e-04 - mean_absolute_error: 0.0155 - val_loss: 4.1603e-04 - val_mean_squared_error: 4.1603e-04 - val_mean_absolute_error: 0.0134\n",
            "Epoch 89/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9405e-04 - mean_squared_error: 4.9405e-04 - mean_absolute_error: 0.0155 - val_loss: 4.2100e-04 - val_mean_squared_error: 4.2100e-04 - val_mean_absolute_error: 0.0135\n",
            "Epoch 90/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9349e-04 - mean_squared_error: 4.9349e-04 - mean_absolute_error: 0.0155 - val_loss: 4.1522e-04 - val_mean_squared_error: 4.1522e-04 - val_mean_absolute_error: 0.0134\n",
            "Epoch 91/100\n",
            "830/830 [==============================] - 2s 3ms/step - loss: 4.9351e-04 - mean_squared_error: 4.9351e-04 - mean_absolute_error: 0.0155 - val_loss: 4.3655e-04 - val_mean_squared_error: 4.3655e-04 - val_mean_absolute_error: 0.0144\n",
            "Epoch 92/100\n",
            "830/830 [==============================] - 2s 3ms/step - loss: 4.9431e-04 - mean_squared_error: 4.9431e-04 - mean_absolute_error: 0.0155 - val_loss: 4.5004e-04 - val_mean_squared_error: 4.5004e-04 - val_mean_absolute_error: 0.0149\n",
            "Epoch 93/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9310e-04 - mean_squared_error: 4.9310e-04 - mean_absolute_error: 0.0155 - val_loss: 4.1490e-04 - val_mean_squared_error: 4.1490e-04 - val_mean_absolute_error: 0.0135\n",
            "Epoch 94/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9233e-04 - mean_squared_error: 4.9233e-04 - mean_absolute_error: 0.0154 - val_loss: 4.3634e-04 - val_mean_squared_error: 4.3634e-04 - val_mean_absolute_error: 0.0145\n",
            "Epoch 95/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9136e-04 - mean_squared_error: 4.9136e-04 - mean_absolute_error: 0.0154 - val_loss: 4.3030e-04 - val_mean_squared_error: 4.3030e-04 - val_mean_absolute_error: 0.0141\n",
            "Epoch 96/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9167e-04 - mean_squared_error: 4.9167e-04 - mean_absolute_error: 0.0154 - val_loss: 4.1553e-04 - val_mean_squared_error: 4.1553e-04 - val_mean_absolute_error: 0.0133\n",
            "Epoch 97/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9084e-04 - mean_squared_error: 4.9084e-04 - mean_absolute_error: 0.0154 - val_loss: 4.1746e-04 - val_mean_squared_error: 4.1746e-04 - val_mean_absolute_error: 0.0136\n",
            "Epoch 98/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.9097e-04 - mean_squared_error: 4.9097e-04 - mean_absolute_error: 0.0154 - val_loss: 4.1747e-04 - val_mean_squared_error: 4.1747e-04 - val_mean_absolute_error: 0.0135\n",
            "Epoch 99/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.8982e-04 - mean_squared_error: 4.8982e-04 - mean_absolute_error: 0.0154 - val_loss: 4.2349e-04 - val_mean_squared_error: 4.2349e-04 - val_mean_absolute_error: 0.0136\n",
            "Epoch 100/100\n",
            "830/830 [==============================] - 3s 3ms/step - loss: 4.8990e-04 - mean_squared_error: 4.8990e-04 - mean_absolute_error: 0.0154 - val_loss: 4.1065e-04 - val_mean_squared_error: 4.1065e-04 - val_mean_absolute_error: 0.0133\n",
            "1639/1639 [==============================] - 2s 1ms/step\n",
            "The Top-k (k=[1,2,4,8]) accurancies are: {1: 0.5605104045471018, 2: 0.8517232829159435, 4: 0.9841690667378741, 6: 0.9989318888401457, 8: 0.9999237063457247}\n",
            "true_max_RSRP -116.08219151174858\n",
            "The RSRP difference is: [1.9889521462984794, 1.0529634844252609, 0.49206720357636613, 0.3042306492179959, 0.20289011037441293]\n",
            "The naive RSRP difference is: (5.795061489201917, 0.33639779511339146)\n",
            "Is my y test data the same is my predicted data? False\n",
            "true_max_RSRP -116.08219151174858\n",
            "Training model fcJalal2\n",
            " --> inside getModel, parser.modelNameString=fcJalal2\n",
            " --> inside getModel, parser.modelNameString=fcJalal2\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_15 (Flatten)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 1024)              33792     \n",
            "                                                                 \n",
            " batch_normalization_36 (Ba  (None, 1024)              4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_48 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " batch_normalization_37 (Ba  (None, 512)               2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_49 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 32)                16416     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 581152 (2.22 MB)\n",
            "Trainable params: 578080 (2.21 MB)\n",
            "Non-trainable params: 3072 (12.00 KB)\n",
            "_________________________________________________________________\n",
            "Model Summary None\n",
            "Epoch 1/100\n",
            "830/830 [==============================] - 5s 4ms/step - loss: 0.1089 - mean_squared_error: 0.1089 - mean_absolute_error: 0.2035 - val_loss: 0.0033 - val_mean_squared_error: 0.0033 - val_mean_absolute_error: 0.0456\n",
            "Epoch 2/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - mean_absolute_error: 0.0571 - val_loss: 0.0016 - val_mean_squared_error: 0.0016 - val_mean_absolute_error: 0.0305\n",
            "Epoch 3/100\n",
            "830/830 [==============================] - 4s 4ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - mean_absolute_error: 0.0354 - val_loss: 9.8544e-04 - val_mean_squared_error: 9.8544e-04 - val_mean_absolute_error: 0.0236\n",
            "Epoch 4/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - mean_absolute_error: 0.0308 - val_loss: 0.0019 - val_mean_squared_error: 0.0019 - val_mean_absolute_error: 0.0340\n",
            "Epoch 5/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - mean_absolute_error: 0.0283 - val_loss: 0.0017 - val_mean_squared_error: 0.0017 - val_mean_absolute_error: 0.0335\n",
            "Epoch 6/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0261 - val_loss: 0.0016 - val_mean_squared_error: 0.0016 - val_mean_absolute_error: 0.0313\n",
            "Epoch 7/100\n",
            "830/830 [==============================] - 4s 4ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - mean_absolute_error: 0.0242 - val_loss: 8.7645e-04 - val_mean_squared_error: 8.7645e-04 - val_mean_absolute_error: 0.0223\n",
            "Epoch 8/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 9.3039e-04 - mean_squared_error: 9.3039e-04 - mean_absolute_error: 0.0227 - val_loss: 0.0010 - val_mean_squared_error: 0.0010 - val_mean_absolute_error: 0.0242\n",
            "Epoch 9/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 8.6280e-04 - mean_squared_error: 8.6280e-04 - mean_absolute_error: 0.0218 - val_loss: 8.0837e-04 - val_mean_squared_error: 8.0837e-04 - val_mean_absolute_error: 0.0209\n",
            "Epoch 10/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 8.0958e-04 - mean_squared_error: 8.0958e-04 - mean_absolute_error: 0.0210 - val_loss: 6.9195e-04 - val_mean_squared_error: 6.9195e-04 - val_mean_absolute_error: 0.0190\n",
            "Epoch 11/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 7.7950e-04 - mean_squared_error: 7.7950e-04 - mean_absolute_error: 0.0205 - val_loss: 7.7212e-04 - val_mean_squared_error: 7.7212e-04 - val_mean_absolute_error: 0.0205\n",
            "Epoch 12/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 7.4972e-04 - mean_squared_error: 7.4972e-04 - mean_absolute_error: 0.0200 - val_loss: 7.1438e-04 - val_mean_squared_error: 7.1438e-04 - val_mean_absolute_error: 0.0195\n",
            "Epoch 13/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 7.2774e-04 - mean_squared_error: 7.2774e-04 - mean_absolute_error: 0.0197 - val_loss: 7.9807e-04 - val_mean_squared_error: 7.9807e-04 - val_mean_absolute_error: 0.0211\n",
            "Epoch 14/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 7.0402e-04 - mean_squared_error: 7.0402e-04 - mean_absolute_error: 0.0193 - val_loss: 8.0950e-04 - val_mean_squared_error: 8.0950e-04 - val_mean_absolute_error: 0.0216\n",
            "Epoch 15/100\n",
            "830/830 [==============================] - 4s 4ms/step - loss: 6.7481e-04 - mean_squared_error: 6.7481e-04 - mean_absolute_error: 0.0188 - val_loss: 5.9827e-04 - val_mean_squared_error: 5.9827e-04 - val_mean_absolute_error: 0.0177\n",
            "Epoch 16/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 6.5225e-04 - mean_squared_error: 6.5225e-04 - mean_absolute_error: 0.0184 - val_loss: 9.5061e-04 - val_mean_squared_error: 9.5061e-04 - val_mean_absolute_error: 0.0237\n",
            "Epoch 17/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 6.3825e-04 - mean_squared_error: 6.3825e-04 - mean_absolute_error: 0.0181 - val_loss: 7.0279e-04 - val_mean_squared_error: 7.0279e-04 - val_mean_absolute_error: 0.0197\n",
            "Epoch 18/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 6.2192e-04 - mean_squared_error: 6.2192e-04 - mean_absolute_error: 0.0178 - val_loss: 5.8041e-04 - val_mean_squared_error: 5.8041e-04 - val_mean_absolute_error: 0.0173\n",
            "Epoch 19/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 6.1224e-04 - mean_squared_error: 6.1224e-04 - mean_absolute_error: 0.0177 - val_loss: 6.2485e-04 - val_mean_squared_error: 6.2485e-04 - val_mean_absolute_error: 0.0184\n",
            "Epoch 20/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 6.0225e-04 - mean_squared_error: 6.0225e-04 - mean_absolute_error: 0.0175 - val_loss: 6.6482e-04 - val_mean_squared_error: 6.6482e-04 - val_mean_absolute_error: 0.0189\n",
            "Epoch 21/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.9482e-04 - mean_squared_error: 5.9482e-04 - mean_absolute_error: 0.0174 - val_loss: 8.4754e-04 - val_mean_squared_error: 8.4754e-04 - val_mean_absolute_error: 0.0226\n",
            "Epoch 22/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.8724e-04 - mean_squared_error: 5.8724e-04 - mean_absolute_error: 0.0172 - val_loss: 5.7453e-04 - val_mean_squared_error: 5.7453e-04 - val_mean_absolute_error: 0.0170\n",
            "Epoch 23/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.8108e-04 - mean_squared_error: 5.8108e-04 - mean_absolute_error: 0.0171 - val_loss: 9.9061e-04 - val_mean_squared_error: 9.9061e-04 - val_mean_absolute_error: 0.0221\n",
            "Epoch 24/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.7643e-04 - mean_squared_error: 5.7643e-04 - mean_absolute_error: 0.0170 - val_loss: 6.2778e-04 - val_mean_squared_error: 6.2778e-04 - val_mean_absolute_error: 0.0179\n",
            "Epoch 25/100\n",
            "830/830 [==============================] - 4s 4ms/step - loss: 5.7090e-04 - mean_squared_error: 5.7090e-04 - mean_absolute_error: 0.0169 - val_loss: 7.1838e-04 - val_mean_squared_error: 7.1838e-04 - val_mean_absolute_error: 0.0196\n",
            "Epoch 26/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.6622e-04 - mean_squared_error: 5.6622e-04 - mean_absolute_error: 0.0168 - val_loss: 5.6358e-04 - val_mean_squared_error: 5.6358e-04 - val_mean_absolute_error: 0.0165\n",
            "Epoch 27/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.6217e-04 - mean_squared_error: 5.6217e-04 - mean_absolute_error: 0.0167 - val_loss: 5.7780e-04 - val_mean_squared_error: 5.7780e-04 - val_mean_absolute_error: 0.0172\n",
            "Epoch 28/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.5996e-04 - mean_squared_error: 5.5996e-04 - mean_absolute_error: 0.0166 - val_loss: 5.5753e-04 - val_mean_squared_error: 5.5753e-04 - val_mean_absolute_error: 0.0166\n",
            "Epoch 29/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.5689e-04 - mean_squared_error: 5.5689e-04 - mean_absolute_error: 0.0166 - val_loss: 6.1559e-04 - val_mean_squared_error: 6.1559e-04 - val_mean_absolute_error: 0.0188\n",
            "Epoch 30/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.5535e-04 - mean_squared_error: 5.5535e-04 - mean_absolute_error: 0.0165 - val_loss: 6.3903e-04 - val_mean_squared_error: 6.3903e-04 - val_mean_absolute_error: 0.0185\n",
            "Epoch 31/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.5421e-04 - mean_squared_error: 5.5421e-04 - mean_absolute_error: 0.0165 - val_loss: 6.9846e-04 - val_mean_squared_error: 6.9846e-04 - val_mean_absolute_error: 0.0204\n",
            "Epoch 32/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.5224e-04 - mean_squared_error: 5.5224e-04 - mean_absolute_error: 0.0165 - val_loss: 9.0964e-04 - val_mean_squared_error: 9.0964e-04 - val_mean_absolute_error: 0.0237\n",
            "Epoch 33/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4916e-04 - mean_squared_error: 5.4916e-04 - mean_absolute_error: 0.0164 - val_loss: 0.0011 - val_mean_squared_error: 0.0011 - val_mean_absolute_error: 0.0268\n",
            "Epoch 34/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4911e-04 - mean_squared_error: 5.4911e-04 - mean_absolute_error: 0.0164 - val_loss: 0.0019 - val_mean_squared_error: 0.0019 - val_mean_absolute_error: 0.0380\n",
            "Epoch 35/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4771e-04 - mean_squared_error: 5.4771e-04 - mean_absolute_error: 0.0164 - val_loss: 0.0011 - val_mean_squared_error: 0.0011 - val_mean_absolute_error: 0.0272\n",
            "Epoch 36/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4669e-04 - mean_squared_error: 5.4669e-04 - mean_absolute_error: 0.0164 - val_loss: 7.6942e-04 - val_mean_squared_error: 7.6942e-04 - val_mean_absolute_error: 0.0221\n",
            "Epoch 37/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4638e-04 - mean_squared_error: 5.4638e-04 - mean_absolute_error: 0.0164 - val_loss: 9.0140e-04 - val_mean_squared_error: 9.0140e-04 - val_mean_absolute_error: 0.0246\n",
            "Epoch 38/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4485e-04 - mean_squared_error: 5.4485e-04 - mean_absolute_error: 0.0164 - val_loss: 8.7886e-04 - val_mean_squared_error: 8.7886e-04 - val_mean_absolute_error: 0.0238\n",
            "Epoch 39/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4469e-04 - mean_squared_error: 5.4469e-04 - mean_absolute_error: 0.0163 - val_loss: 9.0340e-04 - val_mean_squared_error: 9.0340e-04 - val_mean_absolute_error: 0.0242\n",
            "Epoch 40/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4318e-04 - mean_squared_error: 5.4318e-04 - mean_absolute_error: 0.0163 - val_loss: 9.6533e-04 - val_mean_squared_error: 9.6533e-04 - val_mean_absolute_error: 0.0258\n",
            "Epoch 41/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4341e-04 - mean_squared_error: 5.4341e-04 - mean_absolute_error: 0.0163 - val_loss: 9.1153e-04 - val_mean_squared_error: 9.1153e-04 - val_mean_absolute_error: 0.0241\n",
            "Epoch 42/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4316e-04 - mean_squared_error: 5.4316e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0010 - val_mean_squared_error: 0.0010 - val_mean_absolute_error: 0.0271\n",
            "Epoch 43/100\n",
            "830/830 [==============================] - 4s 4ms/step - loss: 5.4248e-04 - mean_squared_error: 5.4248e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0012 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0298\n",
            "Epoch 44/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4169e-04 - mean_squared_error: 5.4169e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0013 - val_mean_squared_error: 0.0013 - val_mean_absolute_error: 0.0315\n",
            "Epoch 45/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4113e-04 - mean_squared_error: 5.4113e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0013 - val_mean_squared_error: 0.0013 - val_mean_absolute_error: 0.0310\n",
            "Epoch 46/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4212e-04 - mean_squared_error: 5.4212e-04 - mean_absolute_error: 0.0163 - val_loss: 7.6560e-04 - val_mean_squared_error: 7.6560e-04 - val_mean_absolute_error: 0.0218\n",
            "Epoch 47/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4136e-04 - mean_squared_error: 5.4136e-04 - mean_absolute_error: 0.0163 - val_loss: 9.4415e-04 - val_mean_squared_error: 9.4415e-04 - val_mean_absolute_error: 0.0250\n",
            "Epoch 48/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4119e-04 - mean_squared_error: 5.4119e-04 - mean_absolute_error: 0.0163 - val_loss: 7.4987e-04 - val_mean_squared_error: 7.4987e-04 - val_mean_absolute_error: 0.0219\n",
            "Epoch 49/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3993e-04 - mean_squared_error: 5.3993e-04 - mean_absolute_error: 0.0163 - val_loss: 9.1451e-04 - val_mean_squared_error: 9.1451e-04 - val_mean_absolute_error: 0.0249\n",
            "Epoch 50/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.4045e-04 - mean_squared_error: 5.4045e-04 - mean_absolute_error: 0.0163 - val_loss: 7.3231e-04 - val_mean_squared_error: 7.3231e-04 - val_mean_absolute_error: 0.0214\n",
            "Epoch 51/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3915e-04 - mean_squared_error: 5.3915e-04 - mean_absolute_error: 0.0163 - val_loss: 8.0741e-04 - val_mean_squared_error: 8.0741e-04 - val_mean_absolute_error: 0.0234\n",
            "Epoch 52/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3881e-04 - mean_squared_error: 5.3881e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0011 - val_mean_squared_error: 0.0011 - val_mean_absolute_error: 0.0277\n",
            "Epoch 53/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3863e-04 - mean_squared_error: 5.3863e-04 - mean_absolute_error: 0.0163 - val_loss: 9.3571e-04 - val_mean_squared_error: 9.3571e-04 - val_mean_absolute_error: 0.0249\n",
            "Epoch 54/100\n",
            "830/830 [==============================] - 4s 4ms/step - loss: 5.3874e-04 - mean_squared_error: 5.3874e-04 - mean_absolute_error: 0.0163 - val_loss: 7.8143e-04 - val_mean_squared_error: 7.8143e-04 - val_mean_absolute_error: 0.0223\n",
            "Epoch 55/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3807e-04 - mean_squared_error: 5.3807e-04 - mean_absolute_error: 0.0163 - val_loss: 8.1561e-04 - val_mean_squared_error: 8.1561e-04 - val_mean_absolute_error: 0.0232\n",
            "Epoch 56/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3789e-04 - mean_squared_error: 5.3789e-04 - mean_absolute_error: 0.0163 - val_loss: 8.5758e-04 - val_mean_squared_error: 8.5758e-04 - val_mean_absolute_error: 0.0239\n",
            "Epoch 57/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3757e-04 - mean_squared_error: 5.3757e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0011 - val_mean_squared_error: 0.0011 - val_mean_absolute_error: 0.0270\n",
            "Epoch 58/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3726e-04 - mean_squared_error: 5.3726e-04 - mean_absolute_error: 0.0162 - val_loss: 8.5549e-04 - val_mean_squared_error: 8.5549e-04 - val_mean_absolute_error: 0.0239\n",
            "Epoch 59/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3654e-04 - mean_squared_error: 5.3654e-04 - mean_absolute_error: 0.0162 - val_loss: 7.7665e-04 - val_mean_squared_error: 7.7665e-04 - val_mean_absolute_error: 0.0222\n",
            "Epoch 60/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3547e-04 - mean_squared_error: 5.3547e-04 - mean_absolute_error: 0.0162 - val_loss: 7.8902e-04 - val_mean_squared_error: 7.8902e-04 - val_mean_absolute_error: 0.0225\n",
            "Epoch 61/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3516e-04 - mean_squared_error: 5.3516e-04 - mean_absolute_error: 0.0162 - val_loss: 8.1452e-04 - val_mean_squared_error: 8.1452e-04 - val_mean_absolute_error: 0.0230\n",
            "Epoch 62/100\n",
            "830/830 [==============================] - 4s 4ms/step - loss: 5.3522e-04 - mean_squared_error: 5.3522e-04 - mean_absolute_error: 0.0162 - val_loss: 9.6809e-04 - val_mean_squared_error: 9.6809e-04 - val_mean_absolute_error: 0.0252\n",
            "Epoch 63/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3585e-04 - mean_squared_error: 5.3585e-04 - mean_absolute_error: 0.0162 - val_loss: 7.9555e-04 - val_mean_squared_error: 7.9555e-04 - val_mean_absolute_error: 0.0223\n",
            "Epoch 64/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3475e-04 - mean_squared_error: 5.3475e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0012 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0289\n",
            "Epoch 65/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3426e-04 - mean_squared_error: 5.3426e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0010 - val_mean_squared_error: 0.0010 - val_mean_absolute_error: 0.0272\n",
            "Epoch 66/100\n",
            "830/830 [==============================] - 4s 4ms/step - loss: 5.3431e-04 - mean_squared_error: 5.3431e-04 - mean_absolute_error: 0.0162 - val_loss: 9.5332e-04 - val_mean_squared_error: 9.5332e-04 - val_mean_absolute_error: 0.0249\n",
            "Epoch 67/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3464e-04 - mean_squared_error: 5.3464e-04 - mean_absolute_error: 0.0162 - val_loss: 0.0011 - val_mean_squared_error: 0.0011 - val_mean_absolute_error: 0.0278\n",
            "Epoch 68/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3375e-04 - mean_squared_error: 5.3375e-04 - mean_absolute_error: 0.0162 - val_loss: 9.6329e-04 - val_mean_squared_error: 9.6329e-04 - val_mean_absolute_error: 0.0253\n",
            "Epoch 69/100\n",
            "830/830 [==============================] - 4s 4ms/step - loss: 5.3316e-04 - mean_squared_error: 5.3316e-04 - mean_absolute_error: 0.0162 - val_loss: 8.7986e-04 - val_mean_squared_error: 8.7986e-04 - val_mean_absolute_error: 0.0240\n",
            "Epoch 70/100\n",
            "830/830 [==============================] - 4s 4ms/step - loss: 5.3219e-04 - mean_squared_error: 5.3219e-04 - mean_absolute_error: 0.0162 - val_loss: 7.5444e-04 - val_mean_squared_error: 7.5444e-04 - val_mean_absolute_error: 0.0218\n",
            "Epoch 71/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3255e-04 - mean_squared_error: 5.3255e-04 - mean_absolute_error: 0.0162 - val_loss: 6.6536e-04 - val_mean_squared_error: 6.6536e-04 - val_mean_absolute_error: 0.0201\n",
            "Epoch 72/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3202e-04 - mean_squared_error: 5.3202e-04 - mean_absolute_error: 0.0162 - val_loss: 9.2349e-04 - val_mean_squared_error: 9.2349e-04 - val_mean_absolute_error: 0.0248\n",
            "Epoch 73/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3144e-04 - mean_squared_error: 5.3144e-04 - mean_absolute_error: 0.0161 - val_loss: 6.7340e-04 - val_mean_squared_error: 6.7340e-04 - val_mean_absolute_error: 0.0202\n",
            "Epoch 74/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3157e-04 - mean_squared_error: 5.3157e-04 - mean_absolute_error: 0.0161 - val_loss: 7.7243e-04 - val_mean_squared_error: 7.7243e-04 - val_mean_absolute_error: 0.0223\n",
            "Epoch 75/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3144e-04 - mean_squared_error: 5.3144e-04 - mean_absolute_error: 0.0161 - val_loss: 6.3637e-04 - val_mean_squared_error: 6.3637e-04 - val_mean_absolute_error: 0.0192\n",
            "Epoch 76/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3087e-04 - mean_squared_error: 5.3087e-04 - mean_absolute_error: 0.0161 - val_loss: 6.6396e-04 - val_mean_squared_error: 6.6396e-04 - val_mean_absolute_error: 0.0203\n",
            "Epoch 77/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.3069e-04 - mean_squared_error: 5.3069e-04 - mean_absolute_error: 0.0161 - val_loss: 6.5397e-04 - val_mean_squared_error: 6.5397e-04 - val_mean_absolute_error: 0.0195\n",
            "Epoch 78/100\n",
            "830/830 [==============================] - 4s 4ms/step - loss: 5.3070e-04 - mean_squared_error: 5.3070e-04 - mean_absolute_error: 0.0161 - val_loss: 6.6385e-04 - val_mean_squared_error: 6.6385e-04 - val_mean_absolute_error: 0.0200\n",
            "Epoch 79/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2976e-04 - mean_squared_error: 5.2976e-04 - mean_absolute_error: 0.0161 - val_loss: 7.5799e-04 - val_mean_squared_error: 7.5799e-04 - val_mean_absolute_error: 0.0219\n",
            "Epoch 80/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2952e-04 - mean_squared_error: 5.2952e-04 - mean_absolute_error: 0.0161 - val_loss: 0.0011 - val_mean_squared_error: 0.0011 - val_mean_absolute_error: 0.0282\n",
            "Epoch 81/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2950e-04 - mean_squared_error: 5.2950e-04 - mean_absolute_error: 0.0161 - val_loss: 5.9278e-04 - val_mean_squared_error: 5.9278e-04 - val_mean_absolute_error: 0.0184\n",
            "Epoch 82/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2959e-04 - mean_squared_error: 5.2959e-04 - mean_absolute_error: 0.0161 - val_loss: 6.2095e-04 - val_mean_squared_error: 6.2095e-04 - val_mean_absolute_error: 0.0192\n",
            "Epoch 83/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2894e-04 - mean_squared_error: 5.2894e-04 - mean_absolute_error: 0.0161 - val_loss: 6.9286e-04 - val_mean_squared_error: 6.9286e-04 - val_mean_absolute_error: 0.0210\n",
            "Epoch 84/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2877e-04 - mean_squared_error: 5.2877e-04 - mean_absolute_error: 0.0161 - val_loss: 5.2630e-04 - val_mean_squared_error: 5.2630e-04 - val_mean_absolute_error: 0.0165\n",
            "Epoch 85/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2844e-04 - mean_squared_error: 5.2844e-04 - mean_absolute_error: 0.0161 - val_loss: 8.2508e-04 - val_mean_squared_error: 8.2508e-04 - val_mean_absolute_error: 0.0232\n",
            "Epoch 86/100\n",
            "830/830 [==============================] - 4s 4ms/step - loss: 5.2707e-04 - mean_squared_error: 5.2707e-04 - mean_absolute_error: 0.0161 - val_loss: 6.2660e-04 - val_mean_squared_error: 6.2660e-04 - val_mean_absolute_error: 0.0190\n",
            "Epoch 87/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2797e-04 - mean_squared_error: 5.2797e-04 - mean_absolute_error: 0.0161 - val_loss: 6.4552e-04 - val_mean_squared_error: 6.4552e-04 - val_mean_absolute_error: 0.0195\n",
            "Epoch 88/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2742e-04 - mean_squared_error: 5.2742e-04 - mean_absolute_error: 0.0161 - val_loss: 7.2534e-04 - val_mean_squared_error: 7.2534e-04 - val_mean_absolute_error: 0.0216\n",
            "Epoch 89/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2733e-04 - mean_squared_error: 5.2733e-04 - mean_absolute_error: 0.0161 - val_loss: 7.0996e-04 - val_mean_squared_error: 7.0996e-04 - val_mean_absolute_error: 0.0213\n",
            "Epoch 90/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2644e-04 - mean_squared_error: 5.2644e-04 - mean_absolute_error: 0.0161 - val_loss: 7.4545e-04 - val_mean_squared_error: 7.4545e-04 - val_mean_absolute_error: 0.0220\n",
            "Epoch 91/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2661e-04 - mean_squared_error: 5.2661e-04 - mean_absolute_error: 0.0161 - val_loss: 6.3388e-04 - val_mean_squared_error: 6.3388e-04 - val_mean_absolute_error: 0.0198\n",
            "Epoch 92/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2599e-04 - mean_squared_error: 5.2599e-04 - mean_absolute_error: 0.0161 - val_loss: 5.6508e-04 - val_mean_squared_error: 5.6508e-04 - val_mean_absolute_error: 0.0177\n",
            "Epoch 93/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2665e-04 - mean_squared_error: 5.2665e-04 - mean_absolute_error: 0.0161 - val_loss: 5.5031e-04 - val_mean_squared_error: 5.5031e-04 - val_mean_absolute_error: 0.0175\n",
            "Epoch 94/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2500e-04 - mean_squared_error: 5.2500e-04 - mean_absolute_error: 0.0160 - val_loss: 7.1058e-04 - val_mean_squared_error: 7.1058e-04 - val_mean_absolute_error: 0.0212\n",
            "Epoch 95/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2575e-04 - mean_squared_error: 5.2575e-04 - mean_absolute_error: 0.0161 - val_loss: 6.9664e-04 - val_mean_squared_error: 6.9664e-04 - val_mean_absolute_error: 0.0207\n",
            "Epoch 96/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2495e-04 - mean_squared_error: 5.2495e-04 - mean_absolute_error: 0.0160 - val_loss: 5.3174e-04 - val_mean_squared_error: 5.3174e-04 - val_mean_absolute_error: 0.0169\n",
            "Epoch 97/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2520e-04 - mean_squared_error: 5.2520e-04 - mean_absolute_error: 0.0161 - val_loss: 7.5844e-04 - val_mean_squared_error: 7.5844e-04 - val_mean_absolute_error: 0.0221\n",
            "Epoch 98/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2456e-04 - mean_squared_error: 5.2456e-04 - mean_absolute_error: 0.0160 - val_loss: 5.2376e-04 - val_mean_squared_error: 5.2376e-04 - val_mean_absolute_error: 0.0165\n",
            "Epoch 99/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2414e-04 - mean_squared_error: 5.2414e-04 - mean_absolute_error: 0.0160 - val_loss: 8.2873e-04 - val_mean_squared_error: 8.2873e-04 - val_mean_absolute_error: 0.0234\n",
            "Epoch 100/100\n",
            "830/830 [==============================] - 3s 4ms/step - loss: 5.2425e-04 - mean_squared_error: 5.2425e-04 - mean_absolute_error: 0.0160 - val_loss: 6.2622e-04 - val_mean_squared_error: 6.2622e-04 - val_mean_absolute_error: 0.0189\n",
            "1639/1639 [==============================] - 2s 1ms/step\n",
            "The Top-k (k=[1,2,4,8]) accurancies are: {1: 0.4996280684354079, 2: 0.8223120791928131, 4: 0.9814034217703943, 6: 0.9986267142230445, 8: 0.9999427797592936}\n",
            "true_max_RSRP -116.08219151174858\n",
            "The RSRP difference is: [2.3763783757149635, 1.2572717749925453, 0.5778304693987532, 0.351438148987029, 0.23127464060313604]\n",
            "The naive RSRP difference is: (5.795061489201917, 0.33639779511339146)\n",
            "Is my y test data the same is my predicted data? False\n",
            "true_max_RSRP -116.08219151174858\n",
            "Training model fcJalal3\n",
            " --> inside getModel, parser.modelNameString=fcJalal3\n",
            " --> inside getModel, parser.modelNameString=fcJalal3\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_17 (Flatten)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 1024)              33792     \n",
            "                                                                 \n",
            " batch_normalization_43 (Ba  (None, 1024)              4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_55 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " batch_normalization_44 (Ba  (None, 512)               2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_56 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_45 (Ba  (None, 256)               1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_57 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_46 (Ba  (None, 128)               512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_58 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_47 (Ba  (None, 64)                256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_59 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 741088 (2.83 MB)\n",
            "Trainable params: 737120 (2.81 MB)\n",
            "Non-trainable params: 3968 (15.50 KB)\n",
            "_________________________________________________________________\n",
            "Model Summary None\n",
            "Epoch 1/100\n",
            "830/830 [==============================] - 9s 7ms/step - loss: 0.1032 - mean_squared_error: 0.1032 - mean_absolute_error: 0.1913 - val_loss: 0.0022 - val_mean_squared_error: 0.0022 - val_mean_absolute_error: 0.0372\n",
            "Epoch 2/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - mean_absolute_error: 0.0544 - val_loss: 0.0012 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0262\n",
            "Epoch 3/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - mean_absolute_error: 0.0416 - val_loss: 0.0011 - val_mean_squared_error: 0.0011 - val_mean_absolute_error: 0.0243\n",
            "Epoch 4/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - mean_absolute_error: 0.0350 - val_loss: 8.3498e-04 - val_mean_squared_error: 8.3498e-04 - val_mean_absolute_error: 0.0215\n",
            "Epoch 5/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - mean_absolute_error: 0.0305 - val_loss: 8.1082e-04 - val_mean_squared_error: 8.1082e-04 - val_mean_absolute_error: 0.0214\n",
            "Epoch 6/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - mean_absolute_error: 0.0270 - val_loss: 7.8479e-04 - val_mean_squared_error: 7.8479e-04 - val_mean_absolute_error: 0.0209\n",
            "Epoch 7/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - mean_absolute_error: 0.0244 - val_loss: 7.9788e-04 - val_mean_squared_error: 7.9788e-04 - val_mean_absolute_error: 0.0216\n",
            "Epoch 8/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 9.1909e-04 - mean_squared_error: 9.1909e-04 - mean_absolute_error: 0.0227 - val_loss: 7.5915e-04 - val_mean_squared_error: 7.5915e-04 - val_mean_absolute_error: 0.0211\n",
            "Epoch 9/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 8.5298e-04 - mean_squared_error: 8.5298e-04 - mean_absolute_error: 0.0218 - val_loss: 7.5444e-04 - val_mean_squared_error: 7.5444e-04 - val_mean_absolute_error: 0.0212\n",
            "Epoch 10/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 8.1603e-04 - mean_squared_error: 8.1603e-04 - mean_absolute_error: 0.0212 - val_loss: 7.2054e-04 - val_mean_squared_error: 7.2054e-04 - val_mean_absolute_error: 0.0206\n",
            "Epoch 11/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 7.8639e-04 - mean_squared_error: 7.8639e-04 - mean_absolute_error: 0.0208 - val_loss: 5.8366e-04 - val_mean_squared_error: 5.8366e-04 - val_mean_absolute_error: 0.0173\n",
            "Epoch 12/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 7.6164e-04 - mean_squared_error: 7.6164e-04 - mean_absolute_error: 0.0204 - val_loss: 0.0011 - val_mean_squared_error: 0.0011 - val_mean_absolute_error: 0.0275\n",
            "Epoch 13/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 7.3821e-04 - mean_squared_error: 7.3821e-04 - mean_absolute_error: 0.0201 - val_loss: 0.0013 - val_mean_squared_error: 0.0013 - val_mean_absolute_error: 0.0294\n",
            "Epoch 14/100\n",
            "830/830 [==============================] - 5s 6ms/step - loss: 7.1656e-04 - mean_squared_error: 7.1656e-04 - mean_absolute_error: 0.0198 - val_loss: 8.7248e-04 - val_mean_squared_error: 8.7248e-04 - val_mean_absolute_error: 0.0232\n",
            "Epoch 15/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 6.9716e-04 - mean_squared_error: 6.9716e-04 - mean_absolute_error: 0.0194 - val_loss: 9.2510e-04 - val_mean_squared_error: 9.2510e-04 - val_mean_absolute_error: 0.0246\n",
            "Epoch 16/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 6.8092e-04 - mean_squared_error: 6.8092e-04 - mean_absolute_error: 0.0192 - val_loss: 9.3798e-04 - val_mean_squared_error: 9.3798e-04 - val_mean_absolute_error: 0.0245\n",
            "Epoch 17/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 6.6768e-04 - mean_squared_error: 6.6768e-04 - mean_absolute_error: 0.0190 - val_loss: 7.4970e-04 - val_mean_squared_error: 7.4970e-04 - val_mean_absolute_error: 0.0205\n",
            "Epoch 18/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 6.5631e-04 - mean_squared_error: 6.5631e-04 - mean_absolute_error: 0.0188 - val_loss: 0.0011 - val_mean_squared_error: 0.0011 - val_mean_absolute_error: 0.0269\n",
            "Epoch 19/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 6.4825e-04 - mean_squared_error: 6.4825e-04 - mean_absolute_error: 0.0187 - val_loss: 6.1257e-04 - val_mean_squared_error: 6.1257e-04 - val_mean_absolute_error: 0.0180\n",
            "Epoch 20/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 6.4089e-04 - mean_squared_error: 6.4089e-04 - mean_absolute_error: 0.0185 - val_loss: 8.3040e-04 - val_mean_squared_error: 8.3040e-04 - val_mean_absolute_error: 0.0214\n",
            "Epoch 21/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 6.3340e-04 - mean_squared_error: 6.3340e-04 - mean_absolute_error: 0.0184 - val_loss: 7.0428e-04 - val_mean_squared_error: 7.0428e-04 - val_mean_absolute_error: 0.0195\n",
            "Epoch 22/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 6.2849e-04 - mean_squared_error: 6.2849e-04 - mean_absolute_error: 0.0183 - val_loss: 5.6699e-04 - val_mean_squared_error: 5.6699e-04 - val_mean_absolute_error: 0.0173\n",
            "Epoch 23/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 6.2400e-04 - mean_squared_error: 6.2400e-04 - mean_absolute_error: 0.0182 - val_loss: 7.3755e-04 - val_mean_squared_error: 7.3755e-04 - val_mean_absolute_error: 0.0208\n",
            "Epoch 24/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 6.1996e-04 - mean_squared_error: 6.1996e-04 - mean_absolute_error: 0.0182 - val_loss: 6.0447e-04 - val_mean_squared_error: 6.0447e-04 - val_mean_absolute_error: 0.0182\n",
            "Epoch 25/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 6.1444e-04 - mean_squared_error: 6.1444e-04 - mean_absolute_error: 0.0181 - val_loss: 7.5634e-04 - val_mean_squared_error: 7.5634e-04 - val_mean_absolute_error: 0.0217\n",
            "Epoch 26/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 6.1355e-04 - mean_squared_error: 6.1355e-04 - mean_absolute_error: 0.0181 - val_loss: 5.2610e-04 - val_mean_squared_error: 5.2610e-04 - val_mean_absolute_error: 0.0164\n",
            "Epoch 27/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 6.1039e-04 - mean_squared_error: 6.1039e-04 - mean_absolute_error: 0.0180 - val_loss: 8.4952e-04 - val_mean_squared_error: 8.4952e-04 - val_mean_absolute_error: 0.0240\n",
            "Epoch 28/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 6.0653e-04 - mean_squared_error: 6.0653e-04 - mean_absolute_error: 0.0180 - val_loss: 0.0011 - val_mean_squared_error: 0.0011 - val_mean_absolute_error: 0.0284\n",
            "Epoch 29/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 6.0339e-04 - mean_squared_error: 6.0339e-04 - mean_absolute_error: 0.0179 - val_loss: 5.1274e-04 - val_mean_squared_error: 5.1274e-04 - val_mean_absolute_error: 0.0167\n",
            "Epoch 30/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 6.0023e-04 - mean_squared_error: 6.0023e-04 - mean_absolute_error: 0.0179 - val_loss: 7.3643e-04 - val_mean_squared_error: 7.3643e-04 - val_mean_absolute_error: 0.0212\n",
            "Epoch 31/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.9859e-04 - mean_squared_error: 5.9859e-04 - mean_absolute_error: 0.0178 - val_loss: 9.9527e-04 - val_mean_squared_error: 9.9527e-04 - val_mean_absolute_error: 0.0257\n",
            "Epoch 32/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.9718e-04 - mean_squared_error: 5.9718e-04 - mean_absolute_error: 0.0178 - val_loss: 5.6785e-04 - val_mean_squared_error: 5.6785e-04 - val_mean_absolute_error: 0.0175\n",
            "Epoch 33/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.9396e-04 - mean_squared_error: 5.9396e-04 - mean_absolute_error: 0.0178 - val_loss: 0.0010 - val_mean_squared_error: 0.0010 - val_mean_absolute_error: 0.0264\n",
            "Epoch 34/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.9250e-04 - mean_squared_error: 5.9250e-04 - mean_absolute_error: 0.0177 - val_loss: 6.0984e-04 - val_mean_squared_error: 6.0984e-04 - val_mean_absolute_error: 0.0188\n",
            "Epoch 35/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.9117e-04 - mean_squared_error: 5.9117e-04 - mean_absolute_error: 0.0177 - val_loss: 4.5083e-04 - val_mean_squared_error: 4.5083e-04 - val_mean_absolute_error: 0.0150\n",
            "Epoch 36/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.8939e-04 - mean_squared_error: 5.8939e-04 - mean_absolute_error: 0.0177 - val_loss: 4.7161e-04 - val_mean_squared_error: 4.7161e-04 - val_mean_absolute_error: 0.0154\n",
            "Epoch 37/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.8623e-04 - mean_squared_error: 5.8623e-04 - mean_absolute_error: 0.0176 - val_loss: 5.3364e-04 - val_mean_squared_error: 5.3364e-04 - val_mean_absolute_error: 0.0165\n",
            "Epoch 38/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.8663e-04 - mean_squared_error: 5.8663e-04 - mean_absolute_error: 0.0176 - val_loss: 5.1735e-04 - val_mean_squared_error: 5.1735e-04 - val_mean_absolute_error: 0.0170\n",
            "Epoch 39/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.8403e-04 - mean_squared_error: 5.8403e-04 - mean_absolute_error: 0.0176 - val_loss: 9.1754e-04 - val_mean_squared_error: 9.1754e-04 - val_mean_absolute_error: 0.0250\n",
            "Epoch 40/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.8315e-04 - mean_squared_error: 5.8315e-04 - mean_absolute_error: 0.0176 - val_loss: 6.3712e-04 - val_mean_squared_error: 6.3712e-04 - val_mean_absolute_error: 0.0192\n",
            "Epoch 41/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.8226e-04 - mean_squared_error: 5.8226e-04 - mean_absolute_error: 0.0176 - val_loss: 4.9388e-04 - val_mean_squared_error: 4.9388e-04 - val_mean_absolute_error: 0.0158\n",
            "Epoch 42/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.8211e-04 - mean_squared_error: 5.8211e-04 - mean_absolute_error: 0.0176 - val_loss: 4.9147e-04 - val_mean_squared_error: 4.9147e-04 - val_mean_absolute_error: 0.0159\n",
            "Epoch 43/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.7995e-04 - mean_squared_error: 5.7995e-04 - mean_absolute_error: 0.0175 - val_loss: 4.9380e-04 - val_mean_squared_error: 4.9380e-04 - val_mean_absolute_error: 0.0156\n",
            "Epoch 44/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.7899e-04 - mean_squared_error: 5.7899e-04 - mean_absolute_error: 0.0175 - val_loss: 4.3677e-04 - val_mean_squared_error: 4.3677e-04 - val_mean_absolute_error: 0.0144\n",
            "Epoch 45/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.7870e-04 - mean_squared_error: 5.7870e-04 - mean_absolute_error: 0.0175 - val_loss: 4.8200e-04 - val_mean_squared_error: 4.8200e-04 - val_mean_absolute_error: 0.0156\n",
            "Epoch 46/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.7703e-04 - mean_squared_error: 5.7703e-04 - mean_absolute_error: 0.0175 - val_loss: 5.7801e-04 - val_mean_squared_error: 5.7801e-04 - val_mean_absolute_error: 0.0184\n",
            "Epoch 47/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.7534e-04 - mean_squared_error: 5.7534e-04 - mean_absolute_error: 0.0175 - val_loss: 6.7437e-04 - val_mean_squared_error: 6.7437e-04 - val_mean_absolute_error: 0.0203\n",
            "Epoch 48/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.7585e-04 - mean_squared_error: 5.7585e-04 - mean_absolute_error: 0.0175 - val_loss: 5.1094e-04 - val_mean_squared_error: 5.1094e-04 - val_mean_absolute_error: 0.0166\n",
            "Epoch 49/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.7498e-04 - mean_squared_error: 5.7498e-04 - mean_absolute_error: 0.0175 - val_loss: 5.9350e-04 - val_mean_squared_error: 5.9350e-04 - val_mean_absolute_error: 0.0182\n",
            "Epoch 50/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.7408e-04 - mean_squared_error: 5.7408e-04 - mean_absolute_error: 0.0175 - val_loss: 5.0522e-04 - val_mean_squared_error: 5.0522e-04 - val_mean_absolute_error: 0.0160\n",
            "Epoch 51/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.7235e-04 - mean_squared_error: 5.7235e-04 - mean_absolute_error: 0.0174 - val_loss: 5.5228e-04 - val_mean_squared_error: 5.5228e-04 - val_mean_absolute_error: 0.0170\n",
            "Epoch 52/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.7172e-04 - mean_squared_error: 5.7172e-04 - mean_absolute_error: 0.0174 - val_loss: 4.4642e-04 - val_mean_squared_error: 4.4642e-04 - val_mean_absolute_error: 0.0150\n",
            "Epoch 53/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.7082e-04 - mean_squared_error: 5.7082e-04 - mean_absolute_error: 0.0174 - val_loss: 4.3487e-04 - val_mean_squared_error: 4.3487e-04 - val_mean_absolute_error: 0.0146\n",
            "Epoch 54/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.7093e-04 - mean_squared_error: 5.7093e-04 - mean_absolute_error: 0.0174 - val_loss: 4.8190e-04 - val_mean_squared_error: 4.8190e-04 - val_mean_absolute_error: 0.0155\n",
            "Epoch 55/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.7129e-04 - mean_squared_error: 5.7129e-04 - mean_absolute_error: 0.0174 - val_loss: 5.2907e-04 - val_mean_squared_error: 5.2907e-04 - val_mean_absolute_error: 0.0174\n",
            "Epoch 56/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6959e-04 - mean_squared_error: 5.6959e-04 - mean_absolute_error: 0.0174 - val_loss: 4.7342e-04 - val_mean_squared_error: 4.7342e-04 - val_mean_absolute_error: 0.0153\n",
            "Epoch 57/100\n",
            "830/830 [==============================] - 5s 6ms/step - loss: 5.6957e-04 - mean_squared_error: 5.6957e-04 - mean_absolute_error: 0.0174 - val_loss: 5.7730e-04 - val_mean_squared_error: 5.7730e-04 - val_mean_absolute_error: 0.0180\n",
            "Epoch 58/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6926e-04 - mean_squared_error: 5.6926e-04 - mean_absolute_error: 0.0174 - val_loss: 5.7969e-04 - val_mean_squared_error: 5.7969e-04 - val_mean_absolute_error: 0.0187\n",
            "Epoch 59/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6944e-04 - mean_squared_error: 5.6944e-04 - mean_absolute_error: 0.0174 - val_loss: 4.7000e-04 - val_mean_squared_error: 4.7000e-04 - val_mean_absolute_error: 0.0154\n",
            "Epoch 60/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6896e-04 - mean_squared_error: 5.6896e-04 - mean_absolute_error: 0.0174 - val_loss: 4.3985e-04 - val_mean_squared_error: 4.3985e-04 - val_mean_absolute_error: 0.0145\n",
            "Epoch 61/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6736e-04 - mean_squared_error: 5.6736e-04 - mean_absolute_error: 0.0173 - val_loss: 5.1638e-04 - val_mean_squared_error: 5.1638e-04 - val_mean_absolute_error: 0.0166\n",
            "Epoch 62/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6733e-04 - mean_squared_error: 5.6733e-04 - mean_absolute_error: 0.0173 - val_loss: 4.4293e-04 - val_mean_squared_error: 4.4293e-04 - val_mean_absolute_error: 0.0147\n",
            "Epoch 63/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6694e-04 - mean_squared_error: 5.6694e-04 - mean_absolute_error: 0.0173 - val_loss: 4.6043e-04 - val_mean_squared_error: 4.6043e-04 - val_mean_absolute_error: 0.0151\n",
            "Epoch 64/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6610e-04 - mean_squared_error: 5.6610e-04 - mean_absolute_error: 0.0173 - val_loss: 5.0640e-04 - val_mean_squared_error: 5.0640e-04 - val_mean_absolute_error: 0.0160\n",
            "Epoch 65/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6622e-04 - mean_squared_error: 5.6622e-04 - mean_absolute_error: 0.0173 - val_loss: 4.3431e-04 - val_mean_squared_error: 4.3431e-04 - val_mean_absolute_error: 0.0145\n",
            "Epoch 66/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.6559e-04 - mean_squared_error: 5.6559e-04 - mean_absolute_error: 0.0173 - val_loss: 4.8619e-04 - val_mean_squared_error: 4.8619e-04 - val_mean_absolute_error: 0.0157\n",
            "Epoch 67/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6452e-04 - mean_squared_error: 5.6452e-04 - mean_absolute_error: 0.0173 - val_loss: 4.2192e-04 - val_mean_squared_error: 4.2192e-04 - val_mean_absolute_error: 0.0139\n",
            "Epoch 68/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6417e-04 - mean_squared_error: 5.6417e-04 - mean_absolute_error: 0.0173 - val_loss: 4.4240e-04 - val_mean_squared_error: 4.4240e-04 - val_mean_absolute_error: 0.0145\n",
            "Epoch 69/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6405e-04 - mean_squared_error: 5.6405e-04 - mean_absolute_error: 0.0173 - val_loss: 4.5847e-04 - val_mean_squared_error: 4.5847e-04 - val_mean_absolute_error: 0.0150\n",
            "Epoch 70/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6388e-04 - mean_squared_error: 5.6388e-04 - mean_absolute_error: 0.0173 - val_loss: 5.0535e-04 - val_mean_squared_error: 5.0535e-04 - val_mean_absolute_error: 0.0168\n",
            "Epoch 71/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6365e-04 - mean_squared_error: 5.6365e-04 - mean_absolute_error: 0.0173 - val_loss: 4.1357e-04 - val_mean_squared_error: 4.1357e-04 - val_mean_absolute_error: 0.0140\n",
            "Epoch 72/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6320e-04 - mean_squared_error: 5.6320e-04 - mean_absolute_error: 0.0173 - val_loss: 4.3556e-04 - val_mean_squared_error: 4.3556e-04 - val_mean_absolute_error: 0.0145\n",
            "Epoch 73/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6226e-04 - mean_squared_error: 5.6226e-04 - mean_absolute_error: 0.0173 - val_loss: 4.0604e-04 - val_mean_squared_error: 4.0604e-04 - val_mean_absolute_error: 0.0137\n",
            "Epoch 74/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6201e-04 - mean_squared_error: 5.6201e-04 - mean_absolute_error: 0.0173 - val_loss: 4.3644e-04 - val_mean_squared_error: 4.3644e-04 - val_mean_absolute_error: 0.0144\n",
            "Epoch 75/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6223e-04 - mean_squared_error: 5.6223e-04 - mean_absolute_error: 0.0173 - val_loss: 4.4882e-04 - val_mean_squared_error: 4.4882e-04 - val_mean_absolute_error: 0.0151\n",
            "Epoch 76/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.6103e-04 - mean_squared_error: 5.6103e-04 - mean_absolute_error: 0.0173 - val_loss: 4.4629e-04 - val_mean_squared_error: 4.4629e-04 - val_mean_absolute_error: 0.0149\n",
            "Epoch 77/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6120e-04 - mean_squared_error: 5.6120e-04 - mean_absolute_error: 0.0173 - val_loss: 4.5866e-04 - val_mean_squared_error: 4.5866e-04 - val_mean_absolute_error: 0.0148\n",
            "Epoch 78/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.6100e-04 - mean_squared_error: 5.6100e-04 - mean_absolute_error: 0.0173 - val_loss: 4.8880e-04 - val_mean_squared_error: 4.8880e-04 - val_mean_absolute_error: 0.0155\n",
            "Epoch 79/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6169e-04 - mean_squared_error: 5.6169e-04 - mean_absolute_error: 0.0173 - val_loss: 5.2777e-04 - val_mean_squared_error: 5.2777e-04 - val_mean_absolute_error: 0.0165\n",
            "Epoch 80/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6065e-04 - mean_squared_error: 5.6065e-04 - mean_absolute_error: 0.0172 - val_loss: 4.6168e-04 - val_mean_squared_error: 4.6168e-04 - val_mean_absolute_error: 0.0151\n",
            "Epoch 81/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.6071e-04 - mean_squared_error: 5.6071e-04 - mean_absolute_error: 0.0172 - val_loss: 4.8444e-04 - val_mean_squared_error: 4.8444e-04 - val_mean_absolute_error: 0.0162\n",
            "Epoch 82/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5958e-04 - mean_squared_error: 5.5958e-04 - mean_absolute_error: 0.0172 - val_loss: 4.5144e-04 - val_mean_squared_error: 4.5144e-04 - val_mean_absolute_error: 0.0149\n",
            "Epoch 83/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.6025e-04 - mean_squared_error: 5.6025e-04 - mean_absolute_error: 0.0172 - val_loss: 5.0863e-04 - val_mean_squared_error: 5.0863e-04 - val_mean_absolute_error: 0.0159\n",
            "Epoch 84/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5913e-04 - mean_squared_error: 5.5913e-04 - mean_absolute_error: 0.0172 - val_loss: 4.5742e-04 - val_mean_squared_error: 4.5742e-04 - val_mean_absolute_error: 0.0150\n",
            "Epoch 85/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5895e-04 - mean_squared_error: 5.5895e-04 - mean_absolute_error: 0.0172 - val_loss: 5.7106e-04 - val_mean_squared_error: 5.7106e-04 - val_mean_absolute_error: 0.0176\n",
            "Epoch 86/100\n",
            "830/830 [==============================] - 5s 6ms/step - loss: 5.5980e-04 - mean_squared_error: 5.5980e-04 - mean_absolute_error: 0.0172 - val_loss: 5.1118e-04 - val_mean_squared_error: 5.1118e-04 - val_mean_absolute_error: 0.0161\n",
            "Epoch 87/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5733e-04 - mean_squared_error: 5.5733e-04 - mean_absolute_error: 0.0172 - val_loss: 4.4153e-04 - val_mean_squared_error: 4.4153e-04 - val_mean_absolute_error: 0.0148\n",
            "Epoch 88/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.5870e-04 - mean_squared_error: 5.5870e-04 - mean_absolute_error: 0.0172 - val_loss: 5.3559e-04 - val_mean_squared_error: 5.3559e-04 - val_mean_absolute_error: 0.0172\n",
            "Epoch 89/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5851e-04 - mean_squared_error: 5.5851e-04 - mean_absolute_error: 0.0172 - val_loss: 5.7223e-04 - val_mean_squared_error: 5.7223e-04 - val_mean_absolute_error: 0.0173\n",
            "Epoch 90/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5723e-04 - mean_squared_error: 5.5723e-04 - mean_absolute_error: 0.0172 - val_loss: 4.1888e-04 - val_mean_squared_error: 4.1888e-04 - val_mean_absolute_error: 0.0140\n",
            "Epoch 91/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5850e-04 - mean_squared_error: 5.5850e-04 - mean_absolute_error: 0.0172 - val_loss: 5.1874e-04 - val_mean_squared_error: 5.1874e-04 - val_mean_absolute_error: 0.0162\n",
            "Epoch 92/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5836e-04 - mean_squared_error: 5.5836e-04 - mean_absolute_error: 0.0172 - val_loss: 5.5825e-04 - val_mean_squared_error: 5.5825e-04 - val_mean_absolute_error: 0.0174\n",
            "Epoch 93/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.5739e-04 - mean_squared_error: 5.5739e-04 - mean_absolute_error: 0.0172 - val_loss: 4.3939e-04 - val_mean_squared_error: 4.3939e-04 - val_mean_absolute_error: 0.0146\n",
            "Epoch 94/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5798e-04 - mean_squared_error: 5.5798e-04 - mean_absolute_error: 0.0172 - val_loss: 4.8561e-04 - val_mean_squared_error: 4.8561e-04 - val_mean_absolute_error: 0.0157\n",
            "Epoch 95/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5552e-04 - mean_squared_error: 5.5552e-04 - mean_absolute_error: 0.0172 - val_loss: 4.4702e-04 - val_mean_squared_error: 4.4702e-04 - val_mean_absolute_error: 0.0146\n",
            "Epoch 96/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5595e-04 - mean_squared_error: 5.5595e-04 - mean_absolute_error: 0.0172 - val_loss: 3.9637e-04 - val_mean_squared_error: 3.9637e-04 - val_mean_absolute_error: 0.0135\n",
            "Epoch 97/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5546e-04 - mean_squared_error: 5.5546e-04 - mean_absolute_error: 0.0172 - val_loss: 4.3036e-04 - val_mean_squared_error: 4.3036e-04 - val_mean_absolute_error: 0.0145\n",
            "Epoch 98/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 5.5504e-04 - mean_squared_error: 5.5504e-04 - mean_absolute_error: 0.0172 - val_loss: 4.5270e-04 - val_mean_squared_error: 4.5270e-04 - val_mean_absolute_error: 0.0150\n",
            "Epoch 99/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 5.5600e-04 - mean_squared_error: 5.5600e-04 - mean_absolute_error: 0.0172 - val_loss: 5.2557e-04 - val_mean_squared_error: 5.2557e-04 - val_mean_absolute_error: 0.0163\n",
            "Epoch 100/100\n",
            "830/830 [==============================] - 5s 6ms/step - loss: 5.5524e-04 - mean_squared_error: 5.5524e-04 - mean_absolute_error: 0.0172 - val_loss: 4.6057e-04 - val_mean_squared_error: 4.6057e-04 - val_mean_absolute_error: 0.0156\n",
            "1639/1639 [==============================] - 3s 2ms/step\n",
            "The Top-k (k=[1,2,4,8]) accurancies are: {1: 0.5642678670201606, 2: 0.8471456636594251, 4: 0.9845505350092506, 6: 0.9988746685994393, 8: 0.9999427797592936}\n",
            "true_max_RSRP -116.08219151174858\n",
            "The RSRP difference is: [2.0263944034570542, 1.0896223601911532, 0.5130566624462805, 0.3108376400960854, 0.2123295677350835]\n",
            "The naive RSRP difference is: (5.795061489201917, 0.33639779511339146)\n",
            "Is my y test data the same is my predicted data? False\n",
            "true_max_RSRP -116.08219151174858\n",
            "Training model fcJalal4\n",
            " --> inside getModel, parser.modelNameString=fcJalal4\n",
            " --> inside getModel, parser.modelNameString=fcJalal4\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_19 (Flatten)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 2048)              67584     \n",
            "                                                                 \n",
            " batch_normalization_53 (Ba  (None, 2048)              8192      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_65 (Dropout)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 1536)              3147264   \n",
            "                                                                 \n",
            " batch_normalization_54 (Ba  (None, 1536)              6144      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_66 (Dropout)        (None, 1536)              0         \n",
            "                                                                 \n",
            " dense_86 (Dense)            (None, 1024)              1573888   \n",
            "                                                                 \n",
            " batch_normalization_55 (Ba  (None, 1024)              4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_67 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_87 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " batch_normalization_56 (Ba  (None, 512)               2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_68 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_88 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_57 (Ba  (None, 256)               1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_69 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_89 (Dense)            (None, 32)                8224      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5474592 (20.88 MB)\n",
            "Trainable params: 5463840 (20.84 MB)\n",
            "Non-trainable params: 10752 (42.00 KB)\n",
            "_________________________________________________________________\n",
            "Model Summary None\n",
            "Epoch 1/100\n",
            "830/830 [==============================] - 12s 10ms/step - loss: 0.3903 - mean_squared_error: 0.3903 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 2/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3900 - mean_squared_error: 0.3900 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 3/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 4/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 5/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 6/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 7/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 8/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 9/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 10/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 11/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 12/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 13/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 14/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 15/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 16/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 17/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 18/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 19/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 20/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 21/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 22/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 23/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 24/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 25/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 26/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 27/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 28/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 29/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 30/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 31/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 32/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 33/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 34/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 35/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 36/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 37/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 38/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 39/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 40/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 41/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 42/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 43/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 44/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 45/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 46/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 47/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 48/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 49/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 50/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 51/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 52/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 53/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 54/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 55/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 56/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 57/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 58/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 59/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 60/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 61/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 62/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 63/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 64/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 65/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 66/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 67/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 68/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 69/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 70/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 71/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 72/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 73/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 74/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 75/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 76/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 77/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 78/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 79/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 80/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 81/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 82/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 83/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 84/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 85/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 86/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 87/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 88/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 89/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 90/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 91/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 92/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 93/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 94/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 95/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 96/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 97/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 98/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 99/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 100/100\n",
            "830/830 [==============================] - 8s 10ms/step - loss: 0.3898 - mean_squared_error: 0.3898 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "1639/1639 [==============================] - 3s 2ms/step\n",
            "The Top-k (k=[1,2,4,8]) accurancies are: {1: 0.5602243033435694, 2: 0.8284537183619752, 4: 0.9789238780064469, 6: 0.9979400713345667, 8: 0.9998664861050183}\n",
            "true_max_RSRP -116.08219151174858\n",
            "The RSRP difference is: [3.488661781507603, 2.0081393927955125, 1.0805177271575457, 0.7392904346444983, 0.5747432630216411]\n",
            "The naive RSRP difference is: (5.795061489201917, 0.33639779511339146)\n",
            "Is my y test data the same is my predicted data? False\n",
            "true_max_RSRP -116.08219151174858\n",
            "Training model fcJalal5\n",
            " --> inside getModel, parser.modelNameString=fcJalal5\n",
            " --> inside getModel, parser.modelNameString=fcJalal5\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_21 (Flatten)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_96 (Dense)            (None, 1024)              33792     \n",
            "                                                                 \n",
            " batch_normalization_63 (Ba  (None, 1024)              4096      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   multiple                  0         \n",
            "                                                                 \n",
            " dropout_75 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_97 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " batch_normalization_64 (Ba  (None, 512)               2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_76 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_98 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_65 (Ba  (None, 256)               1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_77 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_99 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_66 (Ba  (None, 128)               512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_78 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_100 (Dense)           (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_67 (Ba  (None, 64)                256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_79 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_101 (Dense)           (None, 32)                2080      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 741088 (2.83 MB)\n",
            "Trainable params: 737120 (2.81 MB)\n",
            "Non-trainable params: 3968 (15.50 KB)\n",
            "_________________________________________________________________\n",
            "Model Summary None\n",
            "Epoch 1/100\n",
            "830/830 [==============================] - 9s 7ms/step - loss: 0.3902 - mean_squared_error: 0.3902 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 2/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3900 - mean_squared_error: 0.3900 - mean_absolute_error: 0.6181 - val_loss: 0.3906 - val_mean_squared_error: 0.3906 - val_mean_absolute_error: 0.6184\n",
            "Epoch 3/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3906 - val_mean_squared_error: 0.3906 - val_mean_absolute_error: 0.6184\n",
            "Epoch 4/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3906 - val_mean_squared_error: 0.3906 - val_mean_absolute_error: 0.6184\n",
            "Epoch 5/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3906 - val_mean_squared_error: 0.3906 - val_mean_absolute_error: 0.6184\n",
            "Epoch 6/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3906 - val_mean_squared_error: 0.3906 - val_mean_absolute_error: 0.6184\n",
            "Epoch 7/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3906 - val_mean_squared_error: 0.3906 - val_mean_absolute_error: 0.6184\n",
            "Epoch 8/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 9/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 10/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3906 - val_mean_squared_error: 0.3906 - val_mean_absolute_error: 0.6184\n",
            "Epoch 11/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 12/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 13/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 14/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 15/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 16/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 17/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 18/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 19/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 20/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 21/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 22/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 23/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 24/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 25/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 26/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 27/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 28/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 29/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 30/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 31/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 32/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 33/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 34/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 35/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 36/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 37/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 38/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 39/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 40/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 41/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 42/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 43/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 44/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 45/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3905 - val_mean_squared_error: 0.3905 - val_mean_absolute_error: 0.6184\n",
            "Epoch 46/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 47/100\n",
            "830/830 [==============================] - 6s 8ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 48/100\n",
            "830/830 [==============================] - 6s 8ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 49/100\n",
            "830/830 [==============================] - 6s 8ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 50/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 51/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 52/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 53/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 54/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 55/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 56/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 57/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 58/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 59/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 60/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 61/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 62/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 63/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 64/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 65/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 66/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 67/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 68/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 69/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 70/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 71/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 72/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 73/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 74/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 75/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 76/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 77/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 78/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 79/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 80/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 81/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 82/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 83/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 84/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 85/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 86/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 87/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 88/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 89/100\n",
            "830/830 [==============================] - 5s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 90/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 91/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 92/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 93/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3903 - val_mean_squared_error: 0.3903 - val_mean_absolute_error: 0.6184\n",
            "Epoch 94/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 95/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 96/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 97/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 98/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 99/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "Epoch 100/100\n",
            "830/830 [==============================] - 6s 7ms/step - loss: 0.3899 - mean_squared_error: 0.3899 - mean_absolute_error: 0.6181 - val_loss: 0.3904 - val_mean_squared_error: 0.3904 - val_mean_absolute_error: 0.6184\n",
            "1639/1639 [==============================] - 3s 2ms/step\n",
            "The Top-k (k=[1,2,4,8]) accurancies are: {1: 0.5430010108909191, 2: 0.8369986076408095, 4: 0.9793053462778234, 6: 0.9980354384024109, 8: 0.999885559518587}\n",
            "true_max_RSRP -116.08219151174858\n",
            "The RSRP difference is: [3.3593191556556636, 1.995391921607534, 1.139957071691458, 0.7853008799128851, 0.5907308741001858]\n",
            "The naive RSRP difference is: (5.795061489201917, 0.33639779511339146)\n",
            "Is my y test data the same is my predicted data? False\n",
            "true_max_RSRP -116.08219151174858\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "\n",
        "batchSize = 512\n",
        "chkpointFname = '/content/drive/MyDrive/regression_problem/chkpoint'\n",
        "\n",
        "trainDatasetTFint = tf.data.Dataset.from_tensor_slices((xTrain, yTrain))\n",
        "valDatasetTFint = tf.data.Dataset.from_tensor_slices((xVal, yVal))\n",
        "\n",
        "trainDatasetTF = trainDatasetTFint.batch(batchSize)\n",
        "valDatasetTF = valDatasetTFint.batch(batchSize)\n",
        "yTest = yTest.reshape(len(yTest), 32)\n",
        "yTest_descaled = scaler_y_org.inverse_transform(yTest)\n",
        "\n",
        "def train_and_save_history(model, train_dataset, val_dataset, epochs, batch_size, checkpoint_filename, history_filename, xTest, yTest):\n",
        "    # Current date and time for file naming\n",
        "    now = datetime.now().strftime(\"%Y%m%d_h%Hm%Ms%S\")\n",
        "\n",
        "    # Update the checkpoint filename to include date, time and epoch size\n",
        "    checkpoint_filename_with_time = f\"{checkpoint_filename}_epoch{epochs}_{now}\"\n",
        "\n",
        "    # Set up the checkpoint callback\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filename_with_time,\n",
        "        save_weights_only=False, verbose=2,  # or True, depending on your needs\n",
        "        monitor='val_accuracy', mode='max',\n",
        "        save_best_only=True)\n",
        "\n",
        "    model = _getModel()\n",
        "    print(\"Model Summary\", model.summary())\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                  loss=tf.keras.losses.MeanSquaredError(),\n",
        "                  metrics=[tf.keras.metrics.MeanSquaredError(),\n",
        "                           tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "    # Train the model\n",
        "    # history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset,\n",
        "    #                     batch_size=batch_size, callbacks=[model_checkpoint_callback])\n",
        "    history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset,\n",
        "                        batch_size=batch_size)\n",
        "\n",
        "\n",
        "    yhat = model.predict(xTest)\n",
        "    yhat_descaled = scaler_raw_org.inverse_transform(yhat)\n",
        "\n",
        "    k_values = [1, 2, 4, 6, 8]  # The different top-k values to compute\n",
        "    print(\"The Top-k (k=[1,2,4,8]) accurancies are:\", CustomKPI().top_k_accuracies(yhat, yTest, k_values))\n",
        "    print(\"The RSRP difference is:\",CustomKPI()._rsrpDiff(yhat_descaled, yTest_descaled))\n",
        "    print(\"The naive RSRP difference is:\",CustomKPI()._rsrpDiffNaiveBaseline(yTest_descaled, 'preset'))\n",
        "    print(\"Is my y test data the same is my predicted data?\",np.all(yTest == yhat))\n",
        "\n",
        "    # Prediction and performance calculations\n",
        "    top_k_accuracies = CustomKPI().top_k_accuracies(yhat, yTest, k_values)\n",
        "    rsrp_diff = CustomKPI()._rsrpDiff(yhat_descaled, yTest_descaled)\n",
        "    naive_rsrp_diff = CustomKPI()._rsrpDiffNaiveBaseline(yTest_descaled, 'fixed')\n",
        "    is_data_same = np.all(yTest == yhat)\n",
        "\n",
        "    # Save the history to an Excel file\n",
        "    history_df = pd.DataFrame(history.history)\n",
        "    history_df['epoch'] = range(1, len(history_df) + 1)\n",
        "\n",
        "    # Compute and store top_k_accuracies and RSRP differences for each k in k_values\n",
        "    top_k_accuracies_results = CustomKPI().top_k_accuracies(yhat, yTest, k_values)\n",
        "    rsrp_diffs_results = CustomKPI()._L1_rsrpDiffs(yhat_descaled, yTest_descaled, k_values)\n",
        "\n",
        "\n",
        "    # Adding new data to DataFrame\n",
        "    for k in k_values:\n",
        "        history_df[f'Top_1/{k}_Accuracy'] = top_k_accuracies_results[k]*100\n",
        "\n",
        "    for k in k_values:\n",
        "        history_df[f'L1-RSRP_diff_Top_1/{k}'] = rsrp_diffs_results[k]\n",
        "\n",
        "    history_df['naive_L1_RSRP_diff'] = naive_rsrp_diff[0]\n",
        "    history_df['naive_mean_Accuracy'] = naive_rsrp_diff[1]\n",
        "    history_df['Set_B_pattern'] = str(get_indices('fixed', 32, 8))\n",
        "    history_df['flag'] = is_data_same\n",
        "    history_df['datetime'] = now\n",
        "\n",
        "    # Update the history filename to include date and time\n",
        "    history_filename_with_time = f\"{history_filename}_epoch_{epochs}_date_time_{now}.xlsx\"\n",
        "    history_df.to_excel(history_filename_with_time, index=False)\n",
        "\n",
        "# # List of your models (assuming _getModel() is a function that returns different models)\n",
        "# # models = [_getModel() for _ in range(number_of_models)] # Adjust as per your model creation logic\n",
        "# models = [_getModel()] # Adjust as per your model creation logic\n",
        "\n",
        "# List of model name strings\n",
        "# Lmodel_names = ['fc' , 'fcJalal', 'fcJalal2', 'fcJalal3', 'fcJalal4', 'fcJalal5']\n",
        "model_names = ['fcJalal', 'fcJalal2', 'fcJalal3', 'fcJalal4', 'fcJalal5']\n",
        "\n",
        "# Training parameters\n",
        "epoch = 100\n",
        "batchSize = 512\n",
        "\n",
        "# Current date and time for file naming\n",
        "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Iterate over model names\n",
        "for modelNameString in model_names:\n",
        "    print(f\"Training model {modelNameString}\")\n",
        "\n",
        "    # Get the model instance for the current modelNameString\n",
        "    model = _getModel()\n",
        "\n",
        "    # Base directory for checkpoint and history, to be appended with datetime\n",
        "    base_directory = f'/content/drive/MyDrive/regression_problem/results/{now}'\n",
        "\n",
        "    # Create the base directory if it does not exist\n",
        "    if not os.path.exists(base_directory):\n",
        "        os.makedirs(base_directory)\n",
        "\n",
        "    # Filenames for checkpoint and history\n",
        "    chkpointFname = os.path.join(base_directory, f'chkpoint_model_{modelNameString}')\n",
        "    historyFname = os.path.join(base_directory, f'history_model_{modelNameString}')\n",
        "\n",
        "    # Train and save history\n",
        "    train_and_save_history(model, trainDatasetTF, valDatasetTF, epoch, batchSize, chkpointFname, historyFname, xTest, yTest)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYACrt0c23qZ",
        "outputId": "f2f6f951-4161-4555-ddc7-6e97339cb087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "\"\"\"\n",
            "@author: mpervej and jalal\n",
            "\"\"\"\n",
            "from tensorflow.keras import datasets, layers, models, regularizers\n",
            "import tensorflow as tf\n",
            "class getModel:\n",
            "    def __init__(self, _opDim):\n",
            "        self.opDim = _opDim\n",
            "    # def getCnnModel(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "    #     data_augmentation = tf.keras.Sequential([\n",
            "    #         layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
            "    #         layers.experimental.preprocessing.RandomRotation(0.2),\n",
            "    #     ])\n",
            "    #     model = models.Sequential()\n",
            "    #     model.add(data_augmentation)\n",
            "    #     model.add(layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen,_kerWid),\n",
            "    #                             activation='relu', input_shape=(ipLen, ipWid, ipCh) ))\n",
            "    #     # model.add(layers.BatchNorymalization())\n",
            "    #     # model.add(layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n",
            "    #     model.add(layers.Flatten())\n",
            "    #     model.add(layers.Dropout(rate=0.30))\n",
            "    #     model.add(layers.Dense(1024, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), activation='relu'))\n",
            "    #     # model.add(layers.BatchNormalization())\n",
            "    #     model.add(layers.Dropout(rate=0.40))\n",
            "    #     model.add(layers.Dense(self.opDim, activation='softmax'))\n",
            "    #     return model\n",
            "    \n",
            "    def getCnnModelV2(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        xInCopy = xIn\n",
            "\n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        # x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "        x = layers.Flatten()(x)\n",
            "        x = layers.Dropout(rate=0.30)(x)\n",
            "        x = layers.Dense(512, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "                     bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "        x = layers.Dropout(rate=0.50)(x)\n",
            "        x = layers.Dense(self.opDim, activation='softmax', kernel_regularizer=regularizers.L1L2(l2=1e-4))(x)\n",
            "\n",
            "\n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv2')\n",
            "        print(model.summary())\n",
            "        return model\n",
            "\n",
            "    def getCnnModelV3(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        xInCopy = xIn\n",
            "\n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "        x = layers.Flatten()(x)\n",
            "        x = layers.Dropout(rate=0.30)(x)\n",
            "        x = layers.Dense(512, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "                     bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "        x = layers.Dropout(rate=0.50)(x)\n",
            "        x = layers.Dense(self.opDim, activation='softmax', kernel_regularizer=regularizers.L1L2(l2=1e-4))(x)\n",
            "\n",
            "\n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv3')\n",
            "        print(model.summary())\n",
            "        return model\n",
            "\n",
            "    def getCnnModelV4(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        xInCopy = xIn\n",
            "\n",
            "        # note given fpr ex: kerlen=3, kerwid=3, ipfilter=32\n",
            "        # this will show up in the model summary output as:\n",
            "        #     conv2d (Conv2D)                   (None, 8, 4, 32)     320       \n",
            "        #     activation (Activation)           (None, 8, 4, 32)     0         \n",
            "        #     batch_normalization (BatchNorm)   (None, 8, 4, 32)     128       \n",
            "        # there is a nice formula and explanation here:\n",
            "        # https://stackoverflow.com/questions/45561306/understanding-model-summary-keras\n",
            "        # and here: https://towardsdatascience.com/how-to-calculate-the-number-of-parameters-in-keras-models-710683dae0ca\n",
            "        #      param_number = output_channel_number * (input_channel_number * kernel_height * kernel_width + 1)\n",
            "        #\n",
            "        \n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        #x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.BatchNormalization()(x)\n",
            "        #x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "        x = layers.Flatten()(x)\n",
            "        x = layers.Dropout(rate=0.10)(x)  # .3\n",
            "        #x = layers.Dense(512, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "        #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "        x = layers.Dense(1024, activation='relu')(x)\n",
            "        x = layers.Dropout(rate=0.1)(x)  # .5\n",
            "        x = layers.Dense(512, activation='relu')(x)\n",
            "        x = layers.Dropout(rate=0.05)(x)  # .5\n",
            "        x = layers.Dense(256, activation='relu')(x)\n",
            "        x = layers.Dropout(rate=0.05)(x)  # .5\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4))(x)\n",
            "\n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv4')\n",
            "        print(model.summary())\n",
            "        return model\n",
            "\n",
            "    def getCnnModelV5(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        xInCopy = xIn\n",
            "\n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        #x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.BatchNormalization()(x)\n",
            "        #x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "        x = layers.Flatten()(x)\n",
            "        #x = layers.Dropout(rate=0.10)(x)  # .3\n",
            "        #x = layers.Dense(512, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "        #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "        #x = layers.Dense(1024, activation='relu')(x)\n",
            "        #x = layers.Dropout(rate=0.1)(x)  # .5\n",
            "        x = layers.Dense(512, activation='relu')(x)\n",
            "        x = layers.Dropout(rate=0.05)(x)  # .5\n",
            "        x = layers.Dense(256, activation='relu')(x)\n",
            "        x = layers.Dropout(rate=0.05)(x)  # .5\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4))(x)\n",
            "\n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv5')\n",
            "        print(model.summary())\n",
            "        return model\n",
            "\n",
            "    def getCnnModelV6(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "    \n",
            "        # need to undo the reshaping done in prepdatamodules.py circa line 225\n",
            "        #     xTrainData = xScaledData.reshape(len(xScaledData), reshapeK1, reshapeK2, 1)\n",
            "        \n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        x = layers.Reshape((ipLen * ipWid, -1))(xIn)\n",
            "\n",
            "        x = layers.Conv1D(filters=_inputFilter , kernel_size=_kerLen, padding='same', activation='relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Conv1D(filters=2 * _inputFilter, kernel_size=3, strides=1, padding='same', activation='relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(x)\n",
            "        \n",
            "        x = layers.Conv1D(128, 3, activation='relu', padding='same')(x)  # 256\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling1D(2, padding='same')(x)\n",
            "\n",
            "        x = layers.Conv1D(256, 3, activation='relu', padding='same')(x)  # 512\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling1D(2, padding='same')(x)\n",
            "\n",
            "        \"\"\"\n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        #x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.BatchNormalization()(x)\n",
            "        #x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "        \"\"\"\n",
            "        x = layers.Flatten()(x)\n",
            "        #x = layers.Dropout(rate=0.10)(x)  # .3\n",
            "        #x = layers.Dense(512, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "        #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "        #x = layers.Dense(1024, activation='relu')(x)\n",
            "        #x = layers.Dropout(rate=0.1)(x)  # .5\n",
            "        #x = layers.Dense(512, activation='relu')(x)\n",
            "        #x = layers.Dropout(rate=0.05)(x)  # .5\n",
            "        x = layers.Dense(256, activation='relu')(x)\n",
            "        x = layers.Dropout(rate=0.05)(x)  # .5\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4))(x)\n",
            "\n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv6')\n",
            "        print(model.summary())\n",
            "        return model\n",
            "\n",
            "    def getCnnModel(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "        data_augmentation = tf.keras.Sequential([\n",
            "            layers.experimental.preprocessing.RandomFlip(\"horizontal\",\n",
            "                                                         input_shape=(ipLen, ipWid, ipCh)),\n",
            "            # layers.experimental.preprocessing.RandomRotation(0.2, input_shape=(ipLen, ipWid, ipCh)),\n",
            "            # layers.experimental.preprocessing.Normalization(axis=-1)\n",
            "        ])\n",
            "        model = models.Sequential([\n",
            "                    # data_augmentation,\n",
            "                    layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid),\n",
            "                                  input_shape=(ipLen, ipWid, ipCh),\n",
            "                                  activation='relu', kernel_regularizer=regularizers.L1L2(l2=1e-3)),\n",
            "                    # layers.BatchNormalization(),\n",
            "                    # layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)),\n",
            "                    layers.Flatten(),\n",
            "                    layers.Dropout(rate=0.30),\n",
            "                    layers.Dense(512, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "                                 bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu'),\n",
            "                    layers.Dropout(rate=0.50),\n",
            "                    layers.Dense(self.opDim, activation='softmax', kernel_regularizer=regularizers.L1L2(l2=1e-4))\n",
            "                ], name='cnn')\n",
            "        return model\n",
            "\n",
            "    def getCnnModelUp(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "        model = models.Sequential()\n",
            "        # model.add(layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid),\n",
            "        #                         activation='relu', input_shape=(ipLen, ipWid, ipCh)))\n",
            "        # model.add(layers.BatchNormalization())\n",
            "        # model.add(layers.MaxPooling2D(pool_size=(2, 8), strides=(2, 2), padding='same'))\n",
            "        model.add(layers.Conv2DTranspose(filters=4*_inputFilter, kernel_size=(4, 4), strides=(2,2),\n",
            "                                         activation='relu'))\n",
            "        # model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), activation='relu'))\n",
            "        # model.add(layers.BatchNormalization())\n",
            "        model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
            "        model.add(layers.Flatten())\n",
            "        # model.add(layers.Dropout(rate=0.30))\n",
            "        model.add(layers.Dense(512, activation='relu'))\n",
            "        # model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=0.30))\n",
            "        model.add(layers.Dense(self.opDim, activation='softmax'))\n",
            "        return model\n",
            "\n",
            "    def getResModel(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        xInCopy = xIn\n",
            "\n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        # x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        # x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "\n",
            "        # x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        # # x = layers.BatchNormalization()(x)\n",
            "        # x = layers.Activation('relu')(x)\n",
            "        # # x = layers.Dropout(rate=0.25)(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=3 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        # x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "\n",
            "        # x = layers.Conv2D(filters=3 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        # # x = layers.BatchNormalization()(x)\n",
            "        # x = layers.Activation('relu')(x)\n",
            "        # # x = layers.Dropout(rate=0.25)(x)\n",
            "\n",
            "        xShortcut = layers.Conv2D(filters=3*_inputFilter, kernel_size=(_kerLen, _kerWid), strides=(1, 1), padding='same')(xInCopy)\n",
            "        # xShortcut = layers.BatchNormalization()(xShortcut)\n",
            "\n",
            "        x = layers.Add()([x, xShortcut])\n",
            "        x = layers.Activation('relu')(x)\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "\n",
            "        x = layers.Conv2DTranspose(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), strides=(2, 2),\n",
            "                                         activation='relu')(x)\n",
            "        # x = layers.AveragePooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same',\n",
            "                          activation='relu')(x)\n",
            "        # x = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
            "        x = layers.AveragePooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "\n",
            "        x = layers.Flatten()(x)\n",
            "        x = layers.Dense(1024, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.Dropout(rate=0.30)(x)\n",
            "        #\n",
            "        x = layers.Dense(512, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.Dropout(rate=0.30)(x)\n",
            "\n",
            "        x = layers.Dense(self.opDim, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))(x)\n",
            "        x = layers.Activation('softmax')(x)\n",
            "\n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnResNet')\n",
            "        print(model.summary())\n",
            "        return model\n",
            "\n",
            "    def getFCModel(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s):\n",
            "    \n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "        # x = layers.Flatten()(x)\n",
            "        # x = layers.Dropout(rate=0.30)(x)\n",
            "        # x = layers.Dense(512, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "        #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "        # x = layers.Dropout(rate=0.50)(x)\n",
            "        # x = layers.Dense(self.opDim, activation='softmax', kernel_regularizer=regularizers.L1L2(l2=1e-4))(x)\n",
            "\n",
            "        #model = models.Sequential()\n",
            "        #model.add(layers.Dense(fcLayer1s, activation='relu', input_shape=(ipLen, ipWid, ipCh) ) )\n",
            "        ##model.add(layers.BatchNormalization())\n",
            "        #model.add(layers.Flatten())\n",
            "        #model.add(layers.Dropout(rate=0.30))  # .30\n",
            "        #model.add(layers.Dense(fcLayer2s, activation='relu'))\n",
            "        ##model.add(layers.BatchNormalization())\n",
            "        #model.add(layers.Dropout(rate=0.30))  # .30\n",
            "        #model.add(layers.Dense(self.opDim, activation='softmax', kernel_regularizer=regularizers.L1L2))\n",
            "\n",
            "        #model = models.Sequential()\n",
            "        #fcLayer1s, fcLayer2s = 1024, 512\n",
            "        #model.add(layers.Flatten())\n",
            "        #model.add(layers.Dense(2048, activation='relu', input_shape=(ipLen, ipWid, ipCh) ) )\n",
            "        #model.add(layers.BatchNormalization())\n",
            "        #model.add(layers.Dropout(rate=0.20))  # .30\n",
            "        #model.add(layers.Dense(1024, activation='relu', input_shape=(ipLen, ipWid, ipCh) ) )\n",
            "        #model.add(layers.BatchNormalization())\n",
            "        #model.add(layers.Dropout(rate=0.20))  # .30\n",
            "        #model.add(layers.Dense(512, activation='relu'))\n",
            "        #model.add(layers.BatchNormalization())\n",
            "        #model.add(layers.Dropout(rate=0.10))  # .30\n",
            "        #model.add(layers.Dense(256, activation='relu'))\n",
            "        #model.add(layers.BatchNormalization())\n",
            "        #model.add(layers.Dropout(rate=0.10))  # .30\n",
            "        #model.add(layers.Dense(self.opDim, activation='softmax')) # , kernel_regularizer=regularizers.L1L2))\n",
            "        \n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        x = layers.Flatten()(xIn)\n",
            "        #x = layers.Dropout(rate=0.30)(x)\n",
            "        NUNITS = [2048, 1024, 512, 256, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            x = layers.Dropout(rate=0.03)(x) # 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.10)(x)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='fc')\n",
            "        print(model.summary())\n",
            "        \n",
            "        return model\n",
            "\n",
            "    def getFCModel_ceiling(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s):\n",
            "\n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        x = layers.Flatten()(xIn)\n",
            "        # x = layers.Dropout(rate=0.30)(x)\n",
            "        NUNITS = [2048, 1024, 512, 256, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)\n",
            "        for n_units in NUNITS:\n",
            "            # x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            # if n_units == 1024 or n_units == 512:\n",
            "            # x = layers.Dropout(rate=0.03)(x)  # 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        # x = layers.Dropout(rate=0.10)(x)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x)  # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "\n",
            "        model = models.Model(inputs=xIn, outputs=x, name='fc')\n",
            "        print(model.summary())\n",
            "\n",
            "        return model\n",
            "\n",
            "    def getFCModelJalal1_ceiling(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s):\n",
            "        # fcLayer1s, fcLayer2s = 1024, 512\n",
            "        # reshape of the input to an 1-dim vector and then fill the model\n",
            "        model = models.Sequential()\n",
            "        model.add(layers.Flatten(input_shape=(ipLen, ipWid, ipCh)))\n",
            "        model.add(layers.Dense(fcLayer1s, activation='relu'))\n",
            "        # model.add(layers.Dropout(rate=0.1))\n",
            "        model.add(layers.Dense(fcLayer2s, activation='relu'))\n",
            "        # model.add(layers.Dropout(rate=0.1))\n",
            "        model.add(layers.Dense(self.opDim, activation='softmax'))\n",
            "        return model\n",
            "\n",
            "    def getFCModelJalal1(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s):\n",
            "        # fcLayer1s, fcLayer2s = 1024, 512\n",
            "        # reshape of the input to an 1-dim vector and then fill the model\n",
            "        model = models.Sequential()\n",
            "        model.add(layers.Flatten(input_shape=(ipLen, ipWid, ipCh)))\n",
            "        model.add(layers.Dense(fcLayer1s, activation='relu'))\n",
            "        model.add(layers.Dropout(rate=0.1))\n",
            "        model.add(layers.Dense(fcLayer2s, activation='relu'))\n",
            "        model.add(layers.Dropout(rate=0.1))\n",
            "        # model.add(layers.Dense(self.opDim, activation='softmax'))\n",
            "        # for regression \n",
            "        model.add(layers.Dense(self.opDim, activation='linear'))\n",
            "        return model\n",
            "\n",
            "    def getFCModelJalal2(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s,\n",
            "                        dropout_rate=0.1, activation_fn1='relu', activation_fn2='relu'):\n",
            "        \"\"\"\n",
            "        Generate a Fully Connected (FC) Keras model.\n",
            "\n",
            "        Parameters:\n",
            "        - ipCh (int): Number of input channels.\n",
            "        - ipLen (int): Length of the input.\n",
            "        - ipWid (int): Width of the input.\n",
            "        - fcLayer1s (int): Number of units in the first dense layer.\n",
            "        - fcLayer2s (int): Number of units in the second dense layer.\n",
            "        - dropout_rate (float, optional): Dropout rate for regularization. Default is 0.3.\n",
            "        - activation_fn1 (str, optional): Activation function for the first dense layer. Default is 'relu'.\n",
            "        - activation_fn2 (str, optional): Activation function for the second dense layer. Default is 'relu'.\n",
            "\n",
            "        Returns:\n",
            "        - model: Keras Sequential model.\n",
            "        \"\"\"\n",
            "\n",
            "        model = models.Sequential()\n",
            "        model.add(layers.Flatten(input_shape=(ipLen, ipWid, ipCh)))\n",
            "        model.add(layers.Dense(fcLayer1s, activation=activation_fn1))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "        model.add(layers.Dense(fcLayer2s, activation=activation_fn2))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "        model.add(layers.Dense(self.opDim))\n",
            "\n",
            "        return model\n",
            "\n",
            "    def getFCModelJalal3(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s,\n",
            "                        dropout_rate=0.1, activation_fn='relu'):\n",
            "                      \n",
            "        \"\"\"\n",
            "        Generate a Fully Connected (FC) Keras model with additional layers.\n",
            "\n",
            "        Parameters:\n",
            "        - ipCh (int): Number of input channels.\n",
            "        - ipLen (int): Length of the input.\n",
            "        - ipWid (int): Width of the input.\n",
            "        - fcLayer1s (int): Number of units in the first dense layer.\n",
            "        - fcLayer2s (int): Number of units in the second dense layer.\n",
            "        - dropout_rate (float, optional): Dropout rate for regularization. Default is 0.3.\n",
            "        - activation_fn (str, optional): Activation function for the dense layers. Default is 'relu'.\n",
            "\n",
            "        Returns:\n",
            "        - model: Keras Sequential model.\n",
            "        \"\"\"\n",
            "\n",
            "        model = models.Sequential()\n",
            "        model.add(layers.Flatten(input_shape=(ipLen, ipWid, ipCh)))\n",
            "        model.add(layers.Dense(fcLayer1s, activation=activation_fn))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "        model.add(layers.Dense(fcLayer2s, activation=activation_fn))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "\n",
            "        # Additional Layers\n",
            "        model.add(layers.Dense(fcLayer2s // 2, activation=activation_fn))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "        model.add(layers.Dense(fcLayer2s // 4, activation=activation_fn))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "        model.add(layers.Dense(fcLayer2s // 8, activation=activation_fn))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "\n",
            "        model.add(layers.Dense(self.opDim)) # Linear activation\n",
            "        \n",
            "        # Explicitly build the model\n",
            "        # model.build(input_shape=(None, ipLen, ipWid, ipCh))\n",
            "\n",
            "        return model\n",
            "\n",
            "    def getFCModelJalal3_ceiling(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s,\n",
            "                        dropout_rate=0.1, activation_fn='relu'):\n",
            "        \"\"\"\n",
            "        Generate a Fully Connected (FC) Keras model with additional layers.\n",
            "\n",
            "        Parameters:\n",
            "        - ipCh (int): Number of input channels.\n",
            "        - ipLen (int): Length of the input.\n",
            "        - ipWid (int): Width of the input.\n",
            "        - fcLayer1s (int): Number of units in the first dense layer.\n",
            "        - fcLayer2s (int): Number of units in the second dense layer.\n",
            "        - dropout_rate (float, optional): Dropout rate for regularization. Default is 0.3.\n",
            "        - activation_fn (str, optional): Activation function for the dense layers. Default is 'relu'.\n",
            "\n",
            "        Returns:\n",
            "        - model: Keras Sequential model.\n",
            "        \"\"\"\n",
            "\n",
            "        model = models.Sequential()\n",
            "        model.add(layers.Flatten(input_shape=(ipLen, ipWid, ipCh)))\n",
            "        model.add(layers.Dense(fcLayer1s, activation=activation_fn))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        # model.add(layers.Dropout(rate=dropout_rate))\n",
            "        model.add(layers.Dense(fcLayer2s, activation=activation_fn))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        # model.add(layers.Dropout(rate=dropout_rate))\n",
            "\n",
            "        # Additional Layers\n",
            "        model.add(layers.Dense(fcLayer2s // 2, activation=activation_fn))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        # model.add(layers.Dropout(rate=dropout_rate))\n",
            "        model.add(layers.Dense(fcLayer2s // 4, activation=activation_fn))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        # model.add(layers.Dropout(rate=dropout_rate))\n",
            "        model.add(layers.Dense(fcLayer2s // 8, activation=activation_fn))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        # model.add(layers.Dropout(rate=dropout_rate))\n",
            "\n",
            "        model.add(layers.Dense(self.opDim, activation='softmax'))\n",
            "\n",
            "        return model\n",
            "\n",
            "    def getFCModelJalal4(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s,\n",
            "                        dropout_rate=0.1, activation_fn='relu'):\n",
            "        \"\"\"\n",
            "        Generate a Fully Connected (FC) Keras model with larger layers.\n",
            "\n",
            "        Parameters:\n",
            "        - ipCh (int): Number of input channels.\n",
            "        - ipLen (int): Length of the input.\n",
            "        - ipWid (int): Width of the input.\n",
            "        - fcLayer1s (int): Number of units in the first dense layer.\n",
            "        - fcLayer2s (int): Number of units in the second dense layer.\n",
            "        - dropout_rate (float, optional): Dropout rate for regularization. Default is 0.3.\n",
            "        - activation_fn (str, optional): Activation function for the dense layers. Default is 'relu'.\n",
            "\n",
            "        Returns:\n",
            "        - model: Keras Sequential model.\n",
            "        \"\"\"\n",
            "\n",
            "        model = models.Sequential()\n",
            "        model.add(layers.Flatten(input_shape=(ipLen, ipWid, ipCh)))\n",
            "        model.add(layers.Dense(fcLayer1s * 2, activation=activation_fn))  # Doubled neurons\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "        model.add(layers.Dense(fcLayer2s * 3, activation=activation_fn))  # Tripled neurons\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "\n",
            "        # Additional Layers\n",
            "        model.add(layers.Dense(fcLayer2s * 2, activation=activation_fn))  # Doubled neurons\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "        model.add(layers.Dense(fcLayer2s, activation=activation_fn))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "        model.add(layers.Dense(fcLayer2s // 2, activation=activation_fn))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "\n",
            "        model.add(layers.Dense(self.opDim, activation='softmax'))\n",
            "\n",
            "        return model\n",
            "\n",
            "    # use another activation\n",
            "    def getFCModelJalal5(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s,\n",
            "                        dropout_rate=0.1):\n",
            "        \"\"\"\n",
            "        Generate a Fully Connected (FC) Keras model with additional layers\n",
            "        and LeakyReLU activations.\n",
            "\n",
            "        Parameters:\n",
            "        - ipCh (int): Number of input channels.\n",
            "        - ipLen (int): Length of the input.\n",
            "        - ipWid (int): Width of the input.\n",
            "        - fcLayer1s (int): Number of units in the first dense layer.\n",
            "        - fcLayer2s (int): Number of units in the second dense layer.\n",
            "        - dropout_rate (float, optional): Dropout rate for regularization. Default is 0.3.\n",
            "\n",
            "        Returns:\n",
            "        - model: Keras Sequential model.\n",
            "        \"\"\"\n",
            "\n",
            "        leaky_relu = layers.LeakyReLU(alpha=0.1)\n",
            "\n",
            "        model = models.Sequential()\n",
            "        model.add(layers.Flatten(input_shape=(ipLen, ipWid, ipCh)))\n",
            "        model.add(layers.Dense(fcLayer1s))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(leaky_relu)\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "\n",
            "        model.add(layers.Dense(fcLayer2s))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(leaky_relu)\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "\n",
            "        # Additional Layers\n",
            "        model.add(layers.Dense(fcLayer2s // 2))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(leaky_relu)\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "\n",
            "        model.add(layers.Dense(fcLayer2s // 4))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(leaky_relu)\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "\n",
            "        model.add(layers.Dense(fcLayer2s // 8))\n",
            "        model.add(layers.BatchNormalization())\n",
            "        model.add(leaky_relu)\n",
            "        model.add(layers.Dropout(rate=dropout_rate))\n",
            "\n",
            "        model.add(layers.Dense(self.opDim, activation='softmax'))\n",
            "\n",
            "        return model\n",
            "\n",
            "\n",
            "    def getFCModelGot77(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s):\n",
            "    \n",
            "        \n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        x = layers.Flatten()(xIn)\n",
            "        #x = layers.Dropout(rate=0.30)(x)\n",
            "        NUNITS = [2048, 1024, 512, 256, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            x = layers.Dropout(rate=0.03)(x) # 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.10)(x)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='fc77')\n",
            "        print(model.summary())\n",
            "        \n",
            "        return model\n",
            "\n",
            "    def getFCModel88(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s): # 0.7724 fixed blen=8 for 32x1 best so far 8.10.2023 4:26 pm \n",
            "    \n",
            "        \n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        x = layers.Flatten()(xIn)\n",
            "        #x = layers.Dropout(rate=0.30)(x)\n",
            "        NUNITS = [1024, 768, 448, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            x = layers.Dropout(rate=0.07)(x) # 0.03 to 0.05 to 0.07 # 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.11)(x)  # 0.1 to 0.07 to 0.11\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='fc88')\n",
            "        print(model.summary())\n",
            "        \n",
            "        return model\n",
            "\n",
            "    def getFCModel99(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s):\n",
            "    \n",
            "        \n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        x = layers.Flatten()(xIn)\n",
            "        #x = layers.Dropout(rate=0.30)(x)\n",
            "        # f99:  NUNITS = [768, 768, 448, 256]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        \n",
            "        NUNITS = [896, 512, 256, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            # f99: x = layers.Dropout(rate=0.08)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.Dropout(rate=0.05)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.12)(x)  # 0.1 to 0.07 to 0.11 to 0.12(f99) 0.09\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='fc99')\n",
            "        print(model.summary())\n",
            "        \n",
            "        return model\n",
            "\n",
            "    def getFCModel99b(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s):\n",
            "    \n",
            "        \n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        x = layers.Flatten()(xIn)\n",
            "        #x = layers.Dropout(rate=0.30)(x)\n",
            "        # f99:  NUNITS = [768, 768, 448, 256]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        \n",
            "        NUNITS = [896, 512, 256, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            # f99: x = layers.Dropout(rate=0.08)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.Dropout(rate=0.05)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.09)(x)  # 0.1 to 0.07 to 0.11 to 0.12(f99) 0.09(f99b)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='fc99b')\n",
            "        print(model.summary())\n",
            "        \n",
            "        return model\n",
            "\n",
            "    def getFCModel99c(self, ipCh, ipLen, ipWid, fcLayer1s, fcLayer2s):\n",
            "    \n",
            "        \n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        x = layers.Flatten()(xIn)\n",
            "        #x = layers.Dropout(rate=0.30)(x)\n",
            "        # f99:  NUNITS = [768, 768, 448, 256]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        \n",
            "        NUNITS = [1024, 896, 512, 256, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            # f99: x = layers.Dropout(rate=0.08)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.Dropout(rate=0.06)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.09)(x)  # 0.1 to 0.07 to 0.11 to 0.12(f99) 0.09(f99b)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='fc99c')\n",
            "        print(model.summary())\n",
            "        \n",
            "        return model\n",
            "\n",
            "    def getCnnModelV7(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        xInCopy = xIn\n",
            "\n",
            "        # note given fpr ex: kerlen=3, kerwid=3, ipfilter=32\n",
            "        # this will show up in the model summary output as:\n",
            "        #     conv2d (Conv2D)                   (None, 8, 4, 32)     320       \n",
            "        #     activation (Activation)           (None, 8, 4, 32)     0         \n",
            "        #     batch_normalization (BatchNorm)   (None, 8, 4, 32)     128       \n",
            "        # there is a nice formula and explanation here:\n",
            "        # https://stackoverflow.com/questions/45561306/understanding-model-summary-keras\n",
            "        # and here: https://towardsdatascience.com/how-to-calculate-the-number-of-parameters-in-keras-models-710683dae0ca\n",
            "        #      param_number = output_channel_number * (input_channel_number * kernel_height * kernel_width + 1)\n",
            "        #\n",
            "        \n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        #x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.BatchNormalization()(x)\n",
            "        #x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Dropout(rate=0.08)(x) # 0.05 to 0.08, 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "        x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.Flatten()(x)\n",
            "\n",
            "        NUNITS = [896, 512, 256, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            # f99: x = layers.Dropout(rate=0.08)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.Dropout(rate=0.08)(x) # 0.05 to 0.08, 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.1)(x)  # 0.09 to 0.1, 0.1 to 0.07 to 0.11 to 0.12(f99) 0.09(f99b)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv7')\n",
            "        \"\"\"\n",
            "                # x = layers.Dropout(rate=0.25)(x)\n",
            "                x = layers.Flatten()(x)\n",
            "                x = layers.Dropout(rate=0.10)(x)  # .3\n",
            "                #x = layers.Dense(512, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "                #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "                x = layers.Dense(1024, activation='relu')(x)\n",
            "                x = layers.Dropout(rate=0.1)(x)  # .5\n",
            "                x = layers.Dense(512, activation='relu')(x)\n",
            "                x = layers.Dropout(rate=0.05)(x)  # .5\n",
            "                x = layers.Dense(256, activation='relu')(x)\n",
            "                x = layers.Dropout(rate=0.05)(x)  # .5\n",
            "                x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4))(x)\n",
            "\n",
            "                model = models.Model(inputs=xIn, outputs=x, name='cnnv4')\n",
            "        \"\"\"\n",
            "        print(model.summary())\n",
            "        return model\n",
            "\n",
            "    def getCnnModelV8(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        xInCopy = xIn\n",
            "\n",
            "        # note given fpr ex: kerlen=3, kerwid=3, ipfilter=32\n",
            "        # this will show up in the model summary output as:\n",
            "        #     conv2d (Conv2D)                   (None, 8, 4, 32)     320       \n",
            "        #     activation (Activation)           (None, 8, 4, 32)     0         \n",
            "        #     batch_normalization (BatchNorm)   (None, 8, 4, 32)     128       \n",
            "        # there is a nice formula and explanation here:\n",
            "        # https://stackoverflow.com/questions/45561306/understanding-model-summary-keras\n",
            "        # and here: https://towardsdatascience.com/how-to-calculate-the-number-of-parameters-in-keras-models-710683dae0ca\n",
            "        #      param_number = output_channel_number * (input_channel_number * kernel_height * kernel_width + 1)\n",
            "        #\n",
            "        \n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "        x = layers.Dropout(rate=0.08)(x) # 0.05 to 0.08, 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "\n",
            "        x = layers.Conv2D(128,(2,2), activation='relu', padding='same')(x) # 256 to 128\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "        x = layers.Dropout(rate=0.04)(x) # 0.08 to 0.04, 0.05 to 0.08, 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "\n",
            "        #x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.BatchNormalization()(x)\n",
            "        #x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)  # 512 to 256\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.Flatten()(x)\n",
            "\n",
            "        NUNITS = [512, 256, 128]  # removed 896, | 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            # f99: x = layers.Dropout(rate=0.08)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.Dropout(rate=0.08)(x) # 0.05 to 0.08, 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.1)(x)  # 0.09 to 0.1, 0.1 to 0.07 to 0.11 to 0.12(f99) 0.09(f99b)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv8')\n",
            "        print(model.summary())\n",
            "        return model\n",
            "\n",
            "    def getCnnModelV8a(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "        xInCopy = xIn\n",
            "\n",
            "        # note given fpr ex: kerlen=3, kerwid=3, ipfilter=32\n",
            "        # this will show up in the model summary output as:\n",
            "        #     conv2d (Conv2D)                   (None, 8, 4, 32)     320       \n",
            "        #     activation (Activation)           (None, 8, 4, 32)     0         \n",
            "        #     batch_normalization (BatchNorm)   (None, 8, 4, 32)     128       \n",
            "        # there is a nice formula and explanation here:\n",
            "        # https://stackoverflow.com/questions/45561306/understanding-model-summary-keras\n",
            "        # and here: https://towardsdatascience.com/how-to-calculate-the-number-of-parameters-in-keras-models-710683dae0ca\n",
            "        #      param_number = output_channel_number * (input_channel_number * kernel_height * kernel_width + 1)\n",
            "        #\n",
            "        \n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        #x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "        x = layers.Dropout(rate=0.18)(x) # 0.05 to 0.08, 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "\n",
            "        x = layers.Conv2D(128,(2,2), activation='relu', padding='same')(x) # 256 to 128\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        #x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "        x = layers.Dropout(rate=0.14)(x) # 0.08 to 0.04, 0.05 to 0.08, 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "\n",
            "        #x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.BatchNormalization()(x)\n",
            "        #x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)  # 512 to 256\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.Dropout(rate=0.10)(x) # 0.08 to 0.04, 0.05 to 0.08, 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "\n",
            "        x = layers.Flatten()(x)\n",
            "\n",
            "        NUNITS = [512, 256, 128]  # removed 896, | 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            # f99: x = layers.Dropout(rate=0.08)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.Dropout(rate=0.12)(x) # 0.05 to 0.08, 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.1)(x)  # 0.09 to 0.1, 0.1 to 0.07 to 0.11 to 0.12(f99) 0.09(f99b)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv8a')\n",
            "        print(model.summary())\n",
            "        return model\n",
            "\n",
            "    def getCnnModelV9(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "    \n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "\n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        # x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "        x = layers.Dropout(rate=0.12)(x)\n",
            "\n",
            "        #x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        ##x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.BatchNormalization()(x)\n",
            "        #x = layers.Dropout(rate=0.12)(x)\n",
            "\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "        x = layers.Flatten()(x)\n",
            "        \n",
            "        # f99:  NUNITS = [768, 768, 448, 256]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        \n",
            "        NUNITS = [896, 512, 256, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            # f99: x = layers.Dropout(rate=0.08)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.Dropout(rate=0.11)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.1)(x)  # 0.1 to 0.07 to 0.11 to 0.12(f99) 0.09(f99b)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv9')\n",
            "        print(model.summary())\n",
            "        \n",
            "        return model\n",
            "\n",
            "    def getCnnModelV9b(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "    \n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "\n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        # x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.Dropout(rate=0.15)(x)\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "        x = layers.Dropout(rate=0.15)(x)\n",
            "\n",
            "        #x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        ##x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.BatchNormalization()(x)\n",
            "        #x = layers.Dropout(rate=0.12)(x)\n",
            "\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "        x = layers.Flatten()(x)\n",
            "        \n",
            "        # f99:  NUNITS = [768, 768, 448, 256]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        \n",
            "        NUNITS = [896, 512, 256, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            # f99: x = layers.Dropout(rate=0.08)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.Dropout(rate=0.16)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.11)(x)  # 0.1 to 0.07 to 0.11 to 0.12(f99) 0.09(f99b)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv9b')\n",
            "        print(model.summary())\n",
            "        \n",
            "        return model\n",
            "\n",
            "    def getCnnModelV9c(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "    \n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "\n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        # x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.Dropout(rate=0.15)(x)\n",
            "        x = layers.Conv2D(384,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        #x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "        x = layers.Dropout(rate=0.20)(x)\n",
            "\n",
            "        #x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        ##x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.BatchNormalization()(x)\n",
            "        #x = layers.Dropout(rate=0.12)(x)\n",
            "\n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "        x = layers.Flatten()(x)\n",
            "        \n",
            "        # f99:  NUNITS = [768, 768, 448, 256]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        \n",
            "        NUNITS = [896, 512, 256, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            # f99: x = layers.Dropout(rate=0.08)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.Dropout(rate=0.20)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.15)(x)  # 0.1 to 0.07 to 0.11 to 0.12(f99) 0.09(f99b)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv9c')\n",
            "        print(model.summary())\n",
            "        \n",
            "        return model\n",
            "\n",
            "    def getCnnModelV10(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "    \n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "\n",
            "        x = layers.Conv2D(filters=_inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        # x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        \"\"\"\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "        x = layers.Dropout(rate=0.12)(x)\n",
            "\n",
            "        #x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        ##x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.BatchNormalization()(x)\n",
            "        #x = layers.Dropout(rate=0.12)(x)\n",
            "        \n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "        \"\"\"\n",
            "        x = layers.Dropout(rate=0.12)(x)\n",
            "        x = layers.Flatten()(x)\n",
            "        \n",
            "        # f99:  NUNITS = [768, 768, 448, 256]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        \n",
            "        NUNITS = [896, 512, 256, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            # f99: x = layers.Dropout(rate=0.08)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.Dropout(rate=0.15)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.1)(x)  # 0.1 to 0.07 to 0.11 to 0.12(f99) 0.09(f99b)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv10')\n",
            "        print(model.summary())\n",
            "        \n",
            "        return model\n",
            "\n",
            "    def getCnnModelV10a(self, ipCh, ipLen, ipWid, _inputFilter, _kerLen, _kerWid):\n",
            "    \n",
            "        xIn = layers.Input((ipLen, ipWid, ipCh))\n",
            "\n",
            "        x = layers.Conv2D(filters=2 * _inputFilter, kernel_size=(_kerLen, _kerWid), padding='same')(xIn)\n",
            "        # x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.Dropout(rate=0.2)(x)\n",
            "\n",
            "        x = layers.Conv2D(filters=4 * _inputFilter, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.Activation('relu')(x)\n",
            "        x = layers.Dropout(rate=0.15)(x)\n",
            "        \"\"\"\n",
            "        x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.Conv2D(256,(2,2), activation='relu', padding='same')(x)\n",
            "        x = layers.BatchNormalization()(x)\n",
            "        x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
            "        x = layers.Dropout(rate=0.12)(x)\n",
            "\n",
            "        #x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        ##x = layers.Conv2D(512,(2,2), activation='relu', padding='same')(x)\n",
            "        #x = layers.BatchNormalization()(x)\n",
            "        #x = layers.Dropout(rate=0.12)(x)\n",
            "        \n",
            "        # x = layers.Dropout(rate=0.25)(x)\n",
            "        \"\"\"\n",
            "        x = layers.Flatten()(x)\n",
            "        \n",
            "        # f99:  NUNITS = [768, 768, 448, 256]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        \n",
            "        NUNITS = [896, 512, 256, 128]  # 2048, 2048, 1024, 1024, 512, 512, 256, 128)  # 512 to 768, 256 to 448, 128 to 256 1024 to 768\n",
            "        for n_units in NUNITS:\n",
            "            #x = layers.Dense(n_units, kernel_regularizer=regularizers.L1L2(l2=1e-3),\n",
            "            #             bias_regularizer=regularizers.L1L2(l2=1e-3), activation='relu')(x)\n",
            "            x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "            #if n_units == 1024 or n_units == 512:\n",
            "            # f99: x = layers.Dropout(rate=0.08)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.Dropout(rate=0.19)(x) # 0.03 to 0.05 to 0.07 to 0.08# 5 if n_units == 1024 or n_units == 512 else 0.03)(x)\n",
            "            x = layers.BatchNormalization()(x)\n",
            "\n",
            "        x = layers.Dense(n_units, activation='relu')(x)  # relu'\n",
            "        x = layers.Dropout(rate=0.12)(x)  # 0.1 to 0.07 to 0.11 to 0.12(f99) 0.09(f99b)\n",
            "        x = layers.Dense(self.opDim, activation='softmax')(x) # , kernel_regularizer=regularizers.L1L2(l2=1e-4)\n",
            "        \n",
            "        model = models.Model(inputs=xIn, outputs=x, name='cnnv10a')\n",
            "        print(model.summary())\n",
            "        \n",
            "        return model\n",
            "\n",
            "    def getVGG16(self, ipCh, ipLen, ipWid):\n",
            "        \"\"\"Main Authors implementation: https://www.robots.ox.ac.uk/~vgg/research/very_deep/ \"\"\"\n",
            "        model = models.Sequential()\n",
            "        model.add(layers.Conv2D(input_shape=(ipLen, ipWid, ipCh), filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
            "\n",
            "        model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
            "\n",
            "        model.add(layers.Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.MaxPool2D(pool_size=(2, 2), padding='same', strides=(2, 2)))\n",
            "\n",
            "        model.add(layers.Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.MaxPool2D(pool_size=(2, 2), padding='same', strides=(2, 2)))\n",
            "\n",
            "        model.add(layers.Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.MaxPool2D(pool_size=(2, 2), padding='same', strides=(2, 2)))\n",
            "        model.add(layers.Flatten())\n",
            "\n",
            "        model.add(layers.Dense(units=4096, kernel_regularizer=regularizers.L1L2(l1=0, l2=1e-4), activation=\"relu\"))\n",
            "        model.add(layers.Dropout(rate=0.3))\n",
            "        model.add(layers.Dense(units=1024, kernel_regularizer=regularizers.L1L2(l1=0, l2=1e-4), activation=\"relu\"))\n",
            "        model.add(layers.Dropout(rate=0.3))\n",
            "        model.add(layers.Dense(units=self.opDim, activation=\"softmax\"))\n",
            "        return model\n",
            "\n",
            "    def getModifiedVGG(self, ipCh, ipLen, ipWid):\n",
            "        \"\"\"Main Authors implementation: https://www.robots.ox.ac.uk/~vgg/research/very_deep/ \"\"\"\n",
            "        model = models.Sequential()\n",
            "        model.add(layers.Conv2D(input_shape=(ipLen, ipWid, ipCh), filters=64, kernel_size=(11, 9), padding=\"same\", activation=\"relu\"))\n",
            "        # model.add(layers.Conv2D(filters=64, kernel_size=(9, 7), padding=\"same\", activation=\"relu\"))\n",
            "        # model.add(layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
            "        model.add(layers.Dropout(rate=0.25))\n",
            "\n",
            "        model.add(layers.Conv2D(filters=128, kernel_size=(11, 9), padding=\"same\", activation=\"relu\"))\n",
            "        # model.add(layers.Conv2D(filters=128, kernel_size=(9, 7), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
            "\n",
            "        # model.add(layers.Conv2D(filters=256, kernel_size=(11, 9), padding=\"same\", activation=\"relu\"))\n",
            "        # model.add(layers.Conv2D(filters=256, kernel_size=(9, 7), padding=\"same\", activation=\"relu\"))\n",
            "        # model.add(layers.Conv2D(filters=256, kernel_size=(7, 5), padding=\"same\", activation=\"relu\"))\n",
            "        # model.add(layers.MaxPool2D(pool_size=(2, 2), padding='same', strides=(2, 2)))\n",
            "\n",
            "        # model.add(layers.Conv2DTranspose(filters=256, kernel_size=(9, 7), padding=\"same\", activation=\"relu\"))\n",
            "        # model.add(layers.Conv2D(filters=512, kernel_size=(9, 7), padding=\"same\", activation=\"relu\"))\n",
            "        # model.add(layers.Conv2D(filters=512, kernel_size=(7, 5), padding=\"same\", activation=\"relu\"))\n",
            "        # model.add(layers.MaxPool2D(pool_size=(2, 2), padding='same', strides=(2, 2)))\n",
            "        #\n",
            "        model.add(layers.Conv2DTranspose(filters=128, kernel_size=(7, 5), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.MaxPool2D(pool_size=(2, 2), padding='same', strides=(2, 2)))\n",
            "        model.add(layers.Conv2DTranspose(filters=64, kernel_size=(5, 3), padding=\"same\", activation=\"relu\"))\n",
            "        # model.add(layers.Conv2D(filters=512, kernel_size=(7, 5), padding=\"same\", activation=\"relu\"))\n",
            "        model.add(layers.MaxPool2D(pool_size=(2, 2), padding='same', strides=(2, 2)))\n",
            "        model.add(layers.Flatten())\n",
            "\n",
            "        # model.add(layers.Dense(units=1024, activation=\"relu\"))\n",
            "        # model.add(layers.Dropout(rate=0.25))\n",
            "        model.add(layers.Dense(units=512, activation=\"relu\"))\n",
            "        model.add(layers.Dropout(rate=0.25))\n",
            "        model.add(layers.Dense(units=self.opDim, activation=\"softmax\"))\n",
            "        return model\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Replace 'your_script.py' with the name of your Python file\n",
        "drive.mount('/content/drive/')\n",
        "file_name = '/content/drive/MyDrive/regression_problem/models.py'\n",
        "\n",
        "# Open the file and read its contents\n",
        "with open(file_name, 'r') as file:\n",
        "    file_contents = file.read()\n",
        "\n",
        "# Print the file contents\n",
        "print(file_contents)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPWpzQQB9M0gh3pqstfpbNz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}